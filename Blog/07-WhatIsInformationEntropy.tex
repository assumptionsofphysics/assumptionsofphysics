\documentclass[aps,pra,10pt,floatfix,nofootinbib]{revtex4-1}

\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{tikz}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{assump1}{Classical assumption}
\newtheorem*{assump2}{Determinism and Reversibility assumption}

\begin{document}
	
\section{What is information entropy?}

TL;DR .

Starting from the 1950s, the connection between information theory and thermodynamics has been increasingly made clear. What seems not be as clear is that there is a deep link between information theory and Hamiltonian mechanics as well. I want to explore this link in the next few posts and it is appropriate to first have a clear idea on what information entropy is.

The concept of entropy in physics is unfortunately associated with a certain amount of vagueness. While mathematically is well defined, I have never hear a crisp conceptual understanding (e.g. it measures "disorder", it has to do with how much the system is homogeneous, ...). the conceptual idea conceptually people describe it as disorder, or order, or information, or lack of information or how much the system is homogeneous. Entropy in information theory, or Shannon's entropy, instead has a crisp intuitive meaning and therefore I cringe when I hear physicists defining it as "lack of information" or any other vague nonsense.

The scenario is this: you have a distribution over a set of possibilities. You pick an element according to that distribution. On average, how many bits of information you have to use to tell me which element have you picked? How many one-zeros? If that does make sense yet, it will after we go through a few examples.

\section{Examples}

Suppose you have a bag with 40 balls. 20 are green and 20 are red. Suppose you pick a ball and you want to tell me the color. How many bits of information you need to give me? How many one-zero choices? The answer is one bit: 0 for green and 1 for red. That is: all the information can be encoded into one zero-one choice.

Suppose all 40 balls are green. How many bits of information? The answer is zero bits: I already know you are going to get a green ball.

Suppose there are 10 balls for each of these color: green, red, yellow and blue. How many bits of information? We can set 00 for green, 01 for red, 10 for yellow and 11 for blue. That is two zero-one choices: it's two bits of information.

Shannon's entropy simply generalizes this idea: given a $n$ cases each with a probability $p_i$, the average number of bits required to identify an element is:

\begin{equation}
\label{ShannonEntropy}
H(p_1, ..., p_n) = - \sum_{i=1}^{n} p_i \log p_i
\end{equation}

The base of the logarithm determines the units. If it's $2$, then we are using bits. If it's $e$, the natural log, then we are using nats.

\section{Uniqueness of the expression}

It is important to understand that the above expression for entropy is not arbitrary. Consider the following three requirements.

\begin{enumerate}
	\item $H(p_1, ..., p_n)$ is continuous.
	\item $H(p_1, ..., p_n)$ increases if the number of choice increases. That is, suppose you have two uniform distributions over $n$ and $m$ cases respectively. If $m>n$ then $H(\frac{1}{m}, ..., \frac{1}{m})>H(\frac{1}{n}, ..., \frac{1}{n})$.
	\item It does not change if we group/break down choice. Consider the following trees:
	\begin{figure}[h]
	\includegraphics[scale=0.70]{ShannonTree}
	\centering
	\end{figure}
	
	We should get the same result whether we have one distribution or we combine the two. That is: $H(\frac{1}{2}, \frac{1}{3}, \frac{1}{6}) = H(\frac{1}{2},\frac{1}{2}) + \frac{1}{2} H(\frac{2}{3}, \frac{1}{3})$. The coefficient $\frac{1}{2}$ appears because the second choice only occurs half the times.
\end{enumerate}

These are basic requirements that an expression that measures information needs to satisfy. It's not just that Shannon's entropy \eqref{ShannonEntropy} satisfies these conditions: it is the only one that does. While we are not going to derive it here, the derivation is not particularly insightful, it is important to understand that it can be derived. Shannon's entropy is not just an expression that we use because it's convenient or because it's known to work: it's the only one that can work.

\section{Relation to physics}

There are a couple of aspect we need to clarify as it allows to understand why such concept is interesting for physics.

First of all, information entropy has nothing to do with knowledge (e.g. what we know about the system or what can be known). This misunderstanding arises because the concept can be applied to probability distribution that represent our knowledge of the system. For example, if there are four boxes and I do not know in which one the ball is hiding, I have a uniform distribution on four cases which has 2 bits of information entropy. Conversely, if I know where it is, the entropy is 0 bits. So, in this and only this case, more entropy correspond to less knowledge.

But not all distributions represent knowledge and/or are statistical. For example, we could have a mass distribution in space. In this case, information entropy does not equate to lack of knowledge. Our knowledge of the mass distribution is perfect: it just happens that the mass is spread over a region. The information entropy of a mass distribution in space is as much a physical property of the system as it is the mass distribution itself and it tells us how many bits are necessary to identify the position of a part of the system. Unlike the previous case, this is objective number. It is this second use that is the most relevant to physics. Nature does not much care about our knowledge: it cares about objective facts.

Another thing to keep in mind is that information entropy is a quantity that relates two levels of description: the distribution and the elements over which the distribution is defined. This is exactly the same for the thermodynamic entropy, which relates the state of the whole system (i.e. the macro-state) with the state of its components (i.e. the micro-states). For Hamiltonian mechanics, we will study the evolution of distribution over position and momentum and see how it relates to the evolution of particles.

\section{Conclusion}

This hopefully helps you understand what information entropy measures: how many bits of informations are required on average to identify an element in a distribution. Not "knowledge" or "lack of information" or "disorder" or any other vague concept. In the end, information entropy is a number: to understand it you need to understand what that number quantifies.

\end{document}
