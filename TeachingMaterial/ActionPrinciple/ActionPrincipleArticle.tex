\documentclass[10pt,twocolumn, nofootinbib]{revtex4-2}
%\documentclass[aps,pra,10pt,twocolumn,floatfix,nofootinbib]{revtex4-1}
%\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{assumptionsofphysics}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	citecolor=blue,
	urlcolor=blue,
	linkcolor=blue
}
\urlstyle{same}
\frenchspacing

\newcommand\partitle[1]{\textsc{#1}.}


\begin{document}

\title{Geometrical and physical interpretation of the action principle}
\author{Gabriele Carcassi, Christine A. Aidala}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}

\date{\today}


\begin{abstract}
We give a geometrical interpretation for the principle of stationary action in classical Lagrangian particle mechanics. In a nutshell, the difference of the action along a path and its variation effectively ``counts'' the possible evolutions that ``go through'' the area enclosed. If the path corresponds to a possible evolution, all neighbouring evolutions will be parallel, making them tangent to the area enclosed by the path and its variation, thus yielding a stationary action. This treatment gives a full physical account of the geometry of both Hamiltonian and Lagrangian mechanics which is founded on three assumptions: determinism and reversible evolution, independence of the degrees of freedom and equivalence between kinematics and dynamics. The logical equivalence between the three assumptions and the principle of stationary action leads to a much cleaner conceptual understanding.
\end{abstract}

\maketitle

\section{Introduction}

While the principle of stationary action is regarded by many as one of the most important tools in physics, its physical meaning is not completely clear. First of all, the typical characterization of the Lagrangian as the difference between kinetic and potential energy fails even for simple systems, like a charged particle under a magnetic field. Moreover, the Lagrangian for a system is not uniquely defined, making the actual value of the action for a path not directly physically significant.  We are left to wonder: what exactly is the action and why is it stationary for actual trajectories?

As part of our larger project Assumptions of Physics, we developed an approach, called Reverse Physics, which examines current theories to find a set of starting physical assumptions that are sufficient to rederive them. This gives a more physically motivated grounding of the classical theory and the proper setting to characterize both the physics and the geometry underlying the principle of stationary action. The assumption of determinism and reversible evolution corresponds to a diverge-free flow of the states, which preserves volumes in phase space (i.e. the count of states). The assumption of independence of the degrees of freedom means the volume reduces to the product of areas in each d.o.f., which is quantified by a closed two-form. The principle of stationary action arises as a feature of divergence-free fields and closed two-forms, as a general mathematical property of vector calculus and differential geometry\cite{souriau1997structure}. The additional assumption of equivalence between kinematics and dynamics (i.e. we can reconstruct the dynamical state simply by looking at the trajectory) is what allows us to express the principle in Lagrangian form. The argument can proceed in the reverse direction: assuming the principle of stationary action recovers a dynamical system that exhibit those three physical assumptions.

The mathematics needed to run the argument is not new, yet it is typically presented in a way and in a notation that is opaque to physicists. To provide a clear insight to the widest possible readership, we will first address the case of a single degree of freedom using standard vector calculus. This provides already all the geometrical and physical insights in a language that is accessible to all physicists and engineers. Then we proceed to the case of multiple independent degrees of freedom, which requires tools from differential geometry. Here we will use a notation that is similar to the one used in general relativity. We will concentrate mainly on the geometrical and physical meaning of the mathematical objects and their relationships, leaving the full argument and calculations in the appendix.

\section{One degree of freedom}

\begin{figure}
	\includegraphics[width = 0.45\textwidth]{ExtendedPhaseSpace}
	\caption{\footnotesize{Evolutions in the extended phase space and the divergence-free displacement field.}}\label{extended_phase_space}
\end{figure}

As we want to characterize the evolution of states over time, the appropriate setting is phase space extended with the time variable. That is, the space charted by position $q$, momentum $p$ and time $t$ as can be seen in Fig. \ref{extended_phase_space}. In the same way that we write $x^i = \{ x, y, z \}$ for the three dimension of space, we write
\begin{equation}\label{sdof_variables}
	\xi^a = \{ q, p, t\}
\end{equation}
for the three dimensions of the extended phase space.

Under the assumption that
\begin{align}\label{assum_detrev}
	\tag*{(DR)}
	\parbox{2.8in}{the system undergoes deterministic and reversible evolution}
\end{align}
we can define a displacement vector field
\begin{equation}\label{sdof_displacement}
\begin{aligned}
	\vec{S} &= \left\{ \frac{dq}{dt},\frac{dp}{dt},\frac{dt}{dt} \right\} \\
	&= S^a e_a = \frac{d\xi^a}{dt} e_a .
\end{aligned}
\end{equation}
that tells how states move in time.\footnote{Where possible, we will be writing the same expression in both vector calculus and component notations.} The time component of the displacement vector field is constrained as we have
\begin{equation}\label{sdof_time_constraint}
	S^t=\frac{dt}{dt}=1.
\end{equation}

If assumption \ref{assum_detrev} is valid, we expect the flow of states through a closed surface to be zero: as many states flow in and out of the region. Alternatively, if we assign a probability, or probability density, to each trajectory, the assumptions requires that probability to not change, so integrating the probability over a closed surface must yield zero. However we see it, assumption \ref{assum_detrev} means the field is divergence-free,\footnote{Given that this is a three dimensional space, we can use the standard tools of vector calculus.} that is
\begin{equation}\label{sdof_div_free}
	\nabla \cdot \vec{S} = \partial_a S^a = 0.
\end{equation}

Since the displacement field is divergence-free, it admits a vector potential. We have
\begin{equation}\label{sdof_displacement_potential}
\begin{aligned}
	\vec{\theta} &= \{\theta_q, \theta_p, \theta_t\} \\
	&= \theta_a e^a \\
	\vec{S} &= - \nabla \times \vec{\theta} \\
	&= - \epsilon^{abc} \partial_b \theta_c \, e_a. \\
\end{aligned}
\end{equation}
The minus sign is introduced to match conventions in differential geometry. Mathematically, this is analogous to what is done for a magnetic field or for an incompressible fluid.

Because the displacement field must satisfy \ref{sdof_time_constraint}, without loss of generality we can set
\begin{equation}\label{sdof_potential_expression}
\begin{aligned}
	\vec{\theta} &= \{p, 0, -H\} \\
	&= p e^q - H e^t.
\end{aligned}
\end{equation}
By applying definition \ref{sdof_displacement} and expanding \ref{sdof_displacement_potential} with \ref{sdof_potential_expression}, we have
\begin{equation}\label{sdof_Ham_eq}
	\left\{ \frac{dq}{dt},\frac{dp}{dt},\frac{dt}{dt} \right\} = - \nabla \times \vec{\theta} = \left\{ \frac{\partial H}{\partial p},-\frac{\partial H}{\partial q}, 1 \right\},
\end{equation}
which yields Hamilton's equations. Note that the arguments works in reverse: any Hamiltonian system with one degree of freedom yields a divergence-free displacement field.

\begin{figure}
	a \includegraphics[width = 0.45\textwidth]{ActionNonOptimized.png} \\
	b \includegraphics[width = 0.45\textwidth]{ActionOptimized.png}
	\caption{\footnotesize{The variation of the action is the flow of the displacement field $\vec{S}$ through the surface $\Sigma$ that sits between the path $\gamma$ and its variation $\gamma'$. In the second picture we see that the flow is zero if the path is an actual evolution of the system, since the displacement field will be parallel to the path $\gamma$ and therefore the surface $\Sigma$.}}\label{fig_action}
\end{figure}

Now that we are recast Hamiltonian mechanics using vector calculus, let us start constructing the principle of stationary action. As illustrated in Fig. \ref{fig_action}a, take a path $\gamma$ with endpoints $A$ and $B$, not necessarily a solution of the equations of motion. Then take a variation $\gamma'$ of that path and identify a surface $\Sigma$ between them. We can ask: what is the flow of the displacement field $\vec{S}$ through $\Sigma$? Because $\vec{S}$ is divergence-free, the flow through $\Sigma$ will depend only on the contour, therefore the question is well posed. We find
\begin{equation}\label{sdof_action}
\begin{aligned}
	- \iint_{\Sigma} \vec{S} \cdot d\vec{\Sigma} &= \iint_{\Sigma} \left( \nabla \times \vec{\theta} \right) \cdot d\vec{\Sigma} \\
	=  \oint_{\partial \Sigma} \vec{\theta}  \cdot d\vec{\gamma} 
	&= \delta \int_{\gamma} \vec{\theta} \cdot d\vec{\gamma}.
\end{aligned}
\end{equation}

Now suppose $\gamma$ is a solution of the equation of motion, as in Fig. \ref{fig_action}b. Then $\gamma$ is a field line and the flow is tangent to $\Sigma$ no matter what $\gamma'$ we picked. The converse is true: if we look for those paths for which the flow through $\Sigma$ is zero no matter what $\gamma'$, we find a solution to the equation of motion. The solutions, then, are those paths and only those paths for which
\begin{equation}\label{sdof_stationary_action}
	\begin{aligned}
		0 &=\delta \int_{\gamma} \vec{\theta} \cdot d\vec{\gamma} \\
		&= \delta \int^{t_2}_{t_1} \vec{\theta} \cdot \frac{d\vec{\gamma}}{dt} dt \\  
		&= \delta \int^{t_2}_{t_1} p \frac{dq}{dt} - H dt .		
	\end{aligned}
\end{equation}

Lastly we need to be able to express conjugate momentum in terms of the velocity. This can be done if we assume that 
\begin{align}\label{assum_kineq}
	\tag*{(KE)}
	\parbox{2.8in}{the kinematic of the system is enough to reconstruct its dynamics.}
\end{align}
This means that by looking at just the trajectory in space $q(t)$, we are able to reconstruct the state at each moment in time. Therefore we must be able to write $p=p(q,\dot{q})$, and therefore we can also write
\begin{equation}\label{sdof_Lagrangian}
	L(q, \dot{q}) = p(q, \dot{q}) \frac{dq}{dt} - H(q, p(q, \dot{q})).
\end{equation}

We find that a system for which \ref{assum_detrev} and \ref{assum_kineq} are valid can be characterized in terms of the action principle with a suitable Lagrangian. The converse is also true: if the action principle allows for a unique solution, then a Hamiltonian can be written and therefore the system satisfies both \ref{assum_detrev} and \ref{assum_kineq}.

We have thus demistified the action principle, and turned it into a geometrical property: requiring the principle of stationary action is equivalent to requiring that the solutions are the field lines of a divergence-free field. We also have a clear physical meaning: the principle of stationary action is equivalent to assuming determinism/reversibility \ref{assum_detrev} and kinematic equivalence \ref{assum_kineq}. However, we do feel that the principle expresses these requirements in a very roundabout way.

\section{Multiple degrees of freedom}

To generalize to multiple degree of freedom we have to abandon the tools provided by vector calculus and embrace the ones provided by differential geometry. For $N$ independent degrees of freedom, we will have the $2N+1$ variables
\begin{equation}\label{mdof_variables}
	\xi^a = \{ q^i, p_i, t\}.
\end{equation}
The displacement vector field will be
\begin{equation}\label{mdof_displacement}
	\begin{aligned}
		\vec{S} &= S^a e_a = \frac{d\xi^a}{dt} e_a =\frac{dq^i}{dt} e_{q^i} + \frac{dp_i}{dt} e_{p_i} + \frac{dt}{dt} e_t
	\end{aligned}
\end{equation}
which also satisfies \ref{sdof_time_constraint}.

The flow of $\vec{S}$ will still be divergence-free, but this property only tells us that the total number of states is conserved. We also assumed that
\begin{align}\label{assum_indep}
	\tag*{(IND)}
	\parbox{2.8in}{the degrees of freedom are independent.}
\end{align}
This means that, at each moment in time, the total number of states (i.e. the volume) of a parallelopided will be the product of states identified by each degree of freedom (i.e. the areas). To be able to quantify the number of states identified by each degree of freedom, we introduce the rank-2 tensor (more precisely a two-form)
\begin{equation}\label{mdof_form}
	\omega = \omega_{ab} \, e^a \otimes e^b,
\end{equation}
which we call the state counting form.\footnote{Note that this approach is consistent with statistical mechanics.}

The counting form must be of rank two as independent degree of freedom are bi-dimensional (i.e. quantity plus conjugate). Given two vectors $\vec{v}$ and $\vec{w}$, these identify a parallelogram in phase space and 
\begin{equation}\label{mdof_form_applied}
	\omega(\vec{v}, \vec{w}) = \omega_{ab} v^a w^b
\end{equation}
quantifies the number of states over its surface.

The form $\omega$ will need to be anti-symmetric
\begin{equation}\label{mdof_form_antisymm}
	\omega(\vec{v}, \vec{w}) = - \omega(\vec{w}, \vec{v}).
\end{equation}
as the parallelogram identified by $\vec{v}$ and $\vec{w}$ in that order will be the same as the one identified by $\vec{w}$ and $\vec{v}$ with opposite orientation. The form $\omega$ will also be closed, meaning\footnote{In our notation, $d\Sigma$ represents the infinitesimal surface element of integration, which is the argument of the form.}
\begin{equation}\label{mdof_closed_form}
	\oiint_\Sigma \omega(d\Sigma) = 0
\end{equation}
over all closed contractible surfaces. This stems from \ref{assum_indep}. Suppose you take a surface and translate it along another independent variable. The count of states changed along the translation cannot depend on the independent variable. Therefore if we construct any parallelepiped at equal time, two opposite sides will contain the same number of states with opposite orientation, and therefore the integral of $\omega$ over the whole surface would be zero.

Since $\omega$ is closed, in every contractible region it can be expressed as the exterior derivative\footnote{We write the exterior derivative as $\partial \omega$ instead of $d \omega$ so that $d$ is only used for infinitesimal line or surface elements.} of a covector $\theta$:
\begin{equation}\label{mdof_form_potential}
	\begin{aligned}
		\theta &= \theta_a e^a = \theta_{q^i} e^{q^i} + \theta_{p_i} e^{p_i} + \theta_t e^t \\
		\omega &= - \partial \theta = - \left( \partial_a \theta_b - \partial_b \theta_a \right) e^a \otimes e^b
	\end{aligned}
\end{equation}
The minus sign is the convention in symplectic geometry.

The above is the generalization of \ref{sdof_displacement_potential} and it is important to note the differences. The exterior derivative takes the place of the curl and it is $\omega$, not $\vec{S}$, that can be expressed in terms of the potential $\theta$. Yet, there is an important geometrical relationship we still need to take into account: the direction of the displacement does not contribute new states. Physically, this is a consequence of \ref{assum_detrev}: states cannot be created or disappear over time. There we have
\begin{align}\label{mdof_displacement_kills}
	\omega(\vec{S}, \cdot) = 0.
\end{align}
Mathematically we say that the displacement vector kills the form, it makes it zero no matter what vector we put on the other side.

Similarly to the single d.o.f., without loss of generalize we can set
\begin{equation}\label{mdof_potential_expression}
\begin{aligned}
	\theta &= \{p_i, 0, -H\} \\
	&= p_i e^{q^i} - H e^t.
\end{aligned}
\end{equation}
and calculate the components of the counting form
\begin{equation}\label{mdof_form_components}
	\omega_{ab} = (\partial\theta)_{ab} = \begin{bmatrix}
		0 & \delta^j_i & - \partial_{q^i} H \\
		-\delta^i_j & 0 & - \partial_{p_i} H \\
		\partial_{q^j} H & \partial_{p_j} H & 0
	\end{bmatrix}.
\end{equation}

By combining \ref{mdof_displacement}, \ref{mdof_displacement_kills} and \ref{mdof_form_components} one finds
\begin{equation}\label{mdof_Ham_eq}
\begin{aligned}
	S^{p_j} &= \partial_{q^j} H \\
	S^{q^j} &= - \partial_{p_j} H
\end{aligned}
\end{equation}
which recovers Hamilton's equations.

The setup to recover the action principle is the same, except that we need to substitute vector calculus with differential geometry. First we find that the flow through $\Sigma$ is the variation of the action using Stokes' theorem
\begin{equation}\label{mdof_action}
	\begin{aligned}
		- \iint_{\Sigma} \omega(d\Sigma) &= \delta \int_{\gamma} \theta(d\gamma) \\
		&= \delta \int^{t_2}_{t_1} p_i \frac{dq_i}{dt} - H dt .
	\end{aligned}
\end{equation}
If $\gamma$ is a solution of the equations of motions, then $\frac{d\gamma}{dt} = \vec{S}$. By \ref{mdof_displacement_kills}, $\vec{S}$ is the only vector field that kill the form $\omega$ therefore
\begin{equation}\label{mdof_stationary_action}
	\begin{aligned}
		- \iint_{\Sigma} \omega(d\Sigma) &= \iint_{\Sigma} \omega(\vec{S}, d\lambda) dt = 0
	\end{aligned}
\end{equation}
only for the solutions. This recovers the action principle for multiple degrees of freedoms.

\section{Discussion and conclusions}

We have found that the principle of stationary action is equivalent to assumptions \ref{assum_detrev}, \ref{assum_indep} and \ref{assum_kineq} and that the variation of the action can be understood as the flow of evolutions through the surface delimited by the path and its variation. This allows us to fully understand both the physical and geometrical significance of the principle in a much more precise way.

Note that the true physical content is in the displacement field $\vec{S}$ and the counting form $\omega$ whose properties descend from the assumption. Both the Lagrangian and the potential $\theta$ are not uniquely defined and are subject to a convenient, though still arbitrary, choice of gauge. The Lagrangian and the action, then, do not directly encode physical property of the system. Saying that ``nature chooses the path that minimizes the action'', therefore, is not really that insightful because such a quantity is not even uniquely defined. On the other hand, the three equivalent assumptions give us a much more clear picture.

Another interesting element is that a version of the principle of stationary actions holds even if \ref{assum_kineq} does not apply. In this case the ``Lagrangian'' $L(q,\dot{q},p)$ depends on conjugate momentum as well. Also note that the cases where the Lagrangian does not admit a Hamiltonian are exactly the cases where the action principle fails to yield a unique solution. In this sense, the action principle is better understood as a feature of Hamiltonian mechanics, instead of Lagrangian mechanics.

As a final comment, note that privileging the Hamiltonian picture is more in line with quantum mechanics; the idea of $\omega$ as a tool to count states is in line with statistical mechanics; the expression \ref{mdof_potential_expression} is remarkably similar to that of the relativistic four-momentum. This is one of the key insights we get from our Reverse Physics approach: there is a unity among the different physical theories that begs to be brought to light. We are convinced that a version of this geometrical understanding must exist in the quantum world.

\bibliography{bibliography}


\section*{Appendix}

In this section we will present the arguments in full, including some important mathematical details. Given that this article is still aimed at a physics audience, rather than on mathematical rigor, will be on a precise map between physical concepts and their mathematical representation. We divide the appendix into three section: the first will address the single d.o.f. case, the second will give a geometric interpretation of the tools of differential geometry that is suitable for physicists, and lastly we address the multiple d.o.f. case.

\subsection*{One degree of freedom}

For the case of one degree of freedom we are going to use vector calculus. We can because the problem can be studied with three variables (position, momentum and time), which is the only setting where vector calculus truly works. In this sense, nature really gave us a hand as three dimensional spaces are much more intuitive for us, therefore we can develop the ideas in a familiar context before generalizing.

We break down the problem into sequential steps, which serves as a summary for the whole argument. We will have to:
\begin{enumerate}[label=(\roman*)]
	\item show that assuming determinism and reversibility \ref{assum_detrev} is equivalent to assuming that the displacement field $\vec{S}$ defined in \ref{sdof_displacement} exists and is divergence-free as in \ref{sdof_div_free}
	\item show that the vector potential $\vec{\theta}$ for $\vec{S}$ defined in \ref{sdof_displacement_potential} can be expressed, without loss of generality, as \ref{sdof_potential_expression}
	\item show that \ref{sdof_div_free} and \ref{sdof_potential_expression} recover Hamilton's equations as in \ref{sdof_Ham_eq}
	\item show that the flow of $\vec{S}$ through a surface $\Sigma$ delimited by a path $\gamma$ and its variation $\gamma'$ is the variation of the integral of the vector potential, as claimed in \ref{sdof_action}
	\item show that the action is stationary for and only for the solutions of the equations of motion
	\item show that the action can be expressed as functions of position, velocity and time if and only if \ref{assum_kineq} applies.
\end{enumerate}

For (i), determinism and reversibility means that giving the state at some point in time means constraining the state at all times, that the system can evolve in only one way. In math terms, if $X$ is the manifold of all possible states at times, given $P = \{q_0, p_0, t_0\}$ there will be only one possible evolution $\gamma : \mathbb{R} \to X$ such that $\gamma(t_0) = P$.

If $X$ were in a space, the previous mathematical condition would be sufficient, but on a manifold it is not. Consider a damped harmonic oscillator: it satisfies the previous condition, yet the system will concentrate at the equilibrium. What needs to happen is that densities also need to be mapped. That is, if a certain fraction of the system is in a given initial state, the same fraction of the system must be in the corresponding final state. Equivalently, if the system has a given probability of starting in a given initial state, it must have the same probability of ending in the corresponding final state. In mathematical terms, if $\rho : X \to \mathbb{R}$ is a statistical distribution (or a probability density) defined on our extended space time and $\gamma$ is a possible evolution, we must have $\rho(\gamma(t_0)) = \rho(\gamma(t_1))$.

This requirement tells us two things. First, is that $X$ must be a differentiable manifold. To define densities over a set of variables and map them back and forward in time Jacobians must be defined, which means the derivative across state variables must be defined as well, in particular with respect with time, which plays the double role of a coordinate of $X$ and the evolution parameter. This means that the evolutions must be differentiable curves and therefore $\vec{S}$ must be defined which leads to \ref{sdof_displacement}. The double role of time gives us \ref{sdof_time_constraint}.

Secondly, since the densities must be constant over evolutions, the flow of these densities through a closed surface in $X$ must be zero: each evolution will enter and exit the region delimited by the surface, giving no net contribution. Mathematically this requires the displacement to be divergence-free, which leads to \ref{sdof_div_free}.

For (ii), since $\vec{S}$ is divergence-free, we can find a vector potential as shown in \ref{sdof_displacement_potential}. Note that the vector potential is not uniquely identified, since $\nabla \times(\vec{\theta} + \nabla f) = \nabla \times \vec{\theta}$ and therefore the displacement field $\vec{S}$ remains unchanged. This is what is commonly referred to as gauge freedom in physics, and it is analogous to what happens for a magnetic field or for an incompressible fluid.

We use this gauge freedom to set the momentum component to zero. In fact, we can always choose $f(q,p,t)$ such that $\partial_p f = -\theta_p$ and redefine $\vec{\theta}$ to be $\vec{\theta} + \nabla f$. Therefore we can set
\begin{equation*}
	\vec{\theta} = \{\theta_q, 0, \theta_t\}
\end{equation*}
without loss of generality.

We now use the constraint \ref{sdof_time_constraint} on the time component. We have
\begin{align*}
	S_t = \frac{dt}{dt} &= - \left(\frac{\partial}{\partial q}  \theta_p - \frac{\partial}{\partial p}  \theta_q\right) \\
	&= - \left(\frac{\partial}{\partial q}  0 - \frac{\partial}{\partial p}  \theta_q\right) \\
	& = \frac{\partial \theta_q}{\partial p} = 1
\end{align*}
Integrating, we have $\theta_q = p + g(q,t)$ where $g(q,t)$ is an arbitrary function. This function can be set to zero without loss of generality as it can be removed with a gauge transformation where $f(q,t)$ does not depend on $p$, and therefore $\theta_p$ will remain unchanged. Therefore we have:
\begin{equation*}
	\vec{\theta} = \{p, 0, \theta_t\}
\end{equation*}
Lastly, we rename the last component as $\theta_t = -H$ which leads to 
\begin{equation*}
	\vec{\theta} = \{p, 0, -H\}
\end{equation*}
and recovers \ref{sdof_potential_expression}.

For (iii), we expand each component of \ref{sdof_displacement_potential} using \ref{sdof_potential_expression}. Again note the double role of time: $\frac{dp}{dt}$ is not the same as $\frac{\partial p}{\partial t}$.\footnote{We could be more precise and use $\tau$ for the evolution parameter, leaving $t$ just for the coordinate. The treatment would automatically acquire a relativistic flavor and would enlighten us to other interesting results, though it would distract from the main purpose of this paper, the action principle. } In the first case we are taking a total derivative along the evolution, and therefore the momentum can change. In the second case we are taking a partial derivative at constant $q$ and $p$ by definition, and therefore $\frac{\partial p}{\partial t}=0$. We have
\begin{align*}
	S_q = \frac{dq}{dt}
	&= - \left( \frac{\partial}{\partial p} \theta_t - \frac{\partial}{\partial t} \theta_p \right) \\
	&= - \left( \frac{\partial}{\partial p} (-H) - \frac{\partial}{\partial t} 0 \right) \\
	& = \frac{\partial H}{\partial p}
\end{align*}
\begin{align*}
	S_p = \frac{dp}{dt}
	&= - \left( \frac{\partial}{\partial t} \theta_q - \frac{\partial}{\partial q} \theta_t \right) \\
	&= - \left( \frac{\partial}{\partial t} p - \frac{\partial}{\partial q} (-H) \right) \\
	& = - \frac{\partial H}{\partial q}
\end{align*}
\begin{align*}
	S_t = \frac{dt}{dt}
	&= - \left( \frac{\partial}{\partial q} \theta_p - \frac{\partial}{\partial p} \theta_q \right) \\
	&= - \left( \frac{\partial}{\partial q} 0 - \frac{\partial}{\partial p} p \right) \\
	& = 1
\end{align*}
We have recovered Hamilton's equations \ref{sdof_Ham_eq} as the equations for a deterministic and reversible system. The only thing that the equations say is that states move in time at the same rate (i.e. $\frac{dt}{dt} = 1$) with an incompressible flow (i.e. no states are created or destroyed). That is the whole physical and geometrical content of those equations.

Note that if we started from Hamilton's equation, the system would be deterministic and reversible: position and momentum at a fixed time would indeed allow us to pick a single differentiable evolution and, by Liouville's theorem, densities would be constant along such evolution. Therefore we have an equivalence between assumption $\ref{assum_detrev}$ and Hamilton's equations $\ref{sdof_Ham_eq}$ in the single degree of freedom case.

Now that we have developed a clear geometrical and physical understanding of Hamilton's equations, we can concentrate on the action principle. For (iv) we can apply Stokes' theorem. We take a path $\gamma$ and a variation $\gamma'$. Since they share the endpoint, taken together they form a closed line where $\gamma'$ is taken with the opposite orientation. Because $\vec{S}$ is divergence-free, the flow of $\vec{S}$ through a surface depends only on its contour. Moreover, by Stoke's theorem, the flow through the surface will be equal to the integral of the potential over the contour. Putting it all together we have:
\begin{align*}
	- \iint_{\Sigma} \vec{S} \cdot d\vec{\Sigma} &=
	\iint_{\Sigma} \left( \nabla \times \vec{\theta} \right) \cdot d\vec{\Sigma} \\
	&= \oint_{\partial \Sigma} \vec{\theta}  \cdot d\vec{\gamma} \\
	&= \int_{\gamma} \vec{\theta} \cdot d\vec{\gamma} - \int_{\gamma'} \vec{\theta} \cdot d\vec{\gamma'} \\
	&= \delta \int_{\gamma} \vec{\theta} \cdot d\vec{\gamma}.
\end{align*}
This recovers \ref{sdof_action}.

For (v), note that in the neighborhood of $\gamma$ the infinitesimal surface element $d\vec{Sigma}$ has one side along $\gamma$ itself. Therefore we can write $d\vec{Sigma} = d\vec{\gamma} \times d\vec{\lambda}$, where $d\vec{\lambda}$ is the infinitesimal displacement that connects $\gamma$ with $\gamma'$. The integrand then becomes the triple product
\begin{equation*}
	\vec{S} \cdot d\vec{\gamma} \times d\vec{\lambda}.
\end{equation*}
We can understand it geometrically as the volume element given by the displacement $\vec{S}$, the infinitesimal line element $d\vec{\gamma}$ and the infinitesimal variation $d\vec{\lambda}$ that moves $\gamma$ to $\gamma'$.

If $\gamma$ is an actual solution of the equation of motion, $\gamma$ is a field line of $\vec{S}$: $\gamma$ is tangent to $\vec{S}$ at all points. We can write $d\vec{\gamma} = \vec{S} dt$. The triple product, in this case, is zero regardless of what $d\vec{\lambda}$ is. We can reason the other way: we look for a line $\gamma$ such that the triple product is zero regardless of what $d\vec{\lambda}$ is. The only way for that to happen is if $\gamma$ is tangent to $\vec{S}$ at each point, which means $\gamma$ is a solution of the equation of motion. This means that the paths that make the line integral of the vector potential stationary are those and only those that solve the equations of motions. This recovers the principle of the stationary action in Hamiltonian form.

We want to stress that this variational principle applies to any divergence-free field. For example, for a magnetic field we could write:
\begin{equation*}
	\iint_{\Sigma} \vec{B} \cdot d\vec{\Sigma} = \delta \int_{\gamma} \vec{A} \cdot d\vec{\gamma}.
\end{equation*}
We would find that the field lines of $\vec{B}$ are the lines and only the lines for which the integral of the vector potential is stationary. Similarly, we would find that the streamlines of an incompressible fluids are the lines and only the lines for which the integral of the vector potential of the flow is stationary. While these properties may be of limited practical use, the geometrical insight is the same.

For (vi), if the assumption \ref{assum_kineq} of kinematic equivalence  holds, we can from a trajectory in space $q(t)$ reconstruct the state at any time. That is, if we are given $q(t)$ we can reconstruct the full evolution $\gamma(t)$ in the extended phase space.

We saw before that the evolution must be differentiable, which means $q(t)$ must also be differentiable. Therefore the velocity $\dot{q} = \frac{dq}{dt}$ is defined at every instant. Using assumption \ref{assum_detrev}, we know that each evolution is fully identified by position and momentum at a given time. Therefore the space of all possible evolutions is two-dimensional. Given that for each evolution $\gamma(t)$ there is one and only one spatial trajectory $q(t)$, the space of all spatial trajectories must also be two-dimensional and therefore fully identified by position and velocity at a given instant. Therefore there must be a map $p(q, \dot{q}, t)$ that allows us to reconstruct the momentum at a given time by knowing position and velocity at that time.

We can use this map to express the principle of stationary action only in terms of position and velocity we have
\begin{equation}
	\begin{aligned}
		\delta \int_\gamma \vec{\theta} \cdot d\vec{\gamma} &= \delta \int^{t_2}_{t_1} \vec{\theta} \cdot \frac{d\vec{\gamma}}{dt} dt \\
		&= \delta \int^{t_2}_{t_1} \left(\theta_q \frac{dq}{dt} + \theta_p \frac{dp}{dt} + \theta_t \frac{dt}{dt}\right) dt \\
		&= \delta\int^{t_2}_{t_1} \left(p \frac{dq}{dt} + 0 \frac{dp}{dt} - H \frac{dt}{dt}\right) dt \\
		&= \delta \int^{t_2}_{t_1} \left(p \frac{dq}{dt} - H\right) dt \\
		&= \delta \int^{t_2}_{t_1} L(q, \dot{q}, t) \, dt
	\end{aligned}
\end{equation}
We recognize the principle of stationary action in Lagrangian form.

Note that the argument can proceed in the opposite way. If we have a system for which the principle of stationary action yields a unique solution, then we can write $p=\frac{\partial L}{\partial \dot{q}} = p(q, \dot{q}, t)$ which means we are able to reconstruct the momentum, and therefore the state, at each time. The system satisfies \ref{assum_kineq}. Moreover, we can find a unique Hamiltonian such that the solution of the equations of motion are exactly those paths that satisfy the principle of stationary actions. Therefore the system satisfies \ref{assum_detrev} as well.

Before proceed to the generalizations, a few observations are in order.
\begin{itemize}
	\item We stress that the vector potential $\vec{\theta}$ in not uniquely defined by the physics, and therefore neither are the action and the Lagrangian. Giving a precise physical characterizations of those two objects, then, is hard. In our view, it is a hopeless task.
	\item We found that the Hamiltonian $H$ is the time component of the vector potential $\vec{\theta}$, not a scalar. This relativistic aspect emerges naturally out of the assumptions.
	\item One can, in principle, write Lagrangian systems for which the action principle does not yield a single solution (e.g. for $L=0$ all paths are stationary). Consistently, assumption \ref{assum_detrev} fails.
\end{itemize}

\subsection*{Introduction to differential topology}

The generalization to multiple d.o.f will require the use of differential topology. These tools are not typically taught in a standard physics curriculum. If they are, the treatment is the dense and abstract one coming from mathematical tradition, which we have found to be arcane even to some mathematicians: at a topology conference, a math graduate student confessed he sticked with topology because differential geometry was too abstract for him, comment that was taken with sympathy by others.

Apart from the abstract definitions, the notation gets in the way of the physics. For example, writing $\frac{\partial}{\partial x^i}$ for the vector basis makes sense if one is studying the space of linear operators defined by derivations. When studying the space of velocities, however, writing $\vec{v} = v^i\frac{\partial}{\partial x^i}$ decreases understanding. Therefore we will use a notation and nomenclature that is closer to what physicists already use (e.g. Einstein notation) and need to picture. For example, we will use $e_i$ for vector basis and $e^i$ for covector (dual) basis. To those that believe we should just use the ``correct'' mathematical nomenclature, we retort that there is no such thing. Notations and names are chosen within a certain context to make the points of the discourse more clear.

What we will give here, then, is not a fully rigorous introduction to the subject, but rather a geometrical understanding of what these tool describe and a physical motivation as to why they are useful. We hope this will make it easier to follow the next section and to build a geometrical and physical intuition of the principle of stationary actions, which is the whole point of this article.

%This very result, and other connected to it, became apparent to us precisely because we started to translate the language of differential geometry into a form that is less abstract. These results have been in front of our known noses, hidden in plain sight by a notation, useful to some mathematicians, but arcane to others. While we don't claim our notation is ``the best'', it is better for our purposes than the standard ones. We understand that there is a cost associated to learning a new notation, but to us that cost is essentially learning exactly why the tools are useful and what they represent in physics. It is worth the long term investment


%In our opinion, , and In order to fully understand the geometrical and physical meaning of the general case, it helps to have a crisp intuition for the tools of differential topology. We will therefore go through the basic motivation of differential topology from a physicist perspective, so that we can see what differential forms and exterior derivatives are capturing at a a physical level.

To boil it done, the objective is to characterize those quantities that are associated with a region of space and can be understood as sums of independent contributions. The mass $m$ within a region $V$ can be understood as the sum of the contributions of a mass density $\rho$ within each infinitesimal volume $dV$:
\begin{equation*}
	m(V) = \iiint_V \rho dV.
\end{equation*}
The magnetic flux $\Phi$ through a surface $\Sigma$ can be understood as the sum of the contributions of the magnetic field $\vec{B}$ over each infinitesimal $d\Sigma$:
\begin{equation*}
	\Phi(\Sigma) = \iint_\Sigma \vec{B} \cdot \vec{d\Sigma}.
\end{equation*}
The work $W$ over a path $\lambda$ can be understood as the sum of the contributions of a force field $\vec{f}$ over each infinitesimal line $d\gamma$:
\begin{equation*}
	W(\gamma) = \int_\gamma \vec{f} \cdot \vec{d\gamma}.
\end{equation*}

Though the dimensionality changes, a pattern emerges: the functionals $W$, $\Phi$ and $m$ all take a region of space while $\vec{f}$, $\vec{B}$ and $\rho$ act on infinitesimal regions. However, the vector representation gives us three different enough objects (i.e. vector, pseudo-vector and pseudo-scalar) and operations, and therefore fails fail to provide a nice generalization.

Suppose we wrote
\begin{equation*}
\begin{aligned}
	W(\gamma) &= \int_\gamma f(d\gamma) \\
	\Phi(\Sigma) &= \iint_\Sigma B(d\Sigma) \\
	m(V) &= \iiint_V \rho(dV).
\end{aligned}
\end{equation*}
Here we don't have vectors, but functions of infinitesimal regions, which we call differential forms. The force, in this notation, is a one-form (or covector) as it takes one dimensional infinitesimal regions (i.e. vectors); the magnetic field is a two-form; the density is a three-form. We can also say that a scalar field, like the temperature, is a zero-form.

An infinitesimal region can also be understood as a parallelopiped, which is fully identified by its sides. Therefore a differential form can be understood as acting on a set of vectors that matches the dimensionality of the form. A one-form will take one vector, a two-form two vectors and so on. All forms must be anti-symmetric because switching the order of the sides does not change the parallelopiped, just its orientation.

This picture is much easier to generalize to any dimensionality. If we note $S^k$ the space of all the $k$ dimensional subregions, a $k$-functional $F : S^k \to \mathbb{R}$ is a linear functional that takes a $k$-surface and returns a number. This can be expressed as
\begin{equation*}
	F(\sigma) = \int_\sigma \omega(d\sigma)
\end{equation*}
the integral of a $k$-form $\omega : V^k \to \mathbb{R}$ that takes an infinitesimal $k$-surfaces (or equivalently a set of $k$ vectors) and returns a number.

This allows us to identify properties and theorems that apply to all functionals and forms regardless of their dimensionality. For example, if $\sigma_1$ and $\sigma_2$ are two disjoint regions, $F(\sigma_1 \cup \sigma_2) = F(\sigma_1) + F(\sigma_2)$, which also tells us $F(\emptyset) = 0$ for any functional. Additionally, $k$-forms are linear maps and can be written in terms of the tensor product of the co-basis $e^a$. We have:
\begin{equation*}
	\begin{aligned}
		f &= f_a e^a \\
		B &= B_{ab} e^a \otimes e^b \\
		\rho &= \rho_{abc} e^a \otimes e^b \otimes e^c.
	\end{aligned}
\end{equation*}
These tensor are all anti-symmetric as the forms are anti-symmetric.


%If $F_k$ is the set of all linear $k$-functionals and $\Omega_k$ the set of all linear $k$-forms, we can understand $\Omega_k$ as the infiniteimsal verson of $F_k$.



%of one, two or three dimensions

%We say that $m$ is a functional that takes a three dimensional region and returns a number. This functional is linear, in the sense that it respects the limit and it is additive over disjoint regions. That is, $m(V_1 \cup V_2) = m(V_1) + m(V_2)$ if $V_1 \cup V_2 = \emptyset$.





%Since $k$-form are also linear maps, they form a vector space and can be written in terms of a basis. In the same way we have a basis $e_a$ for vectors, we will have a basis $e^a$ for one-forms (or covectors) such that $e^a(e_b) = \delta^a_b$. We have:
%Additionally, all forms are anti-symmetric in all indexes: a changing the order of the vectors reduces to a change of the orientation. Mathematically, this is why one introduces the wedge product $\wedge$ which is an anti-symmetrized version of the tensor product. Given that physicists are already familiar with the idea of anti-symmetric tensors, we think it's better to not introduce the wedge product and work in terms of tensors.

We can see how this works for a force applied along a path. The infinitesimal displacement along $\gamma$ is given by
\begin{equation*}
	d\gamma = dx^a e_a.
\end{equation*}
Therefore 
\begin{equation*}
	\begin{aligned}
		W(\gamma) &= \int_\gamma f(d\gamma) = \int_\gamma  f_a e^a(dx^b e_b) \\
		&= \int_\gamma  f_a dx^b e^a( e_b)
		= \int_\gamma  f_a dx^b \delta^a_b
		= \int_\gamma  f_a dx^a.
	\end{aligned}
\end{equation*}

Of all generalized properties, the most impressive one is certainly the generalized Stokes' theorem. Compare the divergence (or Gauss's) theorem:
\begin{equation*}
	\iiint_V (\nabla \cdot \vec{B}) \, dV = \oiint_{\Sigma = \partial V} \vec{B} \cdot \vec{d\Sigma}
\end{equation*}
with Stokes' theorem
\begin{equation*}
	\iint_\Sigma (\nabla \times \vec{f}) \cdot \vec{d\Sigma} = \oint_{\gamma = \partial \Sigma} \vec{f} \cdot \vec{d\gamma}
\end{equation*}
and the divergence theorem
\begin{equation*}
	\int_\gamma (\nabla \varphi) \cdot \vec{d\gamma} = \left[ \varphi \right]_A^B .
\end{equation*}
In all three cases we have a region where we define the integral of some differential operator applied to a field. This is equated to the integral over the boundary of the field itself. But again, the vector calculus notation prevents us to generalize as the differential operator is somewhat different in the three cases.

To understand conceptually that a generalization must exist, suppose we have a $k$-functional $F$ and its corresponding $k$-form $\omega$. By definition $F$ acts on $k$-surfaces. Now suppose we have a $k+1$-dimensional surface $\Sigma$. This is not a valid argument for $F$. However, its boundary $\partial \Sigma$ will be a $k$-dimensional surface, which is a valid argument for $F$. Therefore from any $F$ we can create a $k+1$-functional, which we can call exterior functional of $F$, noted $\partial F$, such that $\partial F(\Sigma) = F(\partial \Sigma)$. If we look at all three cases above, that is exactly what is happening: the gradient, the curl and the divergence lead to a functional one dimension higher than the original.

Now, since every functional has a corresponding form, $\partial F$ must have a corresponding $k+1$-form which must be fully identified by $\omega$. We note it as $\partial \omega$ and write
\begin{equation*}
	\partial F(\Sigma) = \int_\Sigma \partial \omega(d\Sigma) = \oint_{\sigma = \partial \Sigma} \omega(d\sigma) = F(\partial \Sigma).
\end{equation*}
This is the generalized Stokes' theorem and $\partial \omega$ is the exterior derivative. We use $\partial$ as we leave $d$ reserved for the infinitesimal regions.

With a few steps, one can see that the divergence, curl and gradient correspond to the exterior derivative in three dimensions for zero-form, one-form and two-form respectively, and that the three theorems of vector calculus are particular instances of this generalized one.

The identities
\begin{equation*}
	\begin{aligned}
		\nabla \cdot \nabla \times \vec \vec{f} &= 0 \\
		\nabla \times \nabla \varphi &= 0
	\end{aligned}
\end{equation*}
both correspond to the single identity
\begin{equation*}
	\partial \partial \omega = 0,
\end{equation*}
which states that the exterior derivative applied twice is also zero. This identity can be understood in terms of the functionals. We have and $\partial \partial F(\Sigma) = F(\partial \partial \Sigma)$ and the boundary of a boundary $\partial \partial \Sigma = \emptyset$ is always the empty set. So, no matter what $F$ or $\Sigma$ are, we are evaluating a functional on an empty set, which is zero.

In the same way that a divergence-free field can be written as the curl of a vector potential, or a curl-free field can be written as the gradient of a scalar potential, a form whose exterior derivative $\partial \omega = 0$ is zero (a closed form), can be written as the exterior derivative $\omega = \partial \theta$ of a lower dimensional form, at least is some neighborhood. In terms of functionals, this corresponds to the case where $F(\sigma)=0$ for all contractible surfaces, those surfaces that can be morphed to a point with a continuous transformation. If $F(\sigma)=0$ over all surfaces, instead, then the correspond is exact. 

The overall point here is that understanding differential topology in terms of functionals of regions gives us much better motivation and intuition for the math.

\iffalse

%If we compute the exterior derivative for zero, one and two-form


%The last element we need to introduce is the exterior derivative. We are actually much better equipped to understand this operation as it is much more intuitive on the linear functionals than the corresponding forms.

%First, note that every $k$-surface has a $k-1$ boundary. For example, a three dimensional volume would be enclosed by a two dimensional surface. Therefore we have a boundary operator $\partial \sigma^k \in S^{k-1}$. If the surface does not have a boundary, like the surface of a sphere, then the boundary is the empty set $\emptyset$. Note that the all boundaries are closed surfaces. Therefore the boundary of a boundary is the empty set. That is $\partial \partial \sigma = 0$ for all $\sigma$.

Now recall Stokes' theorem:
On the left side we have a two-functional, a functional of a surface. On the right side we have a one-functional, a functional of a line. The two-functional can be defined in terms of the one-functional: given a surface, take its boundary and pass it to the one functional. Recall Gauss's theorem:
Something similar is happening except we have a three functional on the left and a two functional on the right. Recall the gradient theorem:
\begin{equation*}
	\int_\gamma \nabla \varphi \cdot d\gamma = \left[ \varphi \right]_A^B
\end{equation*}
Here we have a one functional on the left and function of points on the right, which can be understood as a zero functional. We note a pattern, but again the vector calculus notation does not help us express the pattern.

Differential topology generalizes the pattern through the generalized Stokes' theorem
\begin{equation*}
	\int_\sigma \partial \omega(d\sigma) = \oint_{\partial \sigma} \omega(d\partial\sigma)
\end{equation*}
Our notation differs from the standard in that we use $\partial$ as the symbol for the exterior derivative and we make explicit the argument of the forms. We still have, however, to understand what is it that we are generalizing. This is better understood in terms of linear functionals.

Suppose we have a $k$-functional $F$, for example work along a path. Each $k$ dimensional surface $\sigma$ will have a boundary $\partial \sigma$ that is one dimensional. In the example, a two dimensional surface will identify a line. This means that, to each $k$-dimensional surface $\sigma$ we can associate a number $F(\partial \sigma)$. In the example, to a two-dimensional surface we can associate the work along its boundary. Proceeding this way we can define the exterior functional, which we note $\partial F$ and note that $\partial F(\sigma) = F(\partial \sigma)$. The notation is admittedly a bit unusual, but it allows us to show very easily that the exterior functional of the exterior functional is always zero. In fact
\begin{equation*}
	\partial \partial F(\sigma) = \partial F(\partial \sigma) = F(\partial \partial \sigma) = F(\emptyset) = 0
\end{equation*}
This is a direct consequence of the boundaries not having boundaries.

Since every $k$-functional has a corresponding $k$-form, so will the exterior functionals. Using Stokes' theorem we find that the form associated to the exterior functional is exactly the exterior derivative.

Obviously a full treatment of all these concepts goes beyond the scope of this appendix. Yet, we believe it is important to see that we can have a good understanding of these tools in terms of ideas that can be meaningful to physicists. 

\fi

\subsection*{Multiple degrees of freedom}

The case of multiple degrees of freedom proceeds in the same way as the single degree of freedom. We will have to:
\begin{enumerate}[label=(\roman*)]
	\item show that assuming determinism and reversibility \ref{assum_detrev} and independence of degrees of freedom \ref{assum_indep} is equivalent to assuming the existence of the displacement field $\vec{S}$ defined by \ref{mdof_displacement} and the closed counting form $\omega$ defined by \ref{mdof_form} such that $\omega$ is killed only by the displacement field $\vec{S}$ as in \ref{mdof_displacement_kills} or by its rescaling $\varphi \vec{S}$.
	\item show that the potential $\theta$ for $\omega$ defined in \ref{mdof_form_potential} can be expressed, without loss of generality, as \ref{mdof_potential_expression} and that $\omega$ can be expressed as $\ref{mdof_form_components}$
	\item show that \ref{mdof_form_potential}, \ref{mdof_displacement_kills} and \ref{mdof_potential_expression} recover Hamilton's equations as in \ref{mdof_Ham_eq}
	\item show that the integral of $\omega$ through a surface $\Sigma$ delimited by a path $\gamma$ and its variation $\gamma'$ is the variation of the integral of the vector potential, as claimed in \ref{mdof_action}
	\item show that the action is stationary for and only for the solutions of the equations of motion
	\item show that the action can be expressed as functions of position, velocity and time if and only if \ref{assum_kineq} applies.
\end{enumerate}

For (i), \ref{assum_detrev}  will lead to the existence of a displacement field $\vec{S}$ that tells us how state move in the extended phase space. As we saw in the previous case, this is not sufficient to impose the full condition: we need to be able to quantify the number of states as they flow in time. To this end, we need a functional $\mathcal{F}$ that takes an hyper-surfaces $\Sigma$ in the extended phase space and returns the count of states. We have 
\begin{equation}
	\mathcal{F}(\Sigma) = \int_\Sigma \Omega(d\Sigma)
\end{equation}
where $\Omega$ is the related form. Given the one-to-one correspondence between states and evolutions under \ref{assum_detrev}, $\Omega$ can be understood as quantifying either the flow of states through the surface or the states that are present on the surface.

As in the previous case, assumption \ref{assum_detrev} tells us that the integral over a closed hyper-surface is zero, since as many states flow in and out. We have
\begin{equation}
	\begin{aligned}
		\oint_{\Sigma = \partial V} \Omega(d\Sigma) &= 0 = \int_V \partial \Omega(dV) \\
		\partial\Omega &= 0.
	\end{aligned}
\end{equation}
which tells us that $\Omega$ is closed.

Given that $\Omega$ quantifies the flow over $\vec{S}$, the flow for any surface that is tangent to $\vec{S}$ must vanish. If $d\gamma = \vec{S} \varphi dt$ for some scalar $\varphi$, we must have
\begin{equation}
	\Omega(d\gamma, \cdot) = \varphi dt \Omega(\vec{S}, \cdot) = 0.
\end{equation}
The notation is a bit improper as $\vec{S}$ is not technically an infinitesimal displacement, but a rate of displacement given a parameter (i.e. a velocity). However, the slight abuse of notation makes the concept more clear: the direction of motion does not contribute to the flow.

On the other hand, this must be the only direction that does not contribute to the flow. Since $\vec{S}$ cannot be zero because states must flow in time, for any other direction $\vec{T}$ that is not parallel to $\vec{S}$ there must be a surface tangent to $\vec{T}$ and not $\vec{S}$. The flow through that surface cannot be zero. Therefore the only way that the flow is zero for any surface tangent to a vector $\vec{T}$ is if $\vec{T}= \varphi \vec{S}$

We now turn to assumption \ref{assum_indep}. This requires the count of states to factorize: if we have a parallelopiped in phase space, the count of states (i.e. its volume) is the product of the count of the configurations over each initial conditions (i.e. the area of the sides). Equivalently, \ref{assum_indep} must allow the case of a distribution over states with no correlations between the independent d.o.f., in which cases the density must factorize. Since we must be able to quantify the configuration for each d.o.f., and since each d.o.f. is two dimensional, we must have a two-form $\omega$ such that:
\begin{equation}
	\Omega = \omega^N.
\end{equation}

Given that $\Omega$ is closed, we have
\begin{equation}
	\begin{aligned}
		\partial \Omega &= \partial \omega \wedge \omega^{N-1} = 0 \\
		\partial \omega &= 0
	\end{aligned}
\end{equation}
which means that $\omega$ is also closed. This result can be also argued on physical grounds. If we imagine to translate a surface along an independent variable, the number of configurations identified by the surface cannot change. Therefore, if we imagine a parallelopiped, opposite sides must identify the same number of configurations. The integral will take the sides with an opposite orientation, and therefore there will be zero net contribution.


Putting it all together, assuming \ref{assum_detrev} and \ref{assum_indep} gives us the existence of the displacement vector field $\vec{S}$ and the closed counting two-form $\omega$ such that $\vec{S}$ identifies the only vector field that kills $\omega$.

The converse is also true: if we start with the mathematical requirement we recover the physical assumptions. If the displacement vector field $\vec{S}$ is the only vector field that kills the two-form $\omega$, then $\Omega = \omega^N$ is non-zero and is closed form. The system is deterministic and reversible because the displacement is well defined and it is paired of a flow that conserves the count of states. The degrees of freedom are independent because $\Omega = \omega^N$, but it can be seen in a more direct way.

If each pair of state variables $q^i$, $p_i$ forms an independent degree of freedom, the only surfaces spanned by matching position and momentum will define actual configurations, while all other combinations will not define any. Therefore:
\begin{equation}\label{canonical_conditions}
	\begin{aligned}
		\omega(e^{q^i}, e^{p_j}) = \omega_{q^i p_j} &= \delta^i_j = - \omega_{p_j q^i} \\
		\omega(e^{q^i}, e^{q^j}) = \omega_{q^i q^j} &= 0 \\
		\omega(e^{p_i}, e^{p_j}) = \omega_{p_i p_j} &= 0
	\end{aligned}
\end{equation}
If we start with a closed form $\omega$ that has a single degenerate direction, by Darboux theorem we can find coordinates for the non-degenerate part $\omega$ that satisfy the above conditions.

Our system will be now composed of $N$ independent degrees of freedom. This means that we can chart all states at all times using $2N + 1$ variables $\xi^a = \{ q^i, p_i, t\}$. We will use $\xi^a$ when we want to span all variables; $q^i$ and $p_i$ when we want to span the position or the momentum of all degrees of freedom.

As before, we will have a displacement field
\begin{equation}
	\vec{S} = \frac{d\xi^a}{dt} e_{a} = \frac{dq^i}{dt} e_{q^i} + \frac{dp_i}{dt} e_{p_i} + \frac{dt}{dt} e_t.
\end{equation}
However, unlike in the single degree of freedom case, $\vec{S}$ is not the object used fully characterize the geometry of the space.

The main idea is that we want to be able to quantify the number of states identified by each degree of freedom. As degrees of freedom are bi-dimensional, this means quantifying the areas of two-dimensional surfaces. Therefore we introduce the rank-2 tensor (more precisely a two-form)
\begin{equation}
	\omega = \omega_{ab} \, e^a \otimes e^b,
\end{equation}
which we call the state counting tensor. The idea is that a pair of vectors $\vec{v}$ and $\vec{w}$ will identify a parallelogram in phase space and 
\begin{equation}
	\omega(\vec{v}, \vec{w}) = \omega_{ab} v^a w^b
\end{equation}
quantifies the number of states over its surface.

The tensor $\omega$ will need to satisfy additional conditions to be physically meaningful. First of all, it will need to be anti-symmetric. The parallelogram identified by $\vec{v}$ and $\vec{w}$ in that order will be the same as the one identified by $\vec{w}$ and $\vec{v}$ therefore the area is the same. However, the switch changes handedness, and therefore will introduce a minus sign. Therefore we have:
\begin{equation}
	\omega(\vec{v}, \vec{w}) = - \omega(\vec{w}, \vec{v}).
\end{equation}
Since the rank-2 tensor is anti-symmetric, it can be understood as a two-form in differential geometry. Only the components below the diagonal are independent, which means $\sum_1^{2N}i =N(2N+1)$ independent components.

The form $\omega$ will also be closed, meaning
\begin{equation}\label{mdof_closed_form}
	\oiint \omega = 0
\end{equation}
over all closed surfaces. In fact, consider a parallelepiped. Given that the degrees of freedom are independent, and we have deterministic and reversible motion, two opposite sides will contain the same number of states. Given that the two opposite sides will have opposite orientation, their total contribution will be zero. This logic can be extended to all three-dimensional differential volumes, as they can be decomposed into infinitesimal parallelepiped.

Since $\omega$ is closed, in every contractible region it can be expressed as the exterior derivative of a covector $\theta$. We have
\begin{equation}
\begin{aligned}
	\theta &= \theta_a e^a = \theta_{q^i} e^{q^i} + \theta_{p_i} e^{p_i} + \theta_t e^t \\
	\omega &= - \partial \theta = - \left( \partial_a \theta_b - \partial_b \theta_a \right) e^a \otimes e^b
\end{aligned}
\end{equation}
The minus sign is the convention in symplectic geometry.

In the generalization we need to distinguish between vectors, like $\vec{S}$, covectors, like $\theta$, and two-forms, like $\omega$. The single degree of freedom used only three variables, therefore to each parallelogram in $\{q, p, t\}$ there is only one perpendicular direction, which allows us to blur the line between the objects in the following way:
\begin{itemize}
	\item for each covector $\theta$ we can find a vector $\vec{\theta}$ such that $\theta(\vec{S})=\vec{\theta} \cdot \vec{S}$;
	\item for each two-form $\omega$ we can find a vector $\vec{\omega}$ such that $\omega(\vec{v}, \vec{w}) = \vec{\omega} \cdot \vec{v} \times \vec{w}$.
\end{itemize}
This is what allows us to think only in terms of vectors. This cannot be done in the general case. In particular, the generalization of the curl is no longer an operation between vectors, but an operation that takes a one-form and returns a two-form.

Now we need to further characterize $\omega$. Each pair of variables $q^i$, $p_i$ forms an independent degree of freedom. This means that only surfaces spanned by matching position and momentum will define actual states, while all other combinations will not define any state. Therefore:
\begin{equation}\label{canonical_conditions}
\begin{aligned}
	\omega(e^{q^i}, e^{p_j}) = \omega_{q^i p_j} &= \delta^i_j = - \omega_{p_j q^i} \\
	\omega(e^{q^i}, e^{q^j}) = \omega_{q^i q^j} &= 0 \\
	\omega(e^{p_i}, e^{p_j}) = \omega_{p_i p_j} &= 0
\end{aligned}
\end{equation}
Of the $N(2N+1)$ independent components given the anti-symmetry requirement, the first condition sets $N^2$ of these while the last two set $N(N-1)/2$ independent components each. This totals $N(2N-1) = \sum_1^{2N-1}i$ independent components.

Assuming determinism and reversibility, time evolution contributes no new states, it just moves them in time. That is, no states appear or disappear from any degree of freedom. The displacement field doesn't define any state, no matter which other vector it is paired with. Mathematically, we say that the displacement kills the form
\begin{align}\label{mdof_displacement_kills}
	\omega(\vec{S}, \cdot) = 0.
\end{align}
This sets another $2N$ components (since $\omega(\vec{S}, \vec{S}) = 0$ is already set by anti-symmetry). All independent components are now set.

Those familiar with symplectic and contact geometry will recognize that conditions \ref{canonical_conditions} mean that $q^i$ and $p_i$ are canonical coordinates and, by Darboux's theorem, we will be able to write the covector potential as
\begin{equation}\label{mdof_potential_expression}
	\theta = p_i e^{q^i} - H e^t.
\end{equation}
Let us convince ourselves of this result by constructing a proof that is similar to what we have done for the single degree of freedom.

First, we express \ref{canonical_conditions} in terms of the covector potential. We have:
\begin{equation}\label{canonical_potential_conditions}
\begin{aligned}
	\omega(e^{q^i}, e^{p_j}) &= (-\partial\theta)_{q^i p_j} = -(\partial_{q^i}\theta_{p_j} - \partial_{p_j}\theta_{q^i}) = \delta^i_j \\
	\omega(e^{q^i}, e^{q^j}) &= (-\partial\theta)_{q^i q^j} = -(\partial_{q^i}\theta_{q^j} - \partial_{q^j}\theta_{q^i}) = 0 \\
	\omega(e^{p_i}, e^{p_j}) &= (-\partial\theta)_{p_i p_j} = -(\partial_{p_i}\theta_{p_j} - \partial_{p_j}\theta_{p_i}) = 0
\end{aligned}
\end{equation}
We can use our gauge freedom to set $\theta_{p_1} = 0$, much in the same way we did for the simpler case. We now have $\partial_{q^1} \theta_{p_1} = 0$ and, by the first condition, $\partial_{p_1} \theta_{q^1} = 1$. Integrating, we have $\theta_{q^1} = p_1 + g(q^i, p_2, p_3, ..., t)$ where $g$ is an arbitrary function which we can set to zero. Like in the single degree of freedom, we can do this because $g$ does not depend on $p_1$, so it can be eliminated with a gauge transformation that does not change $\theta_{p_1}$. Therefore we have:
\begin{equation}
	\theta = p_1 e^{q^1} + 0 e^{p_1} + \theta_{q^2} e^{q^2} + \theta_{p_2} e^{p_2} + ... + \theta_{t} e^{t}.
\end{equation}

Note that the components for the first degree of freedom do not depend on the other degrees of freedom. That is, for all $i>1$, $\partial_{q^i} \theta_{q^1} = \partial_{p_i} \theta_{q^1} = \partial_{q^i} \theta_{p_1} = \partial_{p_i} \theta_{p_1} = 0$. But by using conditions \ref{canonical_potential_conditions}, we find that the converse is true as well: the components of all other degrees of freedom do not depend on the first. That is, for all $i>1$, $\partial_{q^1} \theta_{q^i} = \partial_{p_1} \theta_{q^i} = \partial_{q^1} \theta_{p_i} = \partial_{p_1} \theta_{p_i} = 0$.

We can then use, again, our gauge freedom with a function that does not depend on the first two variables to set $\theta_{p_2} = 0$. And, with the same reasoning, we will be able to set $\theta_{q^2} = p_2$. And then, again, find that the first two degrees of freedom do not depend on the others, etc. At the end, we will find \ref{mdof_potential_expression}.

At this point, we calculate $\partial\theta(S, \cdot ) $. The components of $\partial\theta$ are:
\begin{equation*}
	(\partial\theta)_{ab} = \begin{bmatrix}
		0 & \delta^j_i & - \partial_{q^i} H \\
		-\delta^i_j & 0 & - \partial_{p_i} H \\
		\partial_{q^j} H & \partial_{p_j} H & 0
	\end{bmatrix}
\end{equation*}
We have
\begin{align*}
	\partial\theta(S, \cdot )  &= S^a (\partial\theta)_{ab} e^b = 0 \\
	&= (S^{q^i}(\partial\theta)_{q^ib} + S^{p_i}(\partial\theta)_{p_ib} + S^{t}(\partial\theta)_{tb}) e^b \\
	&= (S^{q^i}(\partial\theta)_{q^iq^j} + S^{p_i}(\partial\theta)_{p_iq^j} + S^{t}(\partial\theta)_{tq^j}) e^{q^j} + \\
	& (S^{q^i}(\partial\theta)_{q^ip_j} +  S^{p_i}(\partial\theta)_{p_ip_j} + S^{t}(\partial\theta)_{tp_j}) e^{p_j} + \\
	& (S^{q^i}(\partial\theta)_{q^it} + S^{p_i}(\partial\theta)_{p_it} + S^{t}(\partial\theta)_{tt}) e^t \\
	&= (-S^{p_i}\delta^i_j + S^{t}\partial_{q^j} H ) e^{q^j} + \\
	& (S^{q^i}\delta^j_i +  S^{t}\partial_{p_j} H) e^{p_j} + \\
	& (-S^{q^i} \partial_{q^i} H - S^{p_i} \partial_{p_i} H) e^t \\
\end{align*}

All components must be zero, therefore we have the following three equations:
\begin{align*}
	S^{p_j} &= S^{t} \partial_{q^j} H \\
	S^{q^j} &= - S^{t}\partial_{p_j} H \\
	-S^{q^i} \partial_{q^i} H - S^{p_i} \partial_{p_i} H &= S^{t}\partial_{p_i} H \partial_{q^i} H - S^{t} \partial_{q^i} H \partial_{p_i} H = 0
\end{align*}
Note that the last expression is not a new equation: it is identical to zero given the previous two equations. Since $S^t = 1$, we have:
\begin{align*}
	S^{p_j} &= \partial_{q^j} H \\
	S^{q^j} &= - \partial_{p_j} H
\end{align*}
which recovers Hamilton's equations for multiple degrees of freedom.

Once again, we have found that the whole physical content is included in condition \ref{Detrev_assumptions}. The geometry in the extended phase space is fully specified by the state counting form $\omega$. This does not define lengths and angles, like the metric tensor does for a more standard geometric setting. It defines areas of two-dimensional surfaces in terms of states, and angles between two-dimensional surfaces in terms of the degree of independence between degrees of freedom. What the equations say is simply that states move in time at the same rate (i.e. $\frac{dt}{dt} = 1$) with a deterministic and reversible flow (i.e. no states are created or destroyed on any independent degree of freedom).

\subsection{Action principle}
Having reformulated Hamiltonian mechanics in the extended phase space using the language of differential geometry, the geometrical interpretation of the action principle is essentially the same.

The Lagrangian is equal to the potential covector applied to the displacement along a generic path. That is
\begin{equation}
\begin{aligned}
L &= \theta\left(\frac{d\vec{\gamma}}{dt}\right) = \theta_a \frac{d\xi^a}{dt} \\
&= p_i \frac{dq^i}{dt} + 0 \frac{dp_i}{dt} - H \frac{dt}{dt} \\
&= p_i \frac{dq^i}{dt} - H.
\end{aligned}
\end{equation}

As before, we will consider the variation of the line integral $\delta \int_{\gamma} \theta(d\vec{\gamma})$, which can be expressed as
\begin{equation}
	\begin{aligned}
		\int_A^B \theta(d\vec{\gamma}) &= \int^{t_1}_{t_0} \theta\left(\frac{d\vec{\gamma}}{dt}\right) dt \\
		&= \int^{t_1}_{t_0} \left(p_i \frac{dq^i}{dt} - H \right) dt
	\end{aligned}
\end{equation}
The original path $\gamma$ together with its variation $\gamma'$ form a contour $\partial \Sigma$. We can then use Stokes' theorem to transform the line integral of $\theta$ to a surface integral of its exterior derivative. That is:
\begin{align*}
	\delta \int_{\gamma} \theta(d\vec{\gamma}) = 
	&= \int_{\gamma} \theta(d\vec{\gamma}) - \int_{\gamma'} \theta(d\vec{\gamma}') \\
	&= \oint_{\partial \Sigma} \theta(d\vec{\gamma}) \\
	&= \iint_{\Sigma} \partial \theta (d\Sigma) \\
	&= - \iint_{\Sigma} \omega(d\Sigma).
\end{align*}
At each point, one side of $d\Sigma$ will be $d\vec{\gamma}$, therefore $\omega(d\Sigma) = \omega(d\vec{\gamma}, d\vec{\lambda})$ where $d\vec{\lambda}$ is the direction of the variation. If $\gamma$ is an actual evolution, $d\vec{\gamma} = \vec{S} dt$ and, by \ref{mdof_displacement_kills}, we have
\begin{equation}
	\begin{aligned}
	\delta \int_{\gamma} \theta (d\vec{\gamma}) &= - \iint_{\Sigma} \omega(d\Sigma) \\
	&= - \iint_{\Sigma} \omega(d\vec{\gamma}, d\vec{\lambda}) \\
	&= - \iint_{\Sigma} \omega(\vec{S}, d\vec{\lambda}) dt \\
	&= - \iint_{\Sigma} 0 = 0
	\end{aligned}
\end{equation}

The action principle, then, is a direct consequence of the geometry set by \ref{mdof_closed_form} and \ref{mdof_displacement_kills}, which is the generalization of the divergence-free nature of the displacement field $\vec{S}$. Every evolution $\gamma$ is a field line of $\vec{S}$. Any infinitesimal surface $d\Sigma$ enclosed by an evolution and its variation, then, will also be tangent to the displacement field $\vec{S}$ and therefore will contribute no states, $\omega(d\Sigma)$ will be zero. Given that $\omega$ is closed, the integral over $\Sigma$ can be expressed as the line integral of the covector potential $\theta$ over the contour of $\Sigma$.

\subsection{Discussion}

Apart from the technicalities needed to properly frame the problem, multiple degrees of freedom do not add much conceptually. All geometrical and physical content within the theory is about counting states and keeping track of how they move in time, which is done by the two-form $\omega$. As before, the assumption of deterministic and reversible evolution does all the work.

The additional key assumption is that the degrees of freedom must be independent. If the degrees of freedom were not independent, then the number of states within a four-dimensional parallelepiped would not be given by the product of the count of states on the sides. We would need additional structure. If the degrees of freedom started independent but didn't remain so, the displacement field $\vec{S}$ would only preserve the count of states of volumes, not over each degree of freedom, and therefore $\omega(\vec{S}, \cdot) \neq 0$.

The comments on the lack of strict physicality for the values of the Lagrangian and the action remain unchanged. They both depend on a choice of gauge. The comment on the centrality of the extended phase space to understand the action principle is only reinforced. Additionally, note that by changing $t$ to $ct$ expression \ref{mdof_potential_expression} becomes
\begin{equation}\label{mdof_potential_relativistic}
	\theta = p_i e^{q^i} - \frac{H}{c} e^{ct}.
\end{equation}
which bears striking resemblance to the relativistic four-momentum. While it goes outside the scope of this article, the idea is that, in this setting, we already have some pre-relativistic features, even though we have not introduced a metric tensor.

\section{Conclusion}

We have given a geometrical interpretation of the principle of stationary action that is also fully physically motivated by the assumption of deterministic and reversible evolution. The key insight is that a path $\gamma$ and a variation form a closed curve which encloses a surface $\Sigma$. If the path is an actual evolution for the system, all states flow along the surface $\Sigma$, not through it. Using Stokes' theorem, the zero flow condition through the surface $\Sigma$ is transformed to a zero integral condition on its boundary (i.e. on the path $\gamma$ and its variation) leading to the principle of least action.

We have seen that both the Lagrangian and the action depend on the potential, which means they are gauge dependent. This makes the numerical values of those objects not strictly physical. We have seen that the interpretation given also works for those Hamiltonian systems that do not have a corresponding Lagrangian, meaning that the most general expression for the action principle lies on the extended-phase-space Hamiltonian formulation of classical mechanics. We have seen that elements from symplectic and contact geometry are needed for the generalization to multiple degrees of freedom and that they can be formulated in a language that is less abstract and closer to physics conventions.

All considered, we believe this way of looking at classical mechanics provides a more complete characterization and better insights than the usual treatments.


\end{document}