\documentclass[11pt]{article}

\usepackage[margin=1.25in]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{dutchcal}
\usepackage{braket}
\usepackage{enumitem}

\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{calculator}
\usepackage{standalone}



\begin{document}

\title{Hamiltonian mechanics is conservation of information entropy}
\author{Christine A. Aidala, Gabriele Carcassi}


%\ifjournal
%	% As the page margins are so large, the full title does not fit
%	\titlerunning{From physical principles to classical and quantum particle mechanics}
%\fi

\date{\today}

\maketitle

\begin{abstract}
	In this work we show the equivalence between Hamiltonian mechanics and conservation of information entropy. We will show that distribution with coordinate independent values for information entropy require that the manifold on which the distribution is defined is charted by conjugate pairs (i.e. it is a symplectic manifold). We will also show that further requiring that the information entropy is conserved during the evolution yields Hamilton's equations. This will provide new insights in the nature of classical mechanics and physics in general, as it will connect to elements of relativity, statistical mechanics, thermodynamics, quantum mechanics, probability theory and symplectic geometry. This work is part of a larger project, called Assumptions of physics, which aims to rederive the known equations from a handful of physically meaningful starting points.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

Over the past decade there has been a renewed interest in the underpinning of classical mechanics~\cite{North,Curiel,Barrett1,Barrett2}. Different attempts and arguments have been put forward to understand what are the most important feature of the different mathematical formulations, whether one is more fundamental of the other in some sense or whether they are equivalent.

All these attempts, though, suffer from one problem. They concentrate on the mathematical structure. The issue is that, in general, a mathematical object can represent more than one physical entity. For example, the same mathematical framework for linear circuits can be used to describe electric, hydraulic, thermal or mechanical systems. On the other end, the same physical entity can be described by more than one mathematical object. For example, the same cannonball can be described by position and velocity in a Lagrangian setting, position and momentum in a Hamiltonian setting or in a thermodynamics setting by temperature, volume and so on. The connection between math and physics, then, is many-to-many and therefore we can't study only one side. The reason is that mathematics captures only the syntax of a particular description without its semantics, which at that point cannot be recovered. Mathematical structures are just shadows of the physical model they represent.

It should be clear that we do not fault the authors. Even just a century ago, there was a push to find fundamental principles or laws that could serve as a starting point for the different theories. Newtonian mechanics, thermodynamics and special relativity serve as good examples. Nowadays, unfortunately, much of the current fundamental theories, including Lagrangian, Hamiltonian, Quantum mechanics and Quantum Field Theory take as their starting point a particular mathematical structure. That is, we do not know what the value of a Lagrangian is or what the symplectic form represents: we just take them as a given. Moreover, a lot of work in theoretical physics nowadays starts by exploring novel abstract mathematical structures and then trying to see whether some physical meaning or prediction can be extracted. We are not going to debate here how and why this shift happened. We just merely note that it leaves the semantics of our theories ill defined.\footnote{Which is a nice way to say that we don't really know what are we talking about.} 

It is only natural that, given the circumstances, one uses the only thing that seems to be well defined, the math, and tries to get understanding from there. But here lies the fundamental problem: that math is not created by physicists to do physics, it is created by mathematicians to do math. The definitions of topology, differentiable manifold, symplectic manifold, Hilbert space and so on are chosen either because they are convenient to prove theorems or because they are convenient to make calculations.\footnote{In fact, it is common in math to have two such definitions and then prove they are equivalent.} We should not expect them to be a good match for definitions that would be physically meaningful. In other words, there is a very good chance that just looking at just the math may lead us, like horses with blinders, to a very clear path which is not the physically meaningful one.\footnote{To be clear, we do not fault the mathematicians either as they are not responsible for whether their structures are used appropriately or not.}

Fully conscious of these issues, we have embarked in an effort, called Assumptions of Physics~\cite{carc1}, that aims to find a handful of physical principles from which the known laws of physics can be formally derived~\cite{carc2}. The approach has several advantages. First, by starting from physical ideas we put physics back at the center of the discussion, and not relegate it as a mere after the fact interpretation: the semantics is always well defined. Second, it puts physics on sturdier mathematical grounds as we have a precise physical meaning to all the formal objects and we leave out those that do not. Third, it fosters connections between different fields of science, math, philosophy and engineering: it is our belief that nature is one and does not care about such divisions and therefore truly fundamental concepts must span freely across different areas. Fourth, it gives a framework to pose deep questions of the type the philosophy of science community is interested in. And lastly, by being forced to formalize and make explicit our assumptions, we can better understand how and why they fail possibly giving new insights that may lead to new physics. While the project is ambitious, we have already made progress beyond what we thought it could be reasonably done. So there's that.

In this paper we focus on just one particular insight provided by our work: \textbf{a distribution over a manifold conserves information entropy over time if and only if its elements evolve according to Hamiltonian mechanics.} That is, the single requirement of information entropy conservation not only gives us Hamiltonian dynamics but also the structure of phase space (i.e. a symplectic structure where variables are organized in conjugate pairs). In section 2 we show that, in general, information entropy and distributions densities are coordinate dependent. In section 3 we show that the structure of phase space is required to make such objects invariant under coordinate transformation (i.e. observer independent) and therefore give them an objective physical character. Conservation of entropy over time will force that invariance during time evolution, giving rise to Hamilton's equations. In chapter 4 we will briefly discuss the result and show how it necessarily leads to a classical version of the uncertainty principle.

The overall idea is that \textbf{Hamiltonian mechanics is best understood as the deterministic and reversible\footnote{By reversible we mean the ability, given a state, to reconstruct its past. This is the dual of determinism, that is the ability, given a state, to predict its future.} evolution of infinitesimally reducible objects}. As the information needed to identify an element of the distribution does not change in time, knowing the initial state is equivalent to knowing the future or past state. Classical particles, then, should be understood as the limit of subsequent subdivisions, infinitesimally small objects with extent in phase space. This single physical model is captured piecemeal by elements of statistics, probability theory and symplectic geometry which are given a coherent and well defined meaning. In our view, this understanding also meshes better with other ideas from general relativity\footnote{A point particle in general relativity would not follow a geodesic and would, due to the infinite mass density, puncture space-time.}, thermodynamics and quantum mechanics.



\section{Distributions}

The general idea is that we want to describe a quantity that is distributed among different cases. For example, mass or charge over different positions in space, amount of material among different compounds, population at different ages. In all these cases, we assume there is a total quantity $M$, a set of $n$ variables $\xi^a$ over which the quantity is distributed and a function $\rho(\xi^a)$ such that:

\begin{equation}
\int M \rho(\xi^a) d\xi^1 d\xi^2 ... d\xi^n = M
\end{equation}

We say that $\rho$ is the (normalized) density for our distribution and if we want to calculate the amount in a particular region $V$ we have:

\begin{equation}
m_V = M \mu(V) =\int_V M \rho(\xi^a) d\xi^1 d\xi^2 ... d\xi^n
\end{equation}

The above expression would represent the mass or charge within a particular region, the amount of material of some selected set of compounds or the population in a particular age group. The integral is used when some of the variables $\xi^a$ are continuous and a simple sum would be used over the discrete ones.

While the mathematical setting is the same for degrees of belief (credence) or the likelyhood that something will happen (probability), we will assume we are working with an actual physical distribution. Even when we are talking about marginal distributions or information entropy, we are doing it in the context of a system that has parts and each part has a different characterization.

The link to information theory is the following: suppose we have a distribution, known to both of us, and you select one element at random. How many yes or no questions do you have to answer before I know what case you selected? For example, suppose there are 4 balls of which 2 are red and 2 are green, one yes or no question would be sufficient. In general, if $I$ is the number of questions then $C = 2 ^ I$ are the number of cases you can distinguish with those questions. On the other hand, suppose $\rho(\xi_0)=1/5$, then we know that we'd select $\xi_0$ one in 5 cases. That is, the normalized distribution is the inverse of the number of cases: $\rho(\xi_0) = \frac{1}{C}$.  Therefore the information needed to identify a particular element is $I(\xi^a)=\log \frac{1}{\rho(\xi^a)}$. If we take the average and change the base of the logarithm, we have:
\begin{equation}
I(\rho(\xi^a)) = \int \rho(\xi^a)) \ln \left(\frac{1}{\rho(\xi^a))}\right) d\xi^1 d\xi^2 ... d\xi^n =-\int \rho(\xi^a)) \ln (\rho(\xi^a)) d\xi^1 d\xi^2 ... d\xi^n
\end{equation}
which is the information entropy of the distribution.\footnote{The nature of information entropy is often misunderstood, especially in physics. It does not represent the information the distribution has: it represent the \emph{additional} information that would be needed to identify an element. Also, it does not represent knowledge: as we are studying actual physical distributions, any correct distribution has full knowledge of the system.}

Technically, given the more convenient choice of the natural logarithm, the unit of information is the nat, which is equal to $\frac{1}{\ln 2}$ bits. However, in the discussion we will refer to it as measured in bits, as we believe that the majority of the readers is more conformable with the latter unit. Note that for continuous variables the expression has a slightly different meaning: the uniform distribution in a unit volume is assigned zero entropy so $I(\rho(\xi^a))$ represents the relative number of bits to achieve the same level of precision, which can be negative.

\subsection*{Densities and change of variables}

There is one feature about densities we need to be fully aware of: they are not invariant under change of variables. Intuitively, if we change units of the denominator the distribution changes value. For example, $1$ kg/m becomes $1000$ kg/km: the number changes and the units allow us to understand that the overall quantity remains the same but mathematics does not keep track of units. In general, if $\hat{\xi}^a=\hat{\xi}^a(\xi^b)$ we can recover how the density changes by requiring that the total in each volume is the same no matter what variables are used. That is:
\begin{align*}
\int_V \rho(\xi^b) d\xi^1 ... d\xi^n &= \int_V \rho(\hat{\xi}^a) d\hat{\xi}^1 ... d\hat{\xi}^n \\
&=\int_V\rho(\hat{\xi}^a) \left\|\frac{\partial \hat{\xi}^a}{\partial \xi^b}\right\| d\xi^1 ... d\xi^n
\end{align*}
\begin{equation}\label{density_transformation}
\rho(\xi^b) = \rho(\hat{\xi}^a) \left\|\frac{\partial \hat{\xi}^a}{\partial \xi^b}\right\|
\end{equation}
the density is multiplied by the absolute value of the Jacobian determinant of the transformation. As the Jacobian is in general a function of the variables themselves, this also means that whether the density at one point is greater or smaller than at another also depends on the choice of variables.

Similarly, we can see that information entropy is not invariant
\begin{align*}
I(\rho(\xi^b)) &=-\int \rho(\xi^b) \ln (\rho(\xi^b)) d\xi^1 ... d\xi^n \\
&=-\int \rho(\hat{\xi}^a) \left\|\frac{\partial \hat{\xi}^a}{\partial \xi^b}\right\| \ln \left(\rho(\hat{\xi}^a) \left\|\frac{\partial \hat{\xi}^a}{\partial \xi^b}\right\|\right) d\xi^1 ... d\xi^n \\
&=-\int \rho(\hat{\xi}^a) \ln \left(\rho(\hat{\xi}^a) \left\|\frac{\partial \hat{\xi}^a}{\partial \xi^b}\right\|\right) d\hat{\xi}^1 ... d\hat{\xi}^n \\
&=-\int \rho(\hat{\xi}^a) \ln (\rho(\hat{\xi}^a)) d\hat{\xi}^1 ... d\hat{\xi}^n -\int \rho(\hat{\xi}^a) \ln \left\|\frac{\partial \hat{\xi}^a}{\partial \xi^b}\right\| d\hat{\xi}^1 ... d\hat{\xi}^n
\end{align*}
\begin{equation}\label{entropy_transformation}
I(\rho(\xi^b)) =I(\rho(\hat{\xi}^a)) -\int \rho(\hat{\xi}^a) \ln \left\|\frac{\partial \hat{\xi}^a}{\partial \xi^b}\right\| d\hat{\xi}^1 ... d\hat{\xi}^n
\end{equation}
The reason is that we may be changing scale and therefore level of precision. Given that both change is determined by the Jacobian, we conclude that \textbf{during a change of variable the information entropy of all possible distributions does not change if and only if the density remains the same at every point under the same change of variable}.

To sum up, while we'd like to think of a density as a number associated to a point of a space, that's technically incorrect. Given a manifold $\mathcal{M}$ upon which we define the distribution, the latter is \emph{not} $\rho : \mathcal{M} \to \mathbb{R}$. That would be a scalar function, invariant under change of variables, as a point is a point no matter how is it identified. Instead we additionally need a set of functions $\xi^a : \mathcal{M} \to \mathbb{R}$ and only then we can define the density as $\rho : \mathbb{R}^n \to \mathbb{R}$. The density value is defined on the values for a set of variables, not at a point.\footnote{As the density is really a limit, we need to know how that limit is taken and the coordinates decide that.} That is, the density is undefined without a (differentiable) set of coordinates.

\subsection*{Mathematical tools for distributions}

Different fields in math deal with this issue in different ways depending on their aims, which unfortunately leads to a fractured physical understanding given that one needs ideas and results from the different areas. In statistics and information theory one simply accepts the transformation rules and does not try to create invariant objects. This gives us the proper setting to study the statistical properties but it does not help us understand which mathematical elements represent objective physical entities.

In probability theory one starts with three objects: a set of outcomes $\mathcal{M}$, a set of events $\sigma_\mathcal{M}$ where each event is a collection of outcomes, and a probability measure $\mu : \sigma_\mathcal{M} \rightarrow [0,1]$ that assigns a number between zero and one to each event. That is, we don't assign a probability density to the points of $\mathcal{M}$ but we assign a finite probability to finite regions. We can then define a random variable $\xi : \mathcal{M} \rightarrow \mathbb{R}$ as a real valued function of the outcomes. For each random variable we can define its cumulative distribution function $F_\xi(\xi_0)=\mu(\xi<\xi_0)$ and the probability density function is its Radon-–Nikodym derivative $\rho(\xi) = \frac{dF_\xi}{d\xi}$. That is: the integral is the primal object while the density is derived.

This approach actually makes more physical sense: what we measure is the amount of mass in a finite region and the density is the limit for a smaller and smaller ones. The fundamental objects (outcomes, events and probability) are all defined without reference to variables. But in probability theory we have no notion of coordinate systems, vectors and all other geometrical notions.

In differential geometry, one starts with a manifold: a set of points $\mathcal{M}$ which can be given local coordinates $\xi^i : \mathcal{M} \to \mathbb{R}$. One defines a tangent space $\mathsf{T}\mathcal{M}$ where vectors live and cotangent space $\mathsf{T}^*\mathcal{M}$ were linear functions of vectors live. Then we define $n$-forms as multi-linear functions of $n$ vectors that return the value associated with the infinitesimal parallelepiped they form. That is $\nu : (\mathsf{T}^*\mathcal{M})^n \rightarrow \mathbb{R}$. As vectors are coordinate invariant, $n$-forms are coordinate invariant as well so we can write $\mu(V)=\int_V \nu(dV)$ with no reference to coordinates. If $e_a$ are the basis vector associated with $\xi^a$ and $e^b$ are linear functions such that $e^b(e_a)=\delta_a^b$, then we can express each $n$-form as $\rho(\xi^a)e_1\wedge e_2 \wedge ... \wedge e_n$. The density $\rho(\xi^a)$ is the component of the $n$-form expressed in the $\xi^a$ coordinates in the a way analogous to components of a vector.\footnote{Technically, the form is linear so it changes with the Jacobian determinant and not its absolute value. But negative determinants means changing orientation of the region of integration, so fixing that would bring another sign change and $\rho(\xi^a)$ would not change. This is equivalent to saying that some coordinate systems are right handed and some are left handed.}

This approach gives us a way to understand which objects are coordinate independent and which aren't. But it has two issues: it does not allow to easily talk about coordinate dependent operations, such as marginal distributions and information entropy, and it may use definitions that do not map well to physical concepts.\footnote{Typically, a vector is defined as a map between a scalar function of the manifold to another scalar function to the manifold. Or as an equivalence class of trajectories. It is a stretch to think of fluid velocities or electric fields in those terms. For that same reason, we do not use the differential geometry notation $\frac{\partial}{\partial \xi^a}$ and $d\xi^a$ for vector and covector basis because they do not convey the actual physical meaning of these objects as they are used in physics.}

So the situation is that we have a single clear physical objects we want to study, but different and somewhat disconnected mathematical frameworks to study it. This should not be surprising as the frameworks are defined by mathematicians, as we said before, to solve their own needs. But it is an issue when these are used in physics without careful consideration as each mathematical framework only partially captures our physical understanding. That is why we argue that simply examining the mathematical structure used in a physical theory is not enough to gain a complete picture.

\section{Coordinate independent distributions}

Suppose we have a system which we can think of made of infinitesimally small parts, which we call particles. Suppose  that each infinitesimal part can be assigned a state from a state space $\mathcal{S}$. The state of the whole system will be a distribution over those states. Now suppose that this system evolves in a way that is deterministic and reversible, that is the state of a particle at one time is associated with one and only one state at another time. What does that tell us about our distribution?

If the evolution is deterministic and reversible, then all the parts that are in one state will be mapped to another: the density should be exactly mapped to a different state and therefore should not change value. If the evolution is deterministic and reversible, then if I give you enough information to identify the state of an element at one time I also gave you enough information to identify the state of the same element at a future time: the information entropy will remain the same over time. And here lies the problem: how can we make sure that these quantities are preserved over time if they are not even observer independent? How can we know if the density stays the same if we can't even compare the value of the densities at two different elements of the distribution? How can we tell whether the information entropy is conserved if we can't event define a unique value at a given time?

This leads us to the following insight: \textbf{if we want to define deterministc and reversible evolution over distributions, they must be invariant distributions. That is, the densities and information entropy must be independent on the choice of coordinates.} As we noted before, requiring one to be coordinate invariant will automatically require the other to be invariant as well.

If our variables are discrete, all our integrals are sums and all our definitions work. Everything is already coordinate invariant. If our variables are continuous, instead, we have a problem. Because the densities and the information entropy vary with the Jacobian, we need something more. Our state space $\mathcal{S}$ must allow to write densities whose value does not change under coordinate transformation. This seems to be impossible given what we saw before: how can it work?

Suppose $\mathcal{S}$ is a two dimensional manifold. Suppose we have two variables $(q,k)$ such that $k$ uses the inverse units of $q$. For example, if $q$ is meters then $k$ is inverse meters which means an infinitesimal area $dq dk$ will be a pure number. We say $q$ is a coordinate as it is a variable that defines a unit, while $k$ is the corresponding conjugate variable as it uses the inverse unit. Now suppose you change coordinate. For example, $\hat{q}$ will be kilometers and $\hat{k}$ inverse kilometers. We'll have $d\hat{q} d\hat{k} = dq dk$. Therefore we'll also have $\rho(q,k) = \rho(\hat{q}, \hat{k})$: the density is invariant and so is the information entropy.

If we have a set of variables fixed by a single choice of unit then what we discussed before is the only possible way. Let $q$ be the coordinate, the variable that defines the unit. Suppose we have $n$ variable $\xi^a$ such that $\xi^1 = q$. Each vector $v^a$ tangent to that space has $n$-components. If we change unit $\hat{q}=\hat{q}(q)$ we have two constraints: the one unit change and the unitary Jacobian, and these must be enough to determine how the vector components change. The components must be equal to the constraints and therefore they are exactly two variable $\xi^a = (q, k)$. Then $k$ is the conjungate variable of $q$.

Physically, the coordinate plays a double role. It both defines a unit and it helps identify particle states. It is this double role that gives it an important character because a change in $q$ must induce a change in $k$ as their units and the unit of $\rho$ are not independent. Symplectic geometry strips away the units and does not elevate densities as primary objects. It is no wonder the link gets lost.

\subsection*{Hamiltonian mechanics for one degree of freedom}

Let's set $\hbar$ as the unit we use to describe the amount of possible states (or the inverse of a unitary density). We have:
\begin{equation}
m_V = M \mu(V) =\int_V M \rho(q, k) \hbar dq dk = \int_V M \rho(q, p) dq dp
\end{equation}
where we set $p=\hbar k$. We should recognize $p$ as conjugate momentum and $k$ the classical analogue of the wave number. Suppose we change variables. In general the Jacobian will be given by:
\begin{equation}
\label{Poisson}
\begin{aligned}
|J| &= \left| \begin{matrix}
\dfrac{\partial \hat{q}}{\partial q} & \dfrac{\partial \hat{q}}{\partial p} \\[2.2ex]
\dfrac{\partial \hat{p}}{\partial q} & \dfrac{\partial \hat{p}}{\partial p} \end{matrix} \right| = \frac{\partial \hat{q}}{\partial q} \frac{\partial \hat{p}}{\partial p} - \frac{\partial \hat{p}}{\partial x} \frac{\partial \hat{q}}{\partial p} &= \{\hat{q}, \hat{p}\}
\end{aligned}
\end{equation}
which we recognize it to be the Poisson bracket. If the change of variable is simply a change of coordinates, a change of units, we have:
\begin{equation}
\label{coordinate_change}
\begin{aligned}
\hat{q} &= \hat{q}(q) \\
\{\hat{q}, \hat{p}\} &= 1 = \frac{\partial \hat{q}}{\partial q} \frac{\partial \hat{p}}{\partial p} \\
\dfrac{\partial \hat{p}}{\partial p} &= \frac{\partial \hat{q}}{\partial q} ^{-1} = \frac{\partial q}{\partial \hat{q}} \\
\hat{p} &= \frac{\partial q}{\partial \hat{q}} p
\end{aligned}
\end{equation}
Conjugate momentum transforms as a covariant component. Under coordinate transformations we have:
\begin{equation}
\label{density_invariance}
\begin{aligned}
\rho(q,p) &= \rho(\hat{q}, \hat{p}) \\
I(\rho(q,p)) &= I(\rho(\hat{q},\hat{p}))
\end{aligned}
\end{equation}
Let us call $(q,p)$ canonical variables if the density is equal to the invariant density. Then two other variables such that $\{\hat{q}, \hat{p}\}=1$ are also canonical since the Jacobian determinant will be unitary.

We can think of the evolution as a vector field on $\mathcal{S}$ of components $S = (S_q, S_q) = (\frac{dq}{dt}, \frac{dp}{dt})$ that gives the direction in which states move in time. The Jacobian for time evolution will be
\begin{equation}
\label{Jacobian_evolution}
\begin{aligned}
|J| &= \left| \begin{matrix}
1 + \dfrac{\partial S_q}{\partial q}dt & \dfrac{\partial S_q}{\partial p} dt \\[2.2ex]
\dfrac{\partial S_p}{\partial q}  dt & 1 + \dfrac{\partial S_p}{\partial p} dt \end{matrix} \right| \\
&= 1 + \left[ \dfrac{\partial S_q}{\partial q} + \dfrac{\partial S_p}{\partial p} \right]dt + O(dt^2)\\
&= 1 + div(S)dt + O(dt^2)\\
\end{aligned}
\end{equation}

If we want the time evolution to transport the densities and to conserve information entropy, the Jacobian must be unitary and therefore $S$ is divergence-free and admits a potential. We have
\begin{equation}
\label{Potential_Hamilton}
\begin{aligned}
S = - curl(H) &= (\frac{\partial H}{\partial q}, - \frac{\partial H}{\partial p}) \\
\frac{dq}{dt} &= \frac{\partial H}{\partial p}  \\
\frac{dp}{dt} &= - \frac{\partial H}{\partial q}  \\
\end{aligned}
\end{equation}
and we recognize Hamilton's equations. Note that the reverse is true as well: any Hamiltonian evolution will transport densities and conserve information entropy due to Liouville's theorem.\footnote{Again, it's not just that the density and the areas are conserved in time: is that they are the same for all canonical coordinates. That is, we have an objective way to compare them.} Therefore \textbf{Hamiltonian mechanics is the exactly the evolution of (classical) distributions that preserve information entropy and densities}.

We'd like to stress how, once the simple premise of invariant distributions is accepted, all elements of Hamiltonian mechanics are necessary and have a straight forward physical meaning. Not only that: because of the equivalent we know that everything in Hamiltonian mechanics can be understood with just that simple premise. And the premise is not about cotangent bundles of Lie group of quasi-q spaces on an infinite-dimensonal lattice. It's just the simple realization that if we require distributions to have a physically objective reality they must be coordinate independent.

\subsection*{Distributions over multiple degrees of freedom}

To understand what happens in multiple degrees of freedom, let's first look at change of variables of marginal distributions. Suppose we have a distribution $\rho(\xi^a)$ that depends on many coordinates, on many units. If the density is coordinate invariant, then it will be so even if we change one coordinate, say $\xi^1$. As we saw before, then we can find the conjugate variable $\xi^2$ that changes in the opposite way. We can calculate the marginal distribution by integrating over those two quantities.
\begin{equation}
\rho(\xi^b) = \int \rho(\xi^a) d\xi^1 d\xi^2
\end{equation}
where $\xi^b$ are the remaining coordinates. If the variables $\xi^b$ are independent from $\xi^1$ and $\xi^2$, then we must be able to change units independently as well. Which means $\rho(\xi^b)$ must be coordinate invariant.

We can imagine to proceed iteratively, one unit at a time. We'll find that our variables are paired $(q^i, p_i)$  with the first defining the units and the seconds defining the invariant areas. We call each pair an independent degree of freedom as it constitute an independent choice of unit. We recognize $\mathcal{Q}$, the manifold charted by $q^i$, as the (poorly named) configuration space. We are going to call it coordinate space as it includes all variables and only the variables that define the units and the reference frame. The state space for the particles, then, is a set of coordinates plus an equal number of conjugate variables that vary like covector components, therefore we recognize $\mathcal{S}=\mathsf{T}^*\mathcal{Q}$ as the cotangent bundle.\footnote{This does \emph{not} mean that conjugate momentum is a one form, in the sense of a map from a vector to a number. The state space is just a manifold and the structure it has comes from how units transform, not because the variable represent different objects. Unfortunately mathematical structures strip out the units, so that knowledge is lost and then resurfaces in other structures. Changing state variables from position and momentum to position and velocity is not, as one may be mistakenly understand from the math, a change of objects: is relabeling the same states much like changing spatial coordinates relabels positions. It is not, in general, a canonical transformation as the density will not correspond to the invariant one, and it is not simply a change of coordinates. But it is still a valid change of state variables.}

In terms of information theory, this is equivalent to being able to assign a coordinate invariant information entropy to each of the marginal distributions. This means that relationships between the entropy of the total and marginal distributions are preserved. For example, saying that the marginal distributions $\rho_2(q^2)$ and $\rho_1(q^1)$ are independent is equivalent to saying that the entropy of the joint distribution is equal to the sum of the entropy of the marginals: $I(\rho(q^1,q^2)) = I(\rho_2(q^2)) +I(\rho_1(q^1))$.\footnote{This is also equivalent to saying that the joint distribution is the product of the marginals $\rho(q^1,q^2) = \rho_2(q^2)\rho_1(q^1)$.} That is, giving both values is equivalent to giving one and then the other. This means we can think of these relationships and quantities as real physical properties assigned to the degree of freedom and not just to the coordinates.

\subsection*{The geometry of phase space}

Now that we have an intuitive sense of the objects we want to describe physically, we can turn to differential geometry to derive the geometrical structure of the state space and then the equations of motion. What we want to capture is that we can integrate densities over two dimensional sub-manifolds in a way that is coordinate invariant. That is, we need to have a two-form $\omega$ such that:
\begin{equation}
\rho_\Sigma = \int_\Sigma \rho(\xi^a) \omega(d\Sigma)
\end{equation}
is coordinate invariant. In fact, creating a marginal distribution means giving a family of surfaces parameterized by a set of variables, that do not intersect and whose union is the whole space and then calculating the integral as a function of the variables.

The requirement that $\omega$ is a two-form, i.e. linear and anti-symmetric, comes from the fact that it needs to represent areas. That is, it needs to be linear because the area of a parallelogram is a linear function of its sides. And if $v^1$ and $v^2$ are vectors and $\omega(v^1, v^2)$ represents the area, then rotating by 90 degrees will give the same area. That is, $\omega(v^1, v^2) = \omega(v^2, -v^1) = -\omega(v^2, v^1)$. Therefore it must be anti-symmetric. It should also be non-degenerate, i.e. there is no direction for which $\omega$ is always zero or we would not be able to define a volume.

We also want the two-form to be closed, that is the integral
\begin{equation}
\int_\Sigma \omega(d\Sigma)
\end{equation}
is zero if $\Sigma$ is a closed surface. To understand why, note that it corresponds to the case where the density is the unit constant. If the surface is a parallelopiped in some variables, each face that give a positive contribution will have a parallel and equal face that would give an opposite contribution. As the closed integral needs to be zero for all parallelopipeds, it will also be zero for an arbitrary closed surface.

Under a change of variables, the two-form $\omega$ must be invariant. That is, phase space is a symplectic manifold. And, as any symplectic manifald, we can find Darboux variables\footnote{In general, this gives a general understanding of what a differentiable structure is. Any manifold, even a non-differentiable one, can identify points with coordinates. A differentiable structure on a manifold selects a set of charts that allow differentiable coordinate change. But there are many such sets, which one do we pick? We pick the one that allows us to write densities for our distributions. Similarly, a symplectic manifolds selects only the coordinates that allows us to write invariant densities. Also note how both the tangent space and the space of distributions are vector spaces. In a way, the linearity of distributions corresponds to the linearity of the tangent space.} $q^i, p_j$ and express the two-form as
\begin{equation}
\label{Symplectic}
\begin{aligned}
\omega &= dq^i \wedge dp_i = d\xi^a\omega_{ab}d\xi^b \\
\omega_{ab} &=  \left[
\begin{array}{cc}
0 & 1 \\
-1 & 0 \\
\end{array}
\right] \otimes I_n =
\left[
\begin{array}{cc}
0 & I_n \\
-I_n & 0 \\
\end{array}
\right]
\end{aligned}
\end{equation}

The components $\omega_{ab}$ let us understand what geometrical features are captured by $\omega$. It corresponds to a two dimensional vector product within each degree of freedom while it corresponds to a scalar product across degrees of freedom. As $\omega$ is preserved during coordinate transformations, the area within each degree of freedom is preserved meaning we are preserving the marginal distribution as well. Across degree of freedom, instead, we are preserving the angle meaning two perpendicular degrees of freedom remain perpendicular. But here perpedicular means independent: the density within the four dimensional volume is the product of the densities on sides. As these are preserved under coordinate transformation, these now can correspond to actual physical entities.

\subsection*{Hamiltonian mechanics for multiple degrees of freedom}

Deterministic and reversible evolution will preserve the densities and therefore preserve $\omega$. That is, $\omega(v, w) = \omega(v',w')$ where $v'$ and $w'$ are the vector evolved in time. In fancy math terms, deterministic and reversible evolution for a distribution is a symplectomorphism. As before, $S = \left(\frac{d\xi^a}{dt}\right)$ is the vector field corresponding to the evolution. The vector components change according to $v'^a = \partial_b \xi(t+dt)^a v^b = (\delta^a_b + \partial_b S^a dt) v^b$. Therefore we have:
\begin{align*}
v^{a} \omega_{ab} w^{b} &= v'^{a} \omega_{ab} w'^{b}  \\
&= (v^{a} + \partial_{c} S^{a} v^{c} dt) \omega_{ab} ( w^{b} + \partial_{d} S^{b} w^{d} dt) \\
&= v^{a} \omega_{ab} w^{b} + (\partial_{c} S^{a} v^{c} \omega_{ab} w^{b} + v^{a} \omega_{ab} \partial_{d} S^{b} w^{d}) dt \\ &+ O(dt^2)
\end{align*}
If we set $S_{b} \equiv S^{a} \omega_{ab}$, we have:
\begin{equation}
\begin{aligned}
v^{c} w^{b} \partial_{c} S_{b} &- v^{a} w^{d} \partial_{d} S_{a} = 0\\
\partial_{a} S_{b} - \partial_{b} S_{a} &= curl(S_{a}) = 0 \\
S_{a} &= \partial_{a}H \\
\frac{dq^i}{dt} &= \frac{\partial H}{\partial p_i}  \\
\frac{dp_i}{dt} &= - \frac{\partial H}{\partial q^i}
\end{aligned}
\end{equation}

We recognize Hamilton's equations for multiple degrees of freedom.

The symplectic structure was recovered by requiring invariant distributions which in turns requires invariant integration over independent degrees of freedom. Note how the requirement was given outside of the symplectic structure itself and therefore it cannot be recovered simply from that.

\section{Discussion}

Hamiltonian mechanics, through Liouville's theorem, has always had an important role in statistical mechanics. What we are showing is that that role is not a coincidence: it is the very defining characteristic. \textbf{Hamiltonian mechanics is a type of statistical mechanics}, one with no probability and deterministic and reversible laws. The fact that entropy can be defined objectively and is conserved through evolution cannot be simply seen as a curiosity.

To us, it does not make sense to start our physical discourse by imposing a state space for point-like particles, that somehow does not require only points but a particular differentiable structure as well. Then, if we are so keen, put distributions on it which turn out, somewhat magically, to have density and entropy defined in a coordinate independent way. It makes a lot more sense to start by saying we have a finite system which we pretend to be made of infinitesimal parts, which we keep to dividing as we please and derive the state space of particles as the limit of such process of subdivision. This gives uniquely gives us the actual mathematical structure and its physical meaning.

Not only the second approach is preferable because with fewer starting points it justifies the state space, the evolution and why that state space is so compatible with statistical mechanics. It also maps better to what we do in classical physics: we study composite systems (planets, cannonballs, beads on a wire, fluids) which we sometime pretend we can recursively divide. None of those objects, in fact, has a single position or momentum: only the center of mass has a single position and momentum but those are precisely the expectation values $\int q \rho(q,p) dq dp$ and $\int p \rho(q,p) dq dp$ over the distribution. In other words, particles are not point-like in classical mechanics. They are infinitesimal cells of phase-space and that's why they evolution is symplectomorphism: that's why phase space requires a differentiable structure.

Under this light, asking whether classical mechanics is tenable coincides with asking whether we can really keep dividing our object indefinitely. If one is convinced that that is not realistic then he is already convinced that classical mechanics will not hold in those regimes. And if that simplification fails, it's not that we need an altogether new set of rules: the macroscopic object is still the same. We simply need to change how the subdivision works.

Also note that this view tells us exactly what the mathematical framework of symplectic geometry is supposed to represent. The state space of particles is a manifold because we use real numbers to identify the states. Among all possible choice of state variables, we conveniently choose the ones in which the distribution can be expressed as a density. That's the differentiable structure on the manifold. The tangent and cotangent space are the tools to keep track of the process of infinitesimal subdivision and they are linear spaces because the space of all possible distributions is itself a linear space. Moreover the differentiable manifold has a symplectic structure because the density is invariant under coordinate transformation. Therefore among all possible choice of differentiable state variable we choose the canonical ones, the ones in which the density is expressed in appropriate units. Every requirement is physically motivated. No extra requirements are left.

While the power of this shift can really be appreciated in the context of our broder work, it should be clear how with relative few starting points, elements of relativity (e.g. coordinate invariance), statistical mechanics (e.g. physical distributions), quantum mechanics (e.g. wave number, conjugate quantities) and classical Hamiltonian mechanics (e.g. phase space, Poisson brackets) already emerge and form a coherent picture. In particular note how energy (in the form of the Hamiltonian) and entropy (in the form of information entropy) both come out from the single unassuming idea of invariant distributions. And note how conservation of entropy is related to deterministic and reversible evolution. Also note how deterministic and reversible evolution means the that the system is only influenced by itself, which means it is not influenced by anything else, which means it is isolated, which it's linked to energy conservation.\footnote{To be precise, conservation of energy is not an invariant property. The fact that a Hamiltonian exists, though, is an invariant property.} This surely this cannot be all coincidence.

The larger point is that physics is one and it can't be understood piecemeal. Nature does not separate between dynamical systems and thermodynamics, between topologies and $\sigma$-algebras, between classical systems and quantum systems. The boundaries between all these disciplines are as much unfortunate as they are artificial. There is no real understanding of one without the other.

Having a coherent picture across the different areas not only is intellectually more satisfying, but it allows us to switch perspective when the intuition from one fails. As a practical example, we can show how understanding the link between Hamiltonian mechanics and information theory leads very straightforwardly to a classical equivalent for the uncertainty principle.

\subsection*{Classical uncertainty principle}

As we saw, Hamiltonian dynamics not only conserves the energy of all particles, but it also conserves the information entropy of the distribution as a whole. We can imagine that there is going to be some link between the spread of a distribution and the number of bits required to identify an element. So we ask: what is the distribution that minimizes the spread given a certain amount of information entropy? If $I_0$ is the set amount of information entropy and $\sigma_q^2 \sigma_p^2 \equiv \int (q-\mu_q)^2 \rho \, dqdp \int (p-\mu_p)^2 \rho \, dqdp$ is the spread, we can use Lagrange multipliers to find out.
\begin{align*}
L = &\int (q-\mu_q)^2 \rho \, dqdp \int (p-\mu_p)^2 \rho \, dqdp \\
&+ \lambda_1(\int \rho dqdp - 1) \\ &+ \lambda_2(- \int \rho \ln \rho \, dqdp - I_0)\\ 
\delta L = &\int \delta \rho [(q-\mu_q)^2 \sigma_p^2 + \sigma_q^2 (p-\mu_p)^2 + \\ &\lambda_1 - \lambda_2 \ln \rho - \lambda_2 ] dqdp = 0 \\
\lambda_2 \ln \rho = &\lambda_1 - \lambda_2 + (q-\mu_q)^2 \sigma_p^2 + \sigma_q^2 (p-\mu_p)^2 \\
\rho = &e^{\frac{\lambda_1 - \lambda_2}{\lambda_2}}e^{\frac{(q-\mu_q)^2 \sigma_p^2}{\lambda_2}}e^{\frac{\sigma_q^2 (p-\mu_p)^2}{\lambda_2}}\\
\end{align*}
We solve the multipliers and have:
\begin{align*}
\rho = &\frac{1}{ 2 \pi \sigma_q \sigma_p} e^{-\frac{(q-\mu_q)^2}{2\sigma_q^2}} e^{-\frac{(p-\mu_p)^2}{2\sigma_p^2}} \\
I_0 = &\ln (2\pi\sigma_q\sigma_p) + 1
\end{align*}
The distribution that minimizes the spread, then, is the product of two independent Gaussians. As the entropy is conserved during Hamiltonian evolution the product $\sigma_q^2 \sigma_p^2$ can never be less than the one given by the Gaussian distribution of the same entropy. We have:
\begin{align*}
\sigma_q\sigma_p \geq \exp (I_0 - 1) / 2 \pi 
\end{align*}
Intuitively, Hamiltonian mechanics is deterministic and reversible evolution for a distribution and their elements, and therefore it can't shrink it more than a set amount. If it did, it would start concentrating the density, meaning that multiple states have part of their density mapped to a single state and the dynamics would not be reversible.

\section{Conclusion}

In this paper we have shown how classical Hamiltonian mechanics coincides with conservation of information entropy. The structure of phase space is required to be able to define densities and entropy in a way that is coordinate invariant, and therefore physically meaningful. This clarifies the link between Hamiltonian mechanics, statistical mechanics and deterministic and reversible motion.

In the narrower context, it allows to understand what physical object is represented by each mathematical one. This can be summarized in table \ref{dictionary}.
\begin{table}[h]
	\centering
	%	\begin{tabular}{p{0.2\textwidth} p{0.1\textwidth} p{0.1\textwidth} p{0.5\textwidth}}
	\begin{tabular}{c p{0.3\textwidth} p{0.5\textwidth} }
		& Name & Meaning\\ 
		\hline 
		& Classical particle & an infinitesimal part of the system (i.e. the limit of recursive division) \\ 
		$\mathcal{S} =\mathsf{T}^*\mathcal{Q}$ & Phase space \newline (Tangent bundle) & the set of all possible states for particles upon which invariant distributions can be defined \\
		$\xi^a$ & State variables \newline (Unified coordinates) & the set of variables needed to identify the state of a particle \\ 
		$\rho(\xi^a)$ & Density & the amount of material for a given particle state (i.e. in the limit of the recursive division)\\ 
		$I(\rho(\xi^a))$ & Information entropy & the number of bits to identify an element of the distribution to a unitary level of precision\\ 
		$q^i$ & Coordinate & a state variable that also defines a unit \\
		$\mathcal{Q}$ & Coordinate space \newline (Configuration space) & the space charted by all coordinates \\
		$k_i$ & Conjugate coordinate & a state variable that uses the inverse unit of the corresponding coordinate \\
		$\hbar$ & & the unit for measuring the range of possible states within a single degree of freedom \\
		$p_i=\hbar k_i$ & Conjugate momentum & a state variable that together with the corresponding coordinate defines invariant ranges of possible states \\
		$(q^i, p_i)$ & Canonical variables \newline (Canonical coordinates) & a set of variable for which the density is invariant and expressed over units of $\hbar$\\ 
		& Canonical transformation & a change in state variables that does not change the density\\ 
	\end{tabular}
	\caption{Dictionary between mathematical and physical objects.}
	\label{dictionary}
\end{table}
It also allows us to see that deterministic and reversible evolution must conserve entropy and that isolated systems must conserve energy. In the broader context, it allows to make deeper connections between different areas of math and physics, and pushes us to rethink our starting points. In particular, it tells us that classical particles should not be considered point-like objects (upon which entropy and densities cannot be defined), but the limit of recursive subdivisions (i.e. infinitesimal cells of phase space).

These insights come from our project, Assumptions of Physics, that aims to re-derive the standard laws from a handful of physical starting points. We believe that a reorganization of fundamental physics on more physically meaningful conceptual footing and more rigorous mathematical grounds is not only possible, but long overdue. The insights presented here are one step toward that direction as we know them to be still valid and useful in a larger setting, such as understanding the similarities in the mathematical structures of classical and quantum mechanics.

\begin{thebibliography}{0}
		
	\bibitem{North} Jill North; The "Structure" of Physics: A Case Study, The Journal of Philosophy 106, no. 2 (2009), Pages 57-88, http://www.jstor.org/stable/20620153.
	
	\bibitem{Curiel}Erik Curiel; Classical Mechanics Is Lagrangian; It Is Not Hamiltonian, The British Journal for the Philosophy of Science, Volume 65, Issue 2, 1 June 2014, Pages 269–321, https://doi.org/10.1093/bjps/axs034
	
	\bibitem{Barrett1}Thomas William Barrett; On the Structure of Classical Mechanics, The British Journal for the Philosophy of Science, Volume 66, Issue 4, 1 December 2015, Pages 801–828, https://doi.org/10.1093/bjps/axu005

	\bibitem{Barrett2}Thomas William Barrett; Equivalent and Inequivalent Formulations of Classical Mechanics, The British Journal for the Philosophy of Science, , axy017, https://doi.org/10.1093/bjps/axy017

	\bibitem{carc1} G. Carcassi, C. A. Aidala: \emph{Assumptions of Physics} (in preparation). http://assumptionsofphysics.org/book/, Ann Arbor, MI, USA, 2018.
	
	\bibitem{carc2} G. Carcassi, C. A. Aidala, D. J. Baker and L. Bieri; From physical assumptions to classical and quantum Hamiltonian and Lagrangian particle mechanics, Journal of Physics Communications, 2, 4, 045026, 2018.

	
\end{thebibliography}

\end{document}
