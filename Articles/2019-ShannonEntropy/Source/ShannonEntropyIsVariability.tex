\documentclass{article}


%\usepackage{arxiv}


\title{Shannon entropy as the variability of elements within a distribution}

\author{
	Julian Barbour, 
	Gabriele Carcassi
}


\begin{document}

\maketitle

\begin{abstract}
TBD
\end{abstract}




\section{Introduction}

Current disputed status of Shannon entropy as a physical object.

Immense regularity in nature leads to categorization of elements intro distributions. Stress that these distributions may be of different nature? I.e.: statistical distributions of elements that exist at the same time, probability distributions of events that may happen given similar circumstances, credence distributions of expectation for an agent that something will happen. May be better to have a another word instead of "probability" when talking about the elements of the distribution. Maybe "weight" or "fraction"?

A natural question is to ask how much variability an element has within a distribution. Tempted to just count the number of cases over which the distribution is defined. But the shape also affects the variability (e.g. a distribution concentrated in few values means less variability of the elements than one equally distributed).

In this paper we will show that $- \sum p_i \log p_i$ is the most natural form to describe the variability of the elements of a distribution.

(NOTE: defining the concept like this is actually broader than Shannon entropy, which assumes source/destination/message.)

\section{Variability within a distribution}

The general setting is the following. We have a set of elements $E = \{e_\alpha\}_{\alpha=1}^N$. These can represent the galaxies in the Laniakea Supercluster, the animals in the Gal\'{a}pagos islands, the molecules in a box of gas given a particular macrostate, the different takes of spin measurement for a given quantum state, or the words in the complete works of Shakespeare. The choice of the particular set is driven by the interest and practices of scientists and their fields of study and therefore it is, in this sense, discretional. Yet, it must be objective in at least one respect: once a choice is made the actual elements are the same for everybody. Once we arbitrarily decided we will study the galaxies in the Laniakea Supercluster, it is a matter of fact that the Milky Way will be included. This also means that, whatever properties those elements will have, they will represent a matter of fact about that set. In turn, this will make some choices arguably more appropriate or interesting than others. For example, in taxonomy the set of all animals that have feathers may be more interesting than the set of all animals that can fly: the first includes only birds while the second includes most birds, a few mammals and a lot of insects.

Once a set of elements is chosen, we select a property or a set of properties we want to use to characterize the elements. That is, we have a set of possible values $Q=\{q_i\}_{i=1}^I$ and a map $q : E \to Q$ that associates a value for each element. These can represent the galaxy types, the genus of the animal, the possible states of the molecules, the possible values for spin or the spelling of words. This will give us a sequence $\{q(e_\alpha)\}_{\alpha=1}^N$ of the descriptions that are associates with each element in the set. Again, the choice of properties is ultimately discretional, grounded on what particular aspects of the elements we are studying in a given case. Yet, like before, once a choice is made the value for each element is objectively defined. Once we arbitrarily decided we are studying galaxy types it is a matter of fact that the Milky Way is a barred spiral galaxy. Again, one may find that some choices are arguably more appropriate or interesting than others.

Having decided what elements to study and the level of description we are interested in, we bin them; that is, we group them based on that description, disregarding the identity of the particular element. What we will be interested in is only the relative frequency $p_i = N_i / N = \left| \{e_\alpha \in E | q(e_\alpha) = q_i \} \right| / N$ of the elements within each bin. We may do so because we are either not interested or not able to further distinguish the elements. Whatever the reason, once the choices of elements and properties are made, this relative frequency is objective. The nature of the $p_i$ will depend on the previous choices. It may represent the fraction of elements if the set is constituted by a group of objects. It may represent a probability of an outcome is the set is constituted by different takes of similarly prepared systems. If the set is infinite, it will represent some type of convergence. We will therefore call $p_i$ weights, to be clear that we make no commitment on whether we have fractions, frequentist probabilities, Bayesian probabilities or other notions.

Now that we have constructed our distribution, we want to construct an indicator $S$ that quantifies how much variability the elements exhibit within the distributions. That is, we want to quantify the degree of diversity that the values can have within the distribution. To that end, we want to define some suitable requirements for $S$.

We may be tempted to use standard statistical quantities, like the range or the variance, but this is not possible. First, if the values associated to the bins are non-numerical (e.g. types of galaxies, words), statistical quantities are not well defined. Second, permutation of the values does not have an impact on variability of the elements, while statistical quantities will in general be affected. This tells us that our indicator cannot depend on $Q$ itself, but only on the weights $p_i$. That is, we require that $H=H(p_i)$.\footnote{Note that for continuous quantities the weights $p_i$ are affected by the choice of $Q$. We will concentrate on this problem later.}

We  expect small changes in a distribution, small changes of the weights, to produce small changes in variability, therefore we require $H$ to be a continuous function with respect to the $p_i$. We also expect that if we have a uniform distribution $p_i = 1/I$, then greater $I$ should correspond to greater variability. Therefore, in the case of uniform distribution, we require $H$ to be monotonically increasing with $I$.

As we noted before, the level at which we describe the elements is not absolute and can change. For example, we may choose to group the animals in the Gal\'{a}pagos islands first by class (e.g. mammals, birds, reptiles) and then later refine the mammals by species. What we would like to say is that the variability of the overall distribution $H_T = H_C + p_M H_M$ will be the variability over the classes $H_C$ plus the variability of the mammals $H_M$ weighted by the fraction of mammals $p_M$.

To sum up, we have the following three requirements:
\begin{enumerate}
    \item $H$ depends only on $p_i$ and it does so continuously
    \item If $p_i=1/I$ then $H$ is a monotonically increasing function of $I$
    \item Let $p_i$ and $q_j$ be the weights for two distribution respectively over $I$ and $J$ bins. Let $r_k$ be the distribution over $K=I+J-1$ bins constructed by expanding the $a$-th bin of the first distribution using the second distribution. More specifically, let $1 \leq a \leq I$, then $r_k = \{p_1, p_2, ..., p_{a-1}, p_{a}q_1, p_{a}q_2, ..., p_{a}q_J, p_{a+1}, ..., p_I \}$. Then $H(r_k) = H(p_i) + p_{a} H(q_j)$.
\end{enumerate}

We note that these are exactly the requirements Shannon put forth for his entropy\cite{Shannon}, and that these requirements lead to only one possible option: $H(p_i) = - \sum p_i \log(p_i)$.

The Shannon entropy, then, is a clear and important attribute of a distribution: it quantifies the variability of the elements within the distribution. It quantifies the variety of values within the distribution. It is true that the choice of the elements and binning are not fixed, it is true that the meaning of the weights depends on what the distribution is describing, and therefore that the precise meaning of this variability is context dependent. But this is true for any mathematical object: a real number may represent mass, color in the frequency spectrum, the total money supply, the half-life for an isotope a probability and so on. And like a real number represents a quantity, transcending what is physically quantified, the Shannon entropy represents the variability of the elements in a distribution, transcending what the distribution physically signifies.

The Shannon entropy may represent uncertainty in some cases, if the weights are probabilities or credences, but not in the general case. If the weights portray the fraction of the elements that have a certain property, like the fraction of galaxies in the Laniakea Supercluster that are barred spirals, there is nothing uncertain about the distribution. In this case, the Shannon entropy represents how much variation we have within galaxies in terms of galaxy types.

The Shannon entropy may represent knowledge in some cases, but not in general. Consider the following two cases:
\begin{enumerate}
    \item There is 50\% chance you won one million dollars in the lottery and 50\% chance you didn't;
    \item There is 50\% chance you won one million dollars in the lottery and 50\% chance you won half a million dollars.
\end{enumerate}
The distribution in both cases is the same, two bins 50\% chance each, and so is the Shannon entropy. Yet, you know more in the second case: you know you have won at least half a million dollars.

Unfortunately, entropy in general is often associated with vague characterizations as the ones presented. It is said to represent uncertainty, knowledge, lack of knowledge or disorder depending by the authors, which leads to confusion and misunderstanding.  The characterization of Shannon entropy we have given, on the other hand, applies to all cases, it is enough to understand the assumptions required to rederive it and therefore is more fundamental. If we look at the galaxies in our universe, what variability do they exhibit in terms of their types? If we look at the animals in the Gal\'{a}pagos island, what variability is expressed in terms of their species? If we look at the molecules in a given macrostate, what variability do they express in terms of their microstates? If we look at the words in the complete works of Shakespeare, what variability we find in its vocabulary? All these questions are interesting, all these questions are profound and all these questions are answered by a single concept: the Shannon entropy.

If the Shannon entropy is a measure of variability, why is it connected to information? What is it measuring and in what units?

\section{Units of variability}

We have seen that the Shannon entropy measures the variability within a distribution but, in the end, it is a numeric quantity and we cannot expect to understand the concept if we don't understand what the number represents and in what units.

We can gain insight with the following example. Suppose we fix a distribution, say the animals in the Galap\'{a}gos islands. Suppose I pick a specific animal from the distribution and you want to know its species. Suppose the only way for you to get that information is to ask me a series of questions. Suppose each questions has two possible answers, yes or no. How many questions would you have to ask? In other words, we are playing a game of Twenty Questions.

Not all questions will be able to extract the same amount of information. Some questions, like ``is it an animal?" would be redundant. Others, like ``is it an American Flamingo (Phoenicopterus ruber)?", would give us a lot of information in the positive case but little in the negative case. It should be clear, though, that there must be a limit to how few questions you must ask to get to the answer: a single yes/no question cannot be enough. It should, hopefully, be intuitive that to a greater variability within the distribution will correspond a greater number of questions you must ask. That is exactly what the Shannon entropy measures: the minimum average number of questions one has to ask to identify a value in the distribution. It gives us the number of questions for an ideal strategy for our game of Twenty Questions.

If we have binary questions, the logarithms will be in base two and the unit for Shannon entropy will be bits. It will indicate the average number of yes/no questions we need to identify an element within the dsitribution. In general, you can pick any base $b$: in base three we have ternary questions and trits, for ten we have questions with ten possible answers and digits. We can also pick a non-integer base, like the natural base for logarithms, and we will have nats. This is why the Shannon entropy is fundamental in information theory, because it quantifies how much information is needed to transfer a value from a distribution.

Defining a set of questions means choosing an encoding as we are choosing how the information gets codified into our series of bits. To understand how this works, we can briefly review the Hauffman coding, which is the optimal algorithm for symbol-by-symbol coding with a known probability distribution. The idea is that we want all possible answers to each question to be balanced, to provide the same amount of information. The reason is that making one answer more specific (i.e. it applies in fewer cases) means making another less specific (i.e. it applies in more cases). In the case of binary questions, then, we want the probability to answer yes or no to be 50\%.

For example, suppose we have a population of pets in a country is as follows:
\begin{itemize}
    \item dogs 27\%
    \item cats 48\%
    \item fish 10\%
    \item bird 8\%
    \item small mammal 4\%
    \item reptile 3\%
\end{itemize}
For the first question, we group cats on one side and everything else on the other, to form a 48/52 split. So we can ask ``is it a cat?". If the answer is yes, we finished. If not, we need to continue. We can group dogs on one side and everything else on the other to form a 27/25 split. So we can ask `` is it a dog?". If the answer is yes, we are done. If not, we continue. We can group fish with reptile and bird with small mammal to form a 13/14 split. So we can ask ``is it a fish or a reptile?" If the answer is yes, the followup question would be ``is it a fish?" If the answer is no, the followup question would be ``is it a bird?" With this scheme, the enconding is as follows:
\begin{itemize}
    \item dogs 27\% - 01 - 2 bits
    \item cats 48\% - 1 - 1 bit
    \item fish 10\% - 0011 - 4 bits
    \item bird 8\% - 0001 - 4 bits
    \item small mammal 4\% - 0000 - 4 bits
    \item reptile 3\% - 0010 - 4 bits
\end{itemize}
The average bit for encoding is $(.48) * 1 + (.27) * 2 + (.1 + .08 + .04 + .03) * 4 = 2.02$ bits while the Shannon entropy is 1.98 bits. That is, the coding is very close to the ideal case.

It is important, at this point, to understand that the technical use of the term information in information theory does not equate to the normal use of the term which refers to knowledge, intelligible data. The bits by themselves, the yeses and the nos, the ones and the zeros, do not provide knowledge. They need the context of the questions and the distribution to become actual information. For example, when opening a jpeg file, the file itself does not contain the instructions of how to read it. If you do not happen to know what a jpeg is and how to read it, you are not going to be able to interpret, to decode, the string of bits into actual intelligible data. The questions, the distribution, the context are considered given, communicated out-of-band through another scheme. As with any semantic content, this cannot be easily formalized and quantified.

The Shannon entropy of the distribution, then, has nothing to do with the information the distribution itself holds. The distribution is not what is being encoded. The entropy is the information needed to go from the distribution, which is given, to an individual element. It's really the information gap from the population to an element. That is why some people say the entropy is ``lack of information", because it's the information the distribution does not have. But, again, this is just missing the point of what it is being described. If one is not interested in identifying elements there is no ``missing information".

In communication and information theory, information is really encoded information. Communication systems and information processors have no idea whether the source of the data is a digital thermometer or a pressure sensor. There is no knowledge per se, just symbol manipulation that may represent different concepts in different contexts.

The term information entropy, then, is unfortunate for two reasons. First, because is not really entropy in the thermodynamic sense: it is not defined on states, it does know about irreversible processes and it is unrelated to maximization at equilibrium. In fact, it is not related to physical systems. Second, it is not really information in the general sense, only in the very narrow technical sense of coded information within a communication or information system.

So why is it so successfully used in statistical mechanics? Why does it seem to play a fundamental role even for systems far from equilibrium?

\section{Connection to statistical mechanics}

As we saw in the previous sections, Shannon entropy is a very general concept that applies to any type of distribution, no matter what the problem domain. To understand its link to the thermodynamic/statistical mechanics entropies we need two answer two questions. First, what does the Shannon entropy represents in the context of statistical mechanics? Second, why does the Shannon entropy have the same properties of the thermodynamic entropy? That is, we have to understand why it increases during irreversible evolution, it is conserved during reversible evolution, it is maximized at equilibrium and it is additive for independent systems.

The first question is straight-forward to answer. The macroscopic variables of a system (e.g. volume, pression, energy) do not give a full description of its pars. If we fix a macrostate and then measured the state of all microscopic constituents, we would not find the same answer every time. The choice of a macrostate, then, identify a set of possible microstates that are compatible with it. It is then valid to ask the question: what is the variability of microstates one can find within a given macrostates? This would be answered by the Shannon entropy, which will measure the variability in bits required to identify a microstate given the macrostate.

For the others, suppose we have a volume of gas at a particular temperature and pressure in perfect equilibrium. While the microstate is constantly changing, the macrostate remains the same. Effectively, the equilibrium is simply permutating elements of the distribution. As we saw, permutations do not change the variability which has to remain constant. At equilibrium the Shannon entropy has to remain constant.

Suppose we compress the gas adiabatically and then let it re-expand adiabatically to the original state. As the system is isolated, the whole process is deterministic and reversible. An initial microstate has to map with a specific intermidiate and final microstate. Identifying an element at each stage is equivalent to identifying an element in any state. The variability within the distribution at each step, then, cannot change: the same number of bits, the same information, must be enough to identify initial, intermediate or finale microstates. Deterministic and reversible processes conserve Shannon entropy.

Now suppose we have a process that starts from an out-of-equilibrium state and ends in an equilibrium. The initial state will not be fully described by the standard macroscopic variables. In fact, there will be many out-of-equilibrium states that will lead to the same final equilibrium. In this sense the process is irreversible because knowing the final macroscopic variables will not be enough to reconstruct the initial macroscopic state. Different possible initial out-of-equilibrium states merge into a final equilibrium state.

If we assume, though, that the microstate evolves deterministically and reversibly, knowing the final microstate would allow us to reconstruct the initial microstate and what initial out-of-equilibrium macrostate would be compatible with it. As different out-of-equilibrium macrostate merge during the evolution, they merge the possible initial conditions that lead to that same macrostate. The variability of microstates within the initial out-of-equilibrium macrostate, then, must be smaller than the variability of the microstates withing the final equilibrium state. The amount of information needed to identify a microstate at equilibrium allows us to identify the initial microstate across multiple out-of-equilibrium states and therefore is greater than the information needed to identity a microstate in an out-of-equilibrium macrostate. Irreversible processes increase the Shannon entropy which, as we saw before, stabilizes at equilibrium.

Now suppose we have two independent volume of gasses. This means that picking the microstate of one system does not constrain the microstate of the other. That is, the distribution over microstates are statistically independent. In this case, the Shannon entropy is additive.

The Shannon entropy, then, has all the right properties to establish a conceptually meaningful connection with the thermodynamic/statistical mechanics entropies. Yet, there are some mathematical details that need to be worked out. These details don't change the overall intuitive picture, but they present yet again opportunities for confusion.

The first is the role of the Boltzmann constant $k_B$, which is a conversion factor between the Shannon entropy $S = - \sum p_i \log(p_i)$ and the Gibbs entropy $S_G = - k_B p_i \sum \log(p_i)$. The issue here is that temperature, internal energy and entropy are typically linked by the relationship $T = \frac{\partial U}{\partial S}$. Historically, temperature and internal energy are the quantities that were defined first. If we measure temperature in Kelvin and energy in Joules, the above relationship forces us to measure entropy in Joules/Kelvin. If we changed units, $k_B$ would change accordingly.

However, this is not the only way to define the relationships between those quantities. Another way is to have $\beta = \frac{1}{k_B T} = \frac{\partial S}{\partial U}$. In this setting, $U$ is measured in Joules, $S$ in nats (which are dimensionless), $\beta$ is measured in inverse Joules. Temperature is still measured in Kelvin, but we do not regard it as a primary objects. We find this approach, sometimes called entropy first, prefereable for a number of reasons, expecially in statistical mechanics. First, entropy is invariant under coordinate transformations and therefore it is better to keep it a pure number and not mix it with units of energy, which is not invariant under boost.\footnote{This is essential if one ever hopes to put statistical mechanics in a relativistic setting.} Second, the maximization of entropy at equilibrium guaranties us that $S(U)$ is a single valued function (i.e. the maximum given the value of $U$) and $\frac{\partial S}{\partial U}$ is therefore well defined. On the other hand, $U(S)$ is not in general single value (e.g. in the Ising model). This means that temperature can be infinite where the curve inverts, where small changes of energy do not change the entropy, and can be negative in the other region. That is, the infinitely cold system is temperature zero, which cannot be achieved, as it warms up temperature rises until it become infinite, and as the system becomes even hotter the temperature is negative and increase toward zero, which would correspond to an infinitely hot system. On the other hand, $\beta$ is always finite and cannot be infinite. An infinitely cold system, then would correspond to positive infinite $\beta$, as the system warms up beta decreases, passes from zero, becomes negative and an infinitely hot system corresponds to minus infinite $\beta$: this is a much better property for a physically meaningful quantity. Third, when setting up entropy maximization, $\beta$ is quite naturally the Lagrange multiplier associated with energy conservation, therefore mathematically it has a much clearer role. Lastly, one can proceed in both thermodynamic and statistical mechanics with just $\beta$, and convert to temperature only when we need a value on the Kelvin scale. For these reasons, it seems to us $\beta$ plays the more fundamental role and $k_B$ is the conversion coefficient to go from $\beta$ to temperature measured in Kelvin. Adding and removing the Boltzmann constant from the entropy expression, then, is equivalent to shift from an energy/temperature first approach to an entropy/$\beta$ first approach and should be not seen as physically significant.

The other point of possible confusion is that in statistical mechanics there are actually two distributions. A single microstate is itself a distribution of particles over their particle states. There are actually $N$ particles existing at the same time, each with its own particle state. And a macrostate is a probability distribution over microstates. It is important to keep these two distributions straight, especially because a microstate can be represented in two ways.

The first way is as a distribution over a single copy of phase space, sometimes referred to as $\mu$-space. In this case the distribution represents the fraction of particles that are in that state. The Maxwell-Boltzmann distribution $\rho(q^i, p_i) = \left(\frac{\beta m}{2\pi} \right)^{3/2}e^{-\beta \frac{p_ip^i}{2m}}$ is expressed in this space. On $\mu$-space, the Shannon entropy $-\int\rho \log \rho$ does not coincide with the thermodynamic entropy for a simple reason: it does not depend linearly with the number of particles. It is not an extensive quantity. The reason is straight forward: the Shannon entropy works on normalized distributions and gives us the variability of a single element. In this case, the system is composed by $N$ elements, not one. One can use the expression $-N\int\rho \log \rho$ as this represents the variability of $N$ elements.\footnote{This is actually an approximation of the multinomial distribution for large N, which would give the precise result.}

Alternatively, a microstate can be represented as a point over $N$ copies of phase space, sometimes referred to as $\Gamma$-space. It is on this space that the Gibbs entropy is defined and where thermodynamic entropy happens to coincide with the Shannon entropy. Since this is a probability distribution, the microstate corresponds to a single element within the distribution defined by the macrostate. Yet, one has to be careful of two things. The first is that permutations of particles within $\Gamma$-space lead to different points while should not lead to different microstates. This leads to the widely known problem of overcounting which needs to be addressed in the standard way. The second problem is that we should not think of the particles as literally pointlike: they would correspond to delta Dirac distributions over phase-space which have minus infinite Shannon entropy. It is more appropriate, both mathematically and conceptually, to think of indistinguishable particles as identical distributions over $\mu$-space, each contributing the same amount of Shannon entropy.\footnote{Setting to $-\log h$ the entropy corresponding to each degree of freedom of these single particle distributions is a natural way to incorporate the effects of the uncertainty principle.}

The last point of confusion is the invariance of entropy under coordinate transformations. The use of phase-space in statistical mechanics is of paramount importance for one reason and one reason only: it is the only type of manifold where entropy can be defined in a way that is coordinate invariant. The issue is that we do not define fractions/probabilities over continuous variables: we defined densities. For a discrete case, we have a finite amount, like 3\%. For a continuous case we have an infinitesimal contribution, like 3\% per meter. If we switch labels in the discrete case, 3\% remains 3\%. If we switch to kilometers in the continuous case, we have 3000\% per kilometer. The continuous version of the Shannon entropy is affected by a coordinate transformation by the integral of the logarithm of the Jacobian determinant. Any transformation where the Jacobian determinant is not unitary, where the volumes are not preserved, does not preserve the Shannon entropy. This is not a problem for the Shannon entropy in general. It is a problem if we want to give the Shannon entropy any physical meaning.

What happens in phase space is that, under coordinate transformation $\hat{q}^i = \hat{q}^i(q^j)$, the $d\hat{q}^i = \frac{\partial \hat{q}^i}{\partial q^j } dq^j$ vary like vector components while $d\hat{p}_i = \frac{\partial q^j}{\partial \hat{q}^i } dp_j$ vary like covector components. Therefore the areas $d\hat{q}^i d\hat{p}_i = dq^j dp_j$ remain the same. The Jacobian determiants are unitary and the Shannon entropy is invariant. It is not the invariance of the areas over Hamiltonian evolution that is important: it is the invariance over coordinate transformations. Invariance over time would be insignificant if we first didn't have a quantity that was observer independent.

This means that we always have to consider distributions over the full phase-space with the appropriate conjugate coordinates if we want to use $-\rho \log \rho$. If we use distributions over position and velocities or over momentum only, the expression we changed for those particular variables in those coordinate systems.

These technical details are important to not get confused, but they do not alter the conceptual point. The Shannon entropy and the thermodynamic entropy, though describing different things, are conceptually related. The variability of the state of the parts within the state of the whole is what the Shannon entropy quantifies if applied to the macrostates of thermodynamics and the microstates of statistical mechanics. This variability increases during irreversible evolution, more microstates converge to the same macrostate until equilibrium is reached. The variability of one system does not affect the variability of an independent one and therefore it simply sums. This is not a coincidence: this is what thermodynamic entropy ultimately is about.


\section{Conclusion}

\bibliographystyle{unsrt}  
\bibliography{references}



\end{document}
