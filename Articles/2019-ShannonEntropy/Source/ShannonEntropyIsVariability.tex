\documentclass{article}
\textheight 8.9 in \textwidth 6.5 in \oddsidemargin -10pt \topmargin
-30pt

\usepackage{amsmath}
%\usepackage[usenames,dvipsnames]{color}



\title{Universal Characterization of Physical Variability by \\Shannon's $-\sum_ip_i\,\textrm{ln}\,p_i$ Formula}

\author{
	Gabriele Carcassi,
	Julian Barbour and
	Christine A. Aidala 
}


\begin{document}

\maketitle

\tableofcontents

\begin{abstract}
TBD
\end{abstract}




\section{Introduction\label{int}}

The use of Shannon entropy $-\sum p_i \log p_i$ is well established in very different fields of knowledge such as computer science[cite], physics[cite] and ecology[cite]. Within computer science it is a fundamental element in information theory since Shannon developed his mathematical theory of communication. It has been used extensively within statistical mechanics to link the microscopic description of a system to its macroscopic thermodynamic one. It has been used in ecology to characterize biodiversity and calculate species distributions under given constraints. Entropy maximization has been shown to be a powerful tool.

While the techniques are well developed, it is puzzling that this concept can be applied in such different contexts, given that it assumes different features in each one. For example, in information theory the Shannon entropy for a book will change whether we choose to encode words or letters, which means the quantity does not have a fully objective feature: it depends on an arbitrary choice. This is not problematic for an information theoretic quantity, but it is for a physical quantity. In statistical mechanics the $p_i$ represent probabilities, which means the Shannon entropy is a property of an ensemble of physical systems. This leads some to claim that the Shannon entropy is epistemic in nature and represents ignorance. [Add references] In ecology, however, the $p_i$ represent the fraction of individuals belonging to the $i$th species. There is no probabilistic ensemble to which one can associate a degree of ignorance. Finally, thermodynamic entropy is associated to the notion of states and irreversible time evolution, which are not well defined concepts for the other fields. The same expression, then, seems to have different traits and it is not clear why a single concept would apply in all these cases.

To make matters worse, it is fairly typical to describe entropy with notions such as uncertainty[cite], knowledge[cite], lack of knowledge[cite], disorder[cite]. While some of these characterizations are useful in some contexts to give an intuitive picture, they do not always seem to work. A common objection to the characterization as disorder, for example, is that a uniform distribution corresponds to a homogeneous system, which one would expect to be maximally ordered; yet the distribution maximizes the Shannon entropy, which would indicate maximal disorder. Moreover, these notions remain vague in the sense that the formula is not formally derived from those concepts, and therefore it is not clear why that particular indicator would be an appropriate choice. We are left to wonder: is there really some single underlying concept, that, when applied appropriately in the different fields, reduces to the correct notion of entropy? Or is it just a big coincidence?

We propose that there is indeed a characterization of Shannon's formula that is both more general and more appropriate; that is independent of whether it is applied to sources of messages, ensembles of microstates or animal populations. We will argue that the formula is a measure of the \emph{variability of the values within a distribution of a given set of elements}. That is, given a distribution of some elements over some quantity, the Shannon entropy represents how varied, how different from each other, the values are. In fact, it is the only continuous and linear index that measures the degree of variability within a distribution, and those requirements lead naturally to the same three conditions that Shannon set for his formula.\cite{Shannon} In other words, the concept of variability within a distribution leads in a formal way to the correct expression. In all cases where the formula is applied, in fact, we are using distributions as objects of our investigation and the variability within that distribution will be of interest. Yet, the physical meaning of those distributions can be widely different: they may represent the fractional distribution of animals according to their species; they may represent probability distributions of different outcomes; they may represent knowledge of certain facts as in Bayesian statistics. Therefore the variability itself, in the context of these different cases, will necessarily acquire a different meaning. In particular, we will see how this perspective naturally unfolds in the contexts of information theory and statistical mechanics. 

As we believe this is the right characterization, we will therefore use the expression Shannon ``variability'', rather than ``entropy'', ``information'' or ``diversity index'', as it tells us exactly what the quantity describes. It is well known that when Shannon recognized the importance of his formula in communication theory he sought von Neumann's advice on the name he should give to the expression he had found. He was told to call it \emph{entropy}, first, because Boltzmann had used an identical formula to define entropy in statistical mechanics and, second, nobody understood entropy and that would provide protection from criticism [exact words to be found]. We believe von Neumann's advice was not totally helpful and has given rise to confusion. We will reserve the word ``entropy'' for the thermodynamic entropy, the one dealing with physical states and irreversible processes.

This characterization also explains why the Shannon variability has such a broad range of applicability. As nature presents itself in a multitude of forms and combinations, a fundamental task in science is to recognize which objects belong together and study how their different attributes are expressed in different ways or degrees. They may be galaxies, animals or outcomes of the same process; we may study their shape, behavior or numerical quantities. But the idea of grouping them and trying to understand the source of their variation is a common endeavor in science.\footnote{In fact, determining which collection of objects is significant and what attributes are relevant is a significant part of the work.} Measuring and explaining this variability is an essential task. It is this very variety that is the inspiration for and the subject of scientific investigation. A universe without variety, where all objects were the same and remained the same, is barely conceivable and certainly it would leave no space for scientific investigation. In the words of Leibniz, the principle underlying the universe is that it should realize ``as much variety as possible, but with the greatest order possible''.\footnote{MOVE TO BIBLIO Paragraph 58 of Leibniz's \emph{Mondadology}} 

As a final note, we want to stress that the goal of this work is not to present some new technical result. The goal is instead to synthesize all details into a coherent picture that gives them context and a precise relationship to each other. This can help, especially the new practitioners, to avoid common misconceptions. Moreover, while some details presented here may be well known in a particular community, we find that they may be new to another. For instance, we will show an example of Huffmann coding to illustrate the meaning of the numerical value associated with the Shannon variability. This is standard practice in computer science courses, yet it is not in statistical mechanics. Therefore restating all the crucial aspects in one place is beneficial to make the connection between them readily available.


%https://www.researchgate.net/publication/259011656_A_tribute_to_Claude_Shannon_1916-2001_and_a_plea_for_more_rigorous_use_of_species_richness_species_diversity_and_the_'Shannon-Wiener'_Index

%https://web.stanford.edu/~hastie/Papers/maxent_explained.pdf

%https://www.mdpi.com/1099-4300/21/8/794/htm

%https://www.sciencedirect.com/science/article/abs/pii/S0169534714001037

%Leibniz
%We recall, though we have been unable to relocate, a sentence in one of Leibniz's many writings to the effect that reality \emph{is} variety. Whether he made the statement or not, it is consistent with his view that any contingent thing is defined by a collection of attributes, which necessarily must be distinguishable at some epistemic level. Paragraph 58 of Leibniz's \emph{Mondadology} asserts that the principle underlying the universe is that it should realize ``as much variety as possible, but with the greatest order possible''. [REF]

%We conclude this section with a word on terminology. In mathematics, Cantor defined a set as ``a collection into a whole $M$ of definite and separate objects''. He called the objects \emph{elements}. We will also use the word `elements' to denote the things we consider but subject to the proviso that they have a distinguishing feature (or features) that allows us to say that \emph{they are of the same kind}. Thus, we consider sets of a restricted type. The point is this. To constitute a set in accordance with Cantor's definition, it is only necessary for its elements to be \emph{definite and separate} objects. This leaves total freedom as to how such objects in the universe are to be parcelled up into specific sets. For example, a set might consist of Mars and the word `go'. We think such latitude misses one of the most significant aspects of the universe, namely, that the things within it can be distinguished by attributes that are much more specific than the two required by Cantor. Our field of investigation goes beyond mere number, which washes out all attributes apart from the two, of being definite and separate, that allow us to count. They were the only attributes that Cantor needed; we need more.


\section{Variability within a Distribution\label{vwd}}

The general setting is the following. We have a set of elements $E = \{e_\alpha\}_{\alpha=1}^N$. In accordance with the comments made in our introduction, they can for example represent the galaxies in the Laniakea Supercluster, the animals in the Gal\'{a}pagos islands, the molecules in a box of gas given a particular macrostate, the different outcomes of spin measurement for a given quantum state, or the words in the complete works of Shakespeare. The choice of the particular set is driven by the interests and practices of scientists and their fields of study and therefore it is, in this sense, discretionary. Yet, it must be objective in at least one respect: once a choice is made the actual elements are the same for everybody. Once we arbitrarily decided we will study the galaxies in the Laniakea Supercluster, it is a matter of fact that the Milky Way will be included. This also means that, whatever properties those elements will have, they will represent a matter of fact about that set. In turn, this will make some choices arguably more appropriate or interesting than others. For example, in taxonomy the set of all animals that have feathers may be more appropriate than the set of all animals that can fly: the first includes only birds while the second includes most birds, a few mammals and a lot of insects.

Once a set of elements is chosen, we select a property or a set of properties we want to use to characterize the elements. That is, we have a set of possible values $Q=\{q_i\}_{i=1}^I$ and a map $q : E \to Q$ that associates a value to each element. These can represent the galaxy types, the genus of the animal, the possible states of the molecules, the possible values for spin or the spelling of words. This will give us a sequence $\{q(e_\alpha)\}_{\alpha=1}^N$ of the descriptions that are associated with each element in the set. Again, the choice of properties is ultimately discretionary, grounded on what particular aspects of the elements we are studying in a given case. Yet, like before, once a choice is made the value for each element is objectively defined. Once we arbitrarily decided we are studying galaxy types it is a matter of fact that the Milky Way is a barred spiral galaxy. Again, one may find that some choices are arguably more appropriate or interesting than others.

Having decided what elements to study and the level of description we are interested in, we bin them; that is, we group them based on that description, disregarding the identity of the particular element. What we will be interested in is only the relative frequency $p_i = N_i / N = \left| \{e_\alpha \in E | q(e_\alpha) = q_i \} \right| / N$ of the elements within each bin. We may do so because we are either not interested or not able to further distinguish the elements. Whatever the reason, once the choices of elements and properties are made, this relative frequency is objective. The nature of the $p_i$ will depend on the previous choices. It may represent the fraction of elements if the set is constituted by a group of objects. It may represent a probability of an outcome if the set consists of different realizations of similarly prepared systems. We will therefore call $p_i$ weights, to be clear that we make no commitment as to whether we have fractions, frequentist probabilities, Bayesian probabilities or other notions.

Once we have constructed our distribution, we will want to construct an indicator $H$ that quantifies how much variability the elements exhibit within the distribution. That is, we want to quantify the degree of diversity that the values can have within the distribution. To that end, we want to define some suitable requirements for $H$.

We may be tempted to use standard statistical quantities, like the range or the variance, but this is not possible. First, if the values associated with the bins are non-numerical (e.g. types of galaxies, words), such statistical quantities are not well defined. Second, relabeling the values (e.g. switching names, switching units, non-linearly changing coordinates) does not have an impact on variability of the elements, while statistical quantities will in general be affected. This tells us that our indicator cannot depend on $Q$ itself, but only on the weights $p_i$. That is, we require $H=H(p_i)$.\footnote{Note that for continuous quantities the weights are densities and are affected by the choice of $Q$: the unit is required to specify the numeric value (e.g.~$1 \% / $mm) and this will change under unit transformation (e.g.~$1000 \% /$m). Therefore, as the weights $p_i$ themselves depend on the unit, the indicator $H(p_i)$ will in general depend on the choice of $Q$. Though this is a source of additional confusion, for the purpose of defining a measure of variability it does not change things conceptually. We will discuss the problem in connection to entropies in thermodynamics and statistical mechanics, since coordinate invariance is of paramount importance in physics.}

We  expect small changes in a distribution -- small changes of the weights -- to produce small changes in variability; we therefore require $H$ to be a continuous function of the $p_i$. We also expect that as the number of values found within the distribution increases, so will the variability. Therefore if we have a uniform distribution over $I$ cases, so that $p_i = 1/I$, we require $H$ to be monotonically increasing with $I$.

As we noted before, the level at which we describe the elements is not absolute and can change. For example, we may choose to group the animals in the Gal\'{a}pagos islands first by class (e.g. mammals, birds, reptiles) and then later refine the mammals by species. In this case, we would like $H$ to combine linearly with respect to the probability. That is, we want the variability of the overall distribution $H_T = H_C + p_M H_M$ to be the variability over the classes $H_C$ plus the variability of the mammals $H_M$ weighted by the fraction of mammals $p_M$. This also makes the quantity additive when combining two independent distributions.

To sum up, we have the following three requirements:
\begin{enumerate}
\item $H$ depends only on $p_i$ and it does so continuously
\item If $p_i=1/I$ then $H$ is a monotonically increasing function of $I$
\item Let $p_i$ and $q_j$ be the weights for two distributions respectively over $I$ and $J$ bins. Let $r_k$ be the distribution over $K=I+J-1$ bins constructed by expanding the $a^{th}$ bin of the first distribution using the second distribution. More specifically, let $1 \leq a \leq I$, then $r_k = \{p_1, p_2, ..., p_{a-1}, p_{a}q_1, p_{a}q_2, ..., p_{a}q_J, p_{a+1}, ..., p_I \}$. Then $H(r_k) = H(p_i) + p_{a} H(q_j)$.
\end{enumerate}

We note that these are exactly the requirements Shannon put forth for his expression\cite{Shannon}, and that these requirements lead to only one possible option: $H(p_i) = - \sum p_i \log(p_i)$.

The Shannon entropy, which we will call Shannon variability, is a clear and important attribute of a given distribution: it quantifies the variability of the elements within the distribution. It quantifies the variety of values one finds within the distribution. It is true that the choices of the elements and binning are not fixed, it is true that the meaning of the weights depends on what the distribution is describing, and therefore that the precise meaning of this variability is context dependent. But this is true for any mathematical object: a real number may represent mass, color in the frequency spectrum, the total money supply, the half-life for an isotope, a probability and so on. And like a real number represents a quantity, transcending what is physically quantified, the Shannon variability represents the variability of the elements\footnote{We again stress that out elements are physical and therefore possess some property beyond the bare `definite and separate' of Cantor's definition, which was perfectly suited to study numbers. Without these extra attributes, which are `washed out' by said definition, we would not be able to create a distribution in the first place.} in a distribution, transcending what the distribution physically signifies.

The Shannon variability may represent uncertainty in some cases, if the weights are probabilities or credences, but not in the general case. If the weights portray the fraction of the elements that have a certain property, like the fraction of galaxies in the Laniakea Supercluster that are barred spirals, there is nothing uncertain about the distribution. In this case, the Shannon variability represents how much variation we have within galaxies in terms of galaxy types.

The Shannon variability may represent knowledge in some cases, but not in general. Consider the following two cases:
\begin{enumerate}
\item There is 50\% chance you won one million dollars in the lottery and 50\% chance you didn't.
\item There is 50\% chance you won one million dollars in the lottery and 50\% chance you won half a million dollars.
\end{enumerate}
The distribution in both cases is the same, two bins 50\% chance each, and so is the Shannon variability. Yet, you know more in the second case: you know you have won at least half a million dollars.

Unfortunately, entropy in general is often associated with vague characterizations like the the two presented. It is said to represent uncertainty, knowledge, lack of knowledge or disorder depending on the authors[CITATION NEEDED], which leads to confusion and misunderstanding.  The characterization of the Shannon variability measure we have given, on the other hand, applies to all cases and leads naturally to the assumptions required to rederive it. Our characterization therefore is more fundamental. If we look at the galaxies in our universe, what variability do they exhibit in terms of their types? If we look at the animals in the Gal\'{a}pagos island, what variability is expressed in terms of their species? If we look at the molecules in a given macrostate, what variability do they express in terms of their microstates? If we look at the words in the complete works of Shakespeare, what variability do we find in his vocabulary? All these questions are interesting, all these questions are insightful and the answers to all these questions are expressed through just one thing: the Shannon variability.

If the Shannon expression is a measure of variability, how is it connected to information? What is it measuring and in what units?

\section{Units of Variability\label{uv}}



We have seen that the Shannon variability measures the variability within a distribution but, in the end, it is a numeric quantity and we cannot expect to understand the concept if we do not understand what the number represents and in what units.

We can gain insight with the following example. Suppose we fix a distribution, say the animals in the Galap\'{a}gos islands binned by their respective species. Suppose we pick a specific animal from the set and you want to know its species. Suppose the only way for you to get that information is to ask a series of questions with only two possible answers, yes or no. How many questions would you have to ask? In other words, we are playing a game of Twenty Questions.

Not all questions will be able to extract the same amount of information. Some questions, like ``is it an animal?" would be redundant. Others, like ``is it an American Flamingo (Phoenicopterus ruber)?", would give us a lot of information in the positive case but little in the negative case. However, it is clear that there is always a minimum number of questions that must be asked to get to the answer. Given that there are more than two species, a single question cannot be enough. It should however be intuitively clear that to a greater variability within the distribution will correspond a greater number of questions you must ask. That is exactly what the Shannon variability quantifies: the minimum average number of questions one has to ask to identify a value in the distribution. It gives us the number of questions for an ideal strategy for our game of Twenty Questions.

If we have binary questions, the logarithms will be in base two and the unit for Shannon variability will be bits. It will indicate the average number of yes/no questions we need to identify an element within the dsitribution. In general, you can pick any base $b$: in base three we have ternary questions and trits, for ten we have questions with ten possible answers and digits. We can also pick a non-integer base, like the natural base for logarithms, and we will have nats. This is why the Shannon variability is fundamental in information theory, because it quantifies how much information is needed to transfer a value from a distribution.

Defining a set of questions means choosing an encoding as we are choosing how the information gets codified into our series of bits. To understand how this works, we can briefly review the Huffman coding,[CITE] which is the optimal algorithm for symbol-by-symbol coding with a known probability distribution. The idea is that we want all possible answers to each question to be balanced, to provide the same amount of information. The reason is that making one answer more specific (i.e.~it applies in fewer cases) means making another less specific (i.e.~it applies in more cases). In the case of binary questions, then, we ideally want the probability to answer yes or no to be 50\%.

For example, suppose the population of pets in a country is as follows:
\begin{itemize}
\item dogs 27\%
\item cats 48\%
\item fish 10\%
\item bird 8\%
\item small mammal 4\%
\item reptile 3\%
\end{itemize}
For the first question, we group cats on one side and everything else on the other, to form a 48/52 split. So we can ask ``is it a cat?". If the answer is yes, we finished. If not, we need to continue. We can group dogs on one side and everything else on the other to form a 27/25 split. So we can ask `` is it a dog?". If the answer is yes, we are done. If not, we continue. We can group fish with reptile and bird with small mammal to form a 13/14 split. So we can ask ``is it a fish or a reptile?" If the answer is yes, the followup question would be ``is it a fish?" If the answer is no, the followup question would be ``is it a bird?" With this scheme, the encoding, where 1 represents 'yes' and 0 represents 'no' to the each of the questions asked, is as follows:
\begin{itemize}
\item dogs 27\% - 2 questions (i.e.~2 bits) - answers: [no, yes] (i.e.~encoding 01)
\item cats 48\% - 1 question (i.e.~1 bit) - answers: [yes] (i.e.~encoding 1)
\item fish 10\% - 4 questions (i.e.~4 bits) - answers: [no, no, yes, yes] (i.e.~encoding 0011)
\item bird 8\% - 4 questions (i.e.~4 bits) - answers: [no, no, no, yes] (i.e.~encoding 0001)
\item small mammal 4\% - 4 questions (i.e.~4 bits) - answers: [no, no, no, no] (i.e.~encoding 0000)
\item reptile 3\% - 4 questions (i.e.~4 bits) - answers: [no, no, yes, no] (i.e.~encoding 0010)
\end{itemize}
The number of questions needed in each case corresponds to the number of bits. The answers in each case are represented by the encoding. Note how the encoding depends on the specific choice of questions. We can calculate the average number of bits for the encoding to be:  $(.48) * 1 + (.27) * 2 + (.1 + .08 + .04 + .03) * 4 = 2.02$ bits. This represents the average number of bits we would have to use for each animal if we repeated the game many times. We can also calculate the Shannon variability to be $.27 * \log_2(.27) + .48 * \log_2 (.48) + ... =1.98$ bits. This represents the ideal case, the minimum number of questions required to reach a definite answer. Note that our encoding is already very close to the ideal case.

Now that we understand that the Shannon variability is measured by the number of bits required to identify an element from a distribution, it is instructive to derive the same expression from different considerations. Suppose we have a sequence of $N$ elements, say pets like in the previous example. These are taken from $I$ different cases: dogs, cats, fish and so on. Suppose $N_i$ are the of elements of each type, which means $\sum_{i \in I} N_i = N$. Then, given a particular instance of $N_i$, we have $W(N_i) = \frac{N!}{\prod_{i \in I} N_i!}$ possible ways to realize that sequence, which corresponds to all possible permutations. If all permutations are equally likely, then $\log W (N_i)$ represents the number of bits needed to identify one of the sequences. Assuming we use base $b$ for the logarithm, we take $N$ to be large and, using Stirling's approximation $\ln N! \approx N \ln N - N$, we find:
\begin{equation}
\begin{aligned}
\log W(N_i) &= \log \frac{N!}{\prod_{i \in I} N_i!} \\
&= \log N! - \sum_{i \in I} \log N_i! \\
&\approx \frac{1}{\ln b} \left(N \ln N - N - \sum_{i \in I} N_i \ln N_i + \sum_{i \in I} N_i \right) \\
&= \frac{1}{\ln b} \left(\sum_{i \in I} N_i \ln N - \sum_{i \in I} N_i \ln N_i \right) \\
&= -\sum_{i \in I} N_i \left(\log N_i - \log N \right) \\
&= - N \sum_{i \in I} \frac{N_i}{N} \log \frac{N_i}{N} \\
&= - N \sum_{i \in I} p_i \log p_i \\
&= N H(p_i)
\end{aligned}
\end{equation}

The result should not be surprising: it simply tells us that encoding a sequence of $N$ elements is the same as encoding $N$ elements one at a time.

It is important, at this point, to understand that the technical use of the term information in information theory does not equate to the normal use of the term which refers to knowledge, intelligible data. The bits by themselves, the yeses and the nos, the ones and the zeros, do not provide knowledge. They need the context of the questions and the distribution to become actual information. For example, when opening a jpeg file, the file itself does not contain the instructions of how to read it. If you do not happen to know what a jpeg is and how to read it, you are not going to be able to interpret, to decode, the string of bits into actual intelligible data. The questions, the distribution, the context are considered given, communicated out-of-band through another scheme. As with any semantic content, this cannot be easily formalized and quantified.

The Shannon variability of the distribution, then, has nothing to do with the information the distribution itself holds. The distribution is not what is being encoded. The variability is the information needed to go from the distribution, which is given, to an individual element. It is really the information gap from the population to an element. That is why some people say the entropy is ``lack of information", which is justified because, in a way, it is the information about the elements that the distribution cannot provide. But, again, this is just missing the point of what is being described. If one is not interested in identifying elements there is no ``missing information".

In communication and information theory, information is really encoded information. Communication systems and information processors have no idea whether the source of the data is a digital thermometer or a poet. There is no knowledge per se, just symbol manipulation that may represent different concepts in different contexts. The term information entropy, then, is misleading for two reasons. First, because it is really not entropy in the thermodynamic sense: it is not defined on states, it does not know about irreversible processes and it is unrelated to maximization at equilibrium. In fact, it is not related to physical systems. Second, it is not really information in the general sense, only in the very narrow technical sense of encoded information within a communication or information system.

This should suggest that the success of Shannon variability and its use in both information theory and physics does not warrant a fundamental change of perspective in what constitutes a physical object. It is true that any physical process can be used to process information, when properly encoded. It is also true that scientific theories, in the end, are models that can capture only the aspects of nature that can be tested experimentally, the information extracted by the experiment, under suitable circumstances. Therefore the claim that information plays an important role in physical theories has a valid basis.[cite Wheeler, Landauer] But it is also true that that data requires the context to be able to be understood: we need to know what is the subject of our experiment, how to prepare it and how to collect the data. The art of experimental science is neither contained in the mathematical description nor in the data collected. As information, in the information theoretic sense, requires that context to become intelligible, it cannot play a primary role. Therefore the claim that the universe itself is information does not follow.[cite Vedral, Davies]

%Cite Vlatko Vedral: Decoding Reality: The Universe as Quantum Information. Information is "the fundamental building block of the Universe". 

%Cite Paul Davies Information and the Nature of Reality. Information "occupies the ontological basement"

%Cite Wheeler It from bit. "It is wrong, moreover, to regard this or that  physical quantity as sitting "out there" with this or that numerical value it is absorbed." "The information thus solicited make physics and comes in bits"

We have seen what the Shannon variability measures and why it is important in information theory, but it all assumed that the quantities we use for binning are discrete. In physics we are interested in quantities, like position and momentum, that are continuous. How does it work in that case? Wouldn't we need infinitely many bits to identify a continuous value from a continuous distribution?

\section{Continuous distributions}

A standard quick and dirty way to extend distributions from discrete variables to continuous variables is to substitute weights with densities and the sums with integrals. Therefore, instead of having a discrete normalize distribution $\sum_i p_i = 1$, we have a continuous normalized distribution $\int \rho(x)dx=1$. While simply changing $- \sum_i p_i \log p_i$ to $- \int \rho(x) \log \rho(x) dx$ works in most cases, it is easy to get confused because the two expressions have significantly different properties.

In the discrete case, the Shannon variability is always positive. In the continuous case, the Shannon variability can be negative. For example, consider a uniform distribution over a line:
\begin{equation}
\rho(x)=
\begin{cases}
0 & x < a\\
\dfrac{1}{b-a} & a \leq x \leq b\\
0 & b < x
\end{cases}
\end{equation}
We have $H(\rho) = - \int_a^b \frac{1}{b-a} \log \frac{1}{b-a} dx = \log (b-a)$. If $b-a < 1$ the Shannon variability will be negative. What does it mean to have negative number of bits?

When considering continuous distributions, we have to remember that the densities are given per unit. That is, $\rho(x)$ has dimensions $[\frac{1}{x}]$. The variability will be also given relative to the unit. For example, a uniform distribution over a unit interval will correspond to zero variability. A uniform distribution over a two unit interval will correspond to one bit, since we will need one bit to narrow the variability back to one unit. A distribution over half a unit interval will correspond to minus one bit, since we would need to ``lose'' one bit of information to widen the variability back to one unit. In other words, the variability is measured by the number of bits needed to identify an element up to one unit.

This means that infinitesimally narrow distributions, like delta functions, do not work well with the Shannon variability as they would give minus infinite entropy. If one wants entropy over continuous variables to be bounded, then one must work with continuous functions which have a very small but still finite support (i.e. region where the function is non-zero). We will assume all our continuous distributions are like that.

This brings up a second problem: what happens if we changed unit? If we changed the reference, we expect that the entropy will change accordingly. In fact:
\begin{equation}
\begin{aligned}
H(\rho(y)) &= - \int \rho(y) \log \rho(y) dy \\
&= - \int \rho(x) \left|\frac{\partial y}{\partial x}\right| \log \left(\rho(x)\left|\frac{\partial y}{\partial x}\right| \right) \left|\frac{\partial x}{\partial y}\right| dx \\
&= - \int \rho(x)  \log \left(\rho(x)\left|\frac{\partial y}{\partial x}\right| \right)  dx \\
&= - \int \rho(x)  \log \rho(x)  dx - \int \rho(x)  \log \left|\frac{\partial y}{\partial x}\right|  dx \\
&= H(\rho(y)) - \int \rho(x)  \log \left|\frac{\partial y}{\partial x}\right|  dx \\
\end{aligned}
\end{equation}
In general, the Shannon variability over a continuous variable is not invariant under coordinate transformations. Under translations, the Jacobian $\left|\frac{\partial y}{\partial x}\right|$ is unitary, so the Shannon variability does not change. If we stretch or shrink, if we change scale, the Shannon variability changes to measure the variability at the new scale. If the change is non-linear, the variability over different ranges will be counted differently.

This should be particularly perplexing if we want to use this quantity in a physical setting. If we want $\rho(x)$ to represent the state of a macroscopic system and $H(\rho(x))$ to represent entropy, a state variable, how can $H$ change value if I merely expressed $\rho$ over different coordinates? How can maximization of entropy be meaningful if it yields different results depending on the coordinate system? Does that mean that we can use the Shannon formula only on distributions over discrete values?

In physics, there is one thing that makes the Shannon variability work: phase-space. It is the only type of manifold where entropy can be defined in a way that is coordinate invariant. What happens in phase space is that, under coordinate transformation $\hat{q}^i = \hat{q}^i(q^j)$, the $d\hat{q}^i = \frac{\partial \hat{q}^i}{\partial q^j } dq^j$ vary like vector components while $d\hat{p}_i = \frac{\partial q^j}{\partial \hat{q}^i } dp_j$ vary like covector components. Therefore the areas $d\hat{q}^i d\hat{p}_i = dq^j dp_j$ remain the same. The Jacobian determiants are unitary and the Shannon variability is invariant.

The use of phase-space in statistical mechanics is of paramount importance: over this space both the density $\rho(q^i, p
_i)$ and its variability $H(\rho(q^i, p
_i))$ are invariant under arbitrary transformations of $q^i$, including non-linear ones. This is not just a coincidence or convenience: it is essential if we want to give the density at each point of phase space a physically objective character.

The importance of phase space is often attributed to the Liouville's theorem, that states that areas are conserved under Hamiltonian evolution. While this is true, their invariance under coordinate transformations is more important as it is what makes them physically meaningful in the first place. In fact, how could we have conservation over time if we could not compare it in an observer independent way? This means that we always have to consider distributions over the full phase-space with the appropriate conjugate coordinates if we want to use $-\rho \log \rho$. If we use distributions over position and velocities or over momentum only, the expression would change for those particular variables in those coordinate systems. This can be, again, a source of confusion.\cite{Dunkel}


We conclude noting that in quantum mechanics the Shannon variability, in the form of the Von Neumann entropy, is coordinate invariant as well. That is, given a density matrix $\rho$ then $S_{VN} = \textrm{tr}(\rho \log \rho)$ is independent on the basis in which it is calculated.

Now that we have seen how the Shannon variability works in the continuous case and the importance of phase space, we are ready to see how it can be used in statistical mechanics and how it relates to the Boltzman, Gibbs and Von Neumann entropies.


\section{Connection to Statistical Mechanics\label{csm}}

As we saw in the previous sections, Shannon variability is a very general concept that applies to any type of distribution, no matter what the problem domain. It does not require us to define states, processes, reversibility and therefore it is conceptually different and independent from the entropy in thermodynamic and statistical mechanics which requires them. Yet it can, and is, applied successfully in statistical mechanics even in systems far from equilibrium.[cite] How does this work?

As statistical mechanics aims to describe the collective behavior of a large number of physical systems (i.e. particles), the Shannon variability can be applied to their distribution and will correspond to the entropy of the system. Yet, we have to understand that there are two distinct ways to apply the concept simply because there are two types of distributions one can be interested in. We may consider the state of the whole system at a given time, and be interested in the distribution of the particles over all possible particle states. This will lead to the Boltzman entropy. We may consider a statistical ensemble, which is a large collection of independent copies of the system found in different states, and consider the distribution of these different instances over the states of the whole system. This will lead to the Gibbs or the Von Neumann entropy depending on whether the system is classical or quantum. Both these types of distributions are used and are therefore of interest, so we will look at both and clarify the potential sources of confusion.

Suppose we have a large number $N$ of particles taken from a normalized distribution $\rho(q^i, p_i)$. The space is the six dimensional phase-space for a single particle, sometimes called $\mu$-space. As we assume $N$ large, we can think of $N\int_\Sigma \rho(q^i, p_i)dq^idp_i$ as the number of particles that are have within a region $\Sigma$ of phase space. That is, $\rho$ does not represent a probability distribution but an actual physical distribution that tells us the state of the whole system at one instant of time. The Boltzmann entropy is given by $S_B = k_B \log W$, where $W$ will correspond to the different ways that the $N$ particles can be arranged while still satisfying the distribution. If the space were discrete, the computation of the permutations would be straightforward. But how does this work in a continuous space?

As we said before, we treat continuous variables by comparing to a finite unit. We can pick a unit of phase space small enough such that the density $\rho$ is constant over cells of that size. We express $\rho$ as the number of particles within the chosen unit and divide phase space into cells. Now we can calculate all the possible permutations of the particles within the different cells and, in these circumstances, we will find that $\log W = N H(\rho)$. That is, the number of permutations at that level of precision will be equivalent to the number of particles times the variability of the distribution at that level of precision. In other words, the Boltzmann entropy reduces to the Shannon entropy.

The Boltzmann constant $k_B$ should not distract us: its role is simply to allow us to measure temperature in an appropriate unit. If one defines $T = \frac{\partial U}{\partial S}$, and measures temperature in Kelvin and energy in Joules, the above relationship forces us to measure entropy in Joules/Kelvin. Therefore the entropy cannot be expressed as a pure number. However, this is not the only possible definition. Instead of using $T$ as a primary thermodynamic variable, we can use $\beta = \frac{1}{k_B T}$.\footnote{There are a few reasons why one can argue this is a better choice, though ultimately is still a matter of choice.} In this case, one would define $\beta = \frac{\partial S}{\partial U}$, measure energy in Joules, $\beta$ in inverse Joules and entropy would be dimensionless. As one can do all the thermodynamic calculations using just $\beta$, the constant $k_B$ is really just set by the unit system.

This setting, distributions over single particle phase space, is the one used to derive the Maxwell-Boltzmann distribution $\rho(q^i, p_i) = \left(\frac{\beta m}{2\pi} \right)^{3/2}e^{-\beta \frac{p_ip^i}{2m}}$, which is the correct distribution of particles for an ideal gas. It provides a clear picture for what happens in simple cases: the equilibrium distribution is the one where the variability of the states of the particles is maximal. That is, particles will spread out as much as they can under the constraints given by the energy, volume and number of particles. The information needed to identify a particle within the distribution at a given set of precision will increase during an irreversible process. The equilibrium is a statistical equilibrium, particles are moving around, but for one particle that moves in one direction, there is one that moves in another and the overall distribution remains the same. TODO This corresponds to the insight of Boltzmann... 

This approach, though, will only work in the limit of a large number of indistinguishable particles that are independently distributed. If $N$ is not large, we are not guaranteed that another particle will move in the opposite direction. If there are correlations, the motions will not cancel out. More precisely, suppose we have a probability distribution $\hat{\rho}$ for $N$ particles. If they are independently distributed, then the joint probability $\hat{\rho}=\prod_{i}\rho_i$ is the product of the distribution for each particle. If they are indistinguishable, then $\rho_i=\rho$: each particle has the same distribution. If $N$ is large, then number of particles in one region $N_\Sigma$ is very close to the expectation value $N\int_\Sigma \rho(q^i, p_i)dq^idp_i$. If these assumptions are not met, the fluctuations will become relevant and we need a statistical theory to take them into account.\footnote{In [Jaynes] it is shown that a single distribution over $\mu$-space will not recover the correct experimental values for entropy.} Moreover, a change of the number of particles or particles of different species cannot be captured by a single normalized distribution over single particle states.

The more general setting, then, is the following. The macrostate is a probability distribution over all possible complete description of the system, the microstates. That is, we have a distribution over the $6N$ dimensional phase space of $N$ particle, sometimes referred to as $\Gamma$-space, where each point represents the position and momentum of $N$ particles. The Gibbs entropy is $S_G = -k_B \int \rho \log \rho (dq^idp_i)$ which corresponds to the Shannon variability of the distribution. The Gibbs entropy, then, is the variability of a microstate as it moves around within the macrostate. The macrostate of an equilibrium will fully be identified by a set of macroscopic variables, such as temperature, average energy, pression, and so on. Note that these may be quantities that are not defined on an individual microstate but only on the ensemble. The microscopic dynamics will be free to move around as long as those statistical quantities are preserved. The Gibbs entropy, then, tells us the variability of the microstate under the given constraints and, at equilibrium, we will find that variability to be maximal.

There are a couple of problems in this picture that we have to be aware of. The first is that $\Gamma$-space automatically assumes that all particles are distinguishable. This leads to the widely known problem of overcounting which needs to be addressed in the standard way. The second problem is that, though the state of each particle is given by position and momentum, we should not think of them as literally pointlike. As we say, this would correspond to delta Dirac distributions over phase-space which have minus infinite Shannon entropy. It is more appropriate, both mathematically and conceptually, to think of indistinguishable particles as identical peaked distributions, each contributing the same amount of Shannon entropy.\footnote{Setting to $-\log h$ the entropy corresponding to each degree of freedom of these peaked distributions is a natural way to incorporate the effects of the uncertainty principle.} The position and momentum corresponds to the center of mass of the particle.

We note that some authors choose to interpret the probability distribution not as representing the frequency over a large amount of independent trials, but as the knowledge one has about the system. This would make the entropy an observer dependent quantity, instead of an objective notion.\footnote{Jaynes called it the anthropomorphic nature of entropy.} The issue is that, experimentally, the fluctuations we observe are not observer dependent. We believe the confusion comes from the, correct, realization that the system by itself is not enough to determine the thermodynamic variables that are needed to reach the equilibrium. In fact, Jaynes points out:
\begin{quote}
Consider, for example, a crystal of Rochelle salt. For one set of experiments on it, we work with temperature, pressure, and volume. The entropy can be expressed as some function $S_e(T,P)$. For another set of experiments on the same crystal, we work with temperature, the component $e_xy$ of the strain tensor, and the component $P_z$ of electric polarization; the entropy as found in these experiments is a function $S_e(T,e_xy,P_z)$. It is clearly  meaningless to ask, ``What is the entropy of the crystal?''  unless we first specify the set of parameters which define its thermodynamic state.
\end{quote}
This is a particular case of what we stated in the previous sections: a distribution, and therefore its variability, is contingent upon some arbitrary choices. But, again, once those choices are made, the distribution is objective. The choice of the system, in statistical mechanics and thermodynamics, is enough to determine the state space for the microstates, but not enough to determine the set of macrostates that correspond to equilibria. We have to specify the process and the constraints that that process puts on the system. That is, under a different choice of process and constraints the microstates will fluctuate in different ways since we have changed the dynamics of the system. The question, then, is whether the system plus the process plus the set of constraints are enough to determine the probability, or we also need to mix in human knowledge. What we find is that they are sufficient, so the probability distribution does not depend on human knowledge.

The case where the microstate is a quantum state is formally similar to the classical case. Instead of a distribution over the $N$ particle phase space, we have a distribution over the Hilbert space for the quantum system which is represented by a density matrix operator $\rho$. The Von Neumann entropy is given by $S_{VN} = \textrm{tr}(\rho \log \rho)$. This, expanded in a basis, becomes $S_{VN} = - \sum_i \rho_i \log \rho_i$ or $S_{VN} = - \int \rho(x) \log \rho(x)dx$ depending on whether the spectrum is discrete or continuous.

The different entropies in statistical mechanics, then, all have a tight link to the Shannon variability. The Boltzmann entropy corresponds to the variability of a particle within a given microstate, provided that there are a large number of independent and indistinguishable particles. The Gibbs entropy corresponds to the variability of a classical microstate as constrained by the macrostate. The Von Neumann entropy corresponds to the quantum case. The characterization we gave to the Shannon formula, then, is readily applicable to statistical mechanics in a natural way. We find it more precise than the common characterizations, such as disorder or lack of knowledge, and, once internalized, leads to less confusion.


\section{Conclusion}

\bibliographystyle{unsrt}  
\bibliography{bibliography}



\end{document}
