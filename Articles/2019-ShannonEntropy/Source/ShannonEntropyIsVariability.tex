\documentclass{article}
\textheight 8.9 in \textwidth 6.5 in \oddsidemargin -10pt \topmargin
-30pt

%\usepackage{arxiv}
%\usepackage[usenames,dvipsnames]{color}



\title{Universal Characterization of Physical Variability by \\Shannon's $-\sum_ip_i\,\textrm{ln}\,p_i$ Formula}

\author{
	Julian Barbour and 
	Gabriele Carcassi
}


\begin{document}

\maketitle

\tableofcontents

\begin{abstract}
TBD
\end{abstract}




\section{Introduction\label{int}}

The use of Shannon entropy $-\sum p_i \log p_i$ is well established in very different fields of knowledge such as computer science[cite], physics[cite] and ecology[cite]. It is a fundamental element in information theory since Shannon developed his mathematical theory of communication. It has been used extensively within statistical mechanics to link the microscopic description of a system to its macroscopic thermodynamic one. It has been used in ecology to characterize biodiversity and calculate species distributions under given constraints. Entropy maximization has been shown to be a powerful tool in all these settings.

While the techniques are well developed, it is puzzling that this concept can be applied in such different contexts, given that it assumes different features in each one. For example, in information theory the Shannon entropy for a book will change whether we choose to encode words or letters, which means the quantity does not have a fully objective feature: it depends on an arbitrary choice. This is not problematic for an information theoretic quantity, but it is for a physical quantity. In statistical mechanics the $p_i$ represent probabilities, which means the Shannon entropy is a property of an ensemble of physical systems. This leads some to claim that the Shannon entropy is epistemic in nature and represents ignorance. [Add references] In ecology, however, the $p_i$ represent the fraction of individuals belonging to the $i$th species. There is no probabilistic ensemble to which one can associate a degree of ignorance. Finally, thermodynamic entropy is associated to the notion of states and irreversible time evolution, which are not well defined concepts for the other fields. The same expression, then, seems to have acquire different flavors and it is not clear why a single concept would apply in all these cases.

To make matter worse, it is fairly typical to describe entropy with notions such as uncertainty[cite], knowledge[cite], lack of knowledge[cite], disorder[cite]. While some of these characterizations are useful in some contexts to give an intuitive picture, they do not seem to always work. A common objection to the characterization as disorder, for example, is that a uniform distribution corresponds to a homogeneous system, which one would expect to be maximally ordered; yet the distribution maximizes the Shannon entropy, which would indicate maximal disorder. Moreover, these notions remain vague in the sense that the formula is not formally derived by those concepts and therefore it is not clear why that particular indicator would be an appropriate choice. We are left to wonder: is there really some single underlying concept, that, when applied appropriately in the different fields, reduces to the correct notion of entropy? Or is it just a big coincidence?

We propose that there is indeed a characterization of Shannon's formula that is both more general and more appropriate; that is independent on whether it is applied to sources of messages, ensembles of microstates or animal populations. We will argue that the formula is a measure of the \emph{variability of the values within a distribution of a given set of elements}. That is, given a distribution of some elements over some quantity, the Shannon entropy represents how varied, how different from each other, the values are. In fact, it is the only continuous and linear index that measures the degree of variability within a distribution, and those requirements lead naturally to same three conditions that Shannon set for his formula. In other words, the concept of variability with a distribution leads in a formal way to the correct expression. We will then show how this perspective naturally connects to information theory and statistical mechanics when applied in the respective contexts. 

As we believe this is the right characterization, we will therefore use the expression Shannon ``variability'', rather than ``entropy'' or ``information'', as it tells us exactly what the quantity describes. It is well known that when Shannon recognized the importance of his formula in communication theory he sought von Neumann's advice on the name he should give to the expression he had found. He was told to call it \emph{entropy}, first, because Boltzmann had used an identical formula to define entropy in statistical mechanics and, second, nobody understood entropy and that would provide protection from criticism [exact words to be found]. We believe von Neumann's advice was not totally helpful and has given rise to confusion. We will reserve the word entropy for the thermodynamic entropy, the one dealing with physical states and irreversible processes.

This characterization also explains why the Shannon variability has such a broad range of applicability. As nature presents itself in a multitude of forms and combinations, a fundamental task in science is to recognize which objects belong together and study how their different attributes are expressed in different ways or degrees. They may galaxies, animals or outcomes of the same process; we may study their shape, behavior or numerical quantities. But the idea of grouping them and try to understand the source of their variation is a common.\footnote{In fact, determining which collection of objects is significant and what attributes are relevant is significant part of the work.} Measuring and explaining this variability is an essential task. It is this very variety that is the inspiration for and the subject of scientific investigation. A universe without variety, where all objects were the same and remained the same, is barely conceivable and certainly it would leave no space for scientific investigation.

As a final note, we want to stress that the goal of this work is not to present some new technical result. The goal is to put all details together in a coherent picture that gives them context and see precisely how they are related. This can help, especially the new practitioners, to avoid common misconceptions. Moreover, while some details presented here may be well known in a particular community, we find that it may be new to another. For example, we will show an example of Huffmann coding to illustrate the meaning of the numerical value associated with the Shannon variability. This is standard practice in computer science courses, yet it is not in statistical mechanics. Therefore restating all the crucial aspects in one place is beneficial because the connection between them is not always readily available.


%https://www.researchgate.net/publication/259011656_A_tribute_to_Claude_Shannon_1916-2001_and_a_plea_for_more_rigorous_use_of_species_richness_species_diversity_and_the_'Shannon-Wiener'_Index

%https://web.stanford.edu/~hastie/Papers/maxent_explained.pdf

%https://www.mdpi.com/1099-4300/21/8/794/htm

%Leibniz
%We recall, though we have been unable to relocate, a sentence in one of Leibniz's many writings to the effect that reality \emph{is} variety. Whether he made the statement or not, it is consistent with his view that any contingent thing is defined by a collection of attributes, which necessarily must be distinguishable at some epistemic level. Paragraph 58 of Leibniz's \emph{Mondadology} asserts that the principle underlying the universe is that it should realize ``as much variety as possible, but with the greatest order possible''. [REF]

%We conclude this section with a word on terminology. In mathematics, Cantor defined a set as ``a collection into a whole $M$ of definite and separate objects''. He called the objects \emph{elements}. We will also use the word `elements' to denote the things we consider but subject to the proviso that they have a distinguishing feature (or features) that allows us to say that \emph{they are of the same kind}. Thus, we consider sets of a restricted type. The point is this. To constitute a set in accordance with Cantor's definition, it is only necessary for its elements to be \emph{definite and separate} objects. This leaves total freedom as to how such objects in the universe are to be parcelled up into specific sets. For example, a set might consist of Mars and the word `go'. We think such latitude misses one of the most significant aspects of the universe, namely, that the things within it can be distinguished by attributes that are much more specific than the two required by Cantor. Our field of investigation goes beyond mere number, which washes out all attributes apart from the two, of being definite and separate, that allow us to count. They were the only attributes that Cantor needed; we need more.


\section{Variability within a Distribution\label{vwd}}

The general setting is the following. We have a set of elements $E = \{e_\alpha\}_{\alpha=1}^N$. In accordance with the comments made in our introduction, they can for example represent the galaxies in the Laniakea Supercluster, the animals in the Gal\'{a}pagos islands, the molecules in a box of gas given a particular macrostate, the different outcomes of spin measurement for a given quantum state, or the words in the complete works of Shakespeare. The choice of the particular set is driven by the interests and practices of scientists and their fields of study and therefore it is, in this sense, discretionary. Yet, it must be objective in at least one respect: once a choice is made the actual elements are the same for everybody. Once we arbitrarily decided we will study the galaxies in the Laniakea Supercluster, it is a matter of fact that the Milky Way will be included. This also means that, whatever properties those elements will have, they will represent a matter of fact about that set. In turn, this will make some choices arguably more appropriate or interesting than others. For example, in taxonomy the set of all animals that have feathers may be more appropriate than the set of all animals that can fly: the first includes only birds while the second includes most birds, a few mammals and a lot of insects.

Once a set of elements is chosen, we select a property or a set of properties we want to use to characterize the elements. That is, we have a set of possible values $Q=\{q_i\}_{i=1}^I$ and a map $q : E \to Q$ that associates a value to each element. These can represent the galaxy types, the genus of the animal, the possible states of the molecules, the possible values for spin or the spelling of words. This will give us a sequence $\{q(e_\alpha)\}_{\alpha=1}^N$ of the descriptions that are associated with each element in the set. Again, the choice of properties is ultimately discretionary, grounded on what particular aspects of the elements we are studying in a given case. Yet, like before, once a choice is made the value for each element is objectively defined. Once we arbitrarily decided we are studying galaxy types it is a matter of fact that the Milky Way is a barred spiral galaxy. Again, one may find that some choices are arguably more appropriate or interesting than others.

Having decided what elements to study and the level of description we are interested in, we bin them; that is, we group them based on that description, disregarding the identity of the particular element. What we will be interested in is only the relative frequency $p_i = N_i / N = \left| \{e_\alpha \in E | q(e_\alpha) = q_i \} \right| / N$ of the elements within each bin. We may do so because we are either not interested or not able to further distinguish the elements. Whatever the reason, once the choices of elements and properties are made, this relative frequency is objective. The nature of the $p_i$ will depend on the previous choices. It may represent the fraction of elements if the set is constituted by a group of objects. It may represent a probability of an outcome if the set consists of different realizations of similarly prepared systems. We will therefore call $p_i$ weights, to be clear that we make no commitment as to whether we have fractions, frequentist probabilities, Bayesian probabilities or other notions.

Once we have constructed our distribution, we will want to construct an indicator $H$ that quantifies how much variability the elements exhibit within the distribution. That is, we want to quantify the degree of diversity that the values can have within the distribution. To that end, we want to define some suitable requirements for $H$.

We may be tempted to use standard statistical quantities, like the range or the variance, but this is not possible. First, if the values associated with the bins are non-numerical (e.g. types of galaxies, words), such statistical quantities are not well defined. Second, relabeling the values (e.g. switching names, switching units, non-linearly changing coordinates) does not have an impact on variability of the elements, while statistical quantities will in general be affected. This tells us that our indicator cannot depend on $Q$ itself, but only on the weights $p_i$. That is, we require $H=H(p_i)$.\footnote{Note that for continuous quantities the weights are densities and are affected by the choice of $Q$: the unit is required to specify the numeric value (e.g.~$1 \% / $mm) and this will change under unit transformation (e.g.~$1000 \% /$m). Therefore, as the weights $p_i$ themselves depend on the unit, the indicator $H(p_i)$ will in general depend on the choice of $Q$. Though this is a source of additional confusion, for the purpose of defining a measure of variability it does not change things conceptually. We will discuss the problem in connection to entropies in thermodynamics and statistical mechanics, since coordinate invariance is of paramount importance in physics.}

We  expect small changes in a distribution -- small changes of the weights -- to produce small changes in variability; we therefore require $H$ to be a continuous function of the $p_i$. We also expect that as the number of values found within the distribution increases, so will the variability. Therefore if we have a uniform distribution over $I$ cases, so that $p_i = 1/I$, we require $H$ to be monotonically increasing with $I$.

As we noted before, the level at which we describe the elements is not absolute and can change. For example, we may choose to group the animals in the Gal\'{a}pagos islands first by class (e.g. mammals, birds, reptiles) and then later refine the mammals by species. In this case, we would like $H$ to combine linearly with respect to the probability. That is, we want the variability of the overall distribution $H_T = H_C + p_M H_M$ will be the variability over the classes $H_C$ plus the variability of the mammals $H_M$ weighted by the fraction of mammals $p_M$. This also makes the quantity linear when combining two independent distributions.

To sum up, we have the following three requirements:
\begin{enumerate}
\item $H$ depends only on $p_i$ and it does so continuously
\item If $p_i=1/I$ then $H$ is a monotonically increasing function of $I$
\item Let $p_i$ and $q_j$ be the weights for two distributions respectively over $I$ and $J$ bins. Let $r_k$ be the distribution over $K=I+J-1$ bins constructed by expanding the $a^{th}$ bin of the first distribution using the second distribution. More specifically, let $1 \leq a \leq I$, then $r_k = \{p_1, p_2, ..., p_{a-1}, p_{a}q_1, p_{a}q_2, ..., p_{a}q_J, p_{a+1}, ..., p_I \}$. Then $H(r_k) = H(p_i) + p_{a} H(q_j)$.
\end{enumerate}

We note that these are exactly the requirements Shannon put forth for his expression\cite{Shannon}, and that these requirements lead to only one possible option: $H(p_i) = - \sum p_i \log(p_i)$.

The Shannon entropy, which we will call Shannon variability, is a clear and important attribute of a given distribution: it quantifies the variability of the elements within the distribution. It quantifies the variety of values one finds within the distribution. It is true that the choices of the elements and binning are not fixed, it is true that the meaning of the weights depends on what the distribution is describing, and therefore that the precise meaning of this variability is context dependent. But this is true for any mathematical object: a real number may represent mass, color in the frequency spectrum, the total money supply, the half-life for an isotope, a probability and so on. And like a real number represents a quantity, transcending what is physically quantified, the Shannon variability represents the variability of the elements\footnote{We again stress that out elements are physical and therefore possess some property beyond the bare `definite and separate' of Cantor's definition, which was perfectly suited to study numbers. Without these extra attributes, which are `washed out' by said definition, we would not be able to create a distribution in the first place.} in a distribution, transcending what the distribution physically signifies.

The Shannon variability may represent uncertainty in some cases, if the weights are probabilities or credences, but not in the general case. If the weights portray the fraction of the elements that have a certain property, like the fraction of galaxies in the Laniakea Supercluster that are barred spirals, there is nothing uncertain about the distribution. In this case, the Shannon variability represents how much variation we have within galaxies in terms of galaxy types.

The Shannon variability may represent knowledge in some cases, but not in general. Consider the following two cases:
\begin{enumerate}
\item There is 50\% chance you won one million dollars in the lottery and 50\% chance you didn't.
\item There is 50\% chance you won one million dollars in the lottery and 50\% chance you won half a million dollars.
\end{enumerate}
The distribution in both cases is the same, two bins 50\% chance each, and so is the Shannon variability. Yet, you know more in the second case: you know you have won at least half a million dollars.

Unfortunately, entropy in general is often associated with vague characterizations like the the two presented. It is said to represent uncertainty, knowledge, lack of knowledge or disorder depending on the authors[CITATION NEEDED], which leads to confusion and misunderstanding.  The characterization of the Shannon variability measure we have given, on the other hand, applies to all cases and leads naturally to the assumptions required to rederive it. Our characterization therefore is more fundamental. If we look at the galaxies in our universe, what variability do they exhibit in terms of their types? If we look at the animals in the Gal\'{a}pagos island, what variability is expressed in terms of their species? If we look at the molecules in a given macrostate, what variability do they express in terms of their microstates? If we look at the words in the complete works of Shakespeare, what variability do we find in his vocabulary? All these questions are interesting, all these questions are insightful and the answers to all these questions are expressed through just one thing: the Shannon variability.

If the Shannon expression is a measure of variability, how is it connected to information? What is it measuring and in what units?

\section{Units of Variability\label{uv}}



We have seen that the Shannon variability measures the variability within a distribution but, in the end, it is a numeric quantity and we cannot expect to understand the concept if we do not understand what the number represents and in what units.

We can gain insight with the following example. Suppose we fix a distribution, say the animals in the Galap\'{a}gos islands binned by their respective species. Suppose we pick a specific animal from the set and you want to know its species. Suppose the only way for you to get that information is to ask a series of questions with only two possible answers, yes or no. How many questions would you have to ask? In other words, we are playing a game of Twenty Questions.

Not all questions will be able to extract the same amount of information. Some questions, like ``is it an animal?" would be redundant. Others, like ``is it an American Flamingo (Phoenicopterus ruber)?", would give us a lot of information in the positive case but little in the negative case. However, it is clear that there is always a minimum number of questions that must be asked to get to the answer. Given that there are more than two species, a single question cannot be enough. It should however be intuitively clear that to a greater variability within the distribution will correspond a greater number of questions you must ask. That is exactly what the Shannon variability quantifies: the minimum average number of questions one has to ask to identify a value in the distribution. It gives us the number of questions for an ideal strategy for our game of Twenty Questions.

If we have binary questions, the logarithms will be in base two and the unit for Shannon variability will be bits. It will indicate the average number of yes/no questions we need to identify an element within the dsitribution. In general, you can pick any base $b$: in base three we have ternary questions and trits, for ten we have questions with ten possible answers and digits. We can also pick a non-integer base, like the natural base for logarithms, and we will have nats. This is why the Shannon variability is fundamental in information theory, because it quantifies how much information is needed to transfer a value from a distribution.

Defining a set of questions means choosing an encoding as we are choosing how the information gets codified into our series of bits. To understand how this works, we can briefly review the Huffman coding,[CITE] which is the optimal algorithm for symbol-by-symbol coding with a known probability distribution. The idea is that we want all possible answers to each question to be balanced, to provide the same amount of information. The reason is that making one answer more specific (i.e.~it applies in fewer cases) means making another less specific (i.e.~it applies in more cases). In the case of binary questions, then, we ideally want the probability to answer yes or no to be 50\%.

For example, suppose the population of pets in a country is as follows:
\begin{itemize}
\item dogs 27\%
\item cats 48\%
\item fish 10\%
\item bird 8\%
\item small mammal 4\%
\item reptile 3\%
\end{itemize}
For the first question, we group cats on one side and everything else on the other, to form a 48/52 split. So we can ask ``is it a cat?". If the answer is yes, we finished. If not, we need to continue. We can group dogs on one side and everything else on the other to form a 27/25 split. So we can ask `` is it a dog?". If the answer is yes, we are done. If not, we continue. We can group fish with reptile and bird with small mammal to form a 13/14 split. So we can ask ``is it a fish or a reptile?" If the answer is yes, the followup question would be ``is it a fish?" If the answer is no, the followup question would be ``is it a bird?" With this scheme, the encoding, where 1 represents 'yes' and 0 represents 'no' to the each of the questions asked, is as follows:
\begin{itemize}
\item dogs 27\% - 2 questions (i.e.~2 bits) - answers: [no, yes] (i.e.~encoding 01)
\item cats 48\% - 1 question (i.e.~1 bit) - answers: [yes] (i.e.~encoding 1)
\item fish 10\% - 4 questions (i.e.~4 bits) - answers: [no, no, yes, yes] (i.e.~encoding 0011)
\item bird 8\% - 4 questions (i.e.~4 bits) - answers: [no, no, no, yes] (i.e.~encoding 0001)
\item small mammal 4\% - 4 questions (i.e.~4 bits) - answers: [no, no, no, no] (i.e.~encoding 0000)
\item reptile 3\% - 4 questions (i.e.~4 bits) - answers: [no, no, yes, no] (i.e.~encoding 0010)
\end{itemize}
The number of questions needed in each case corresponds to the number of bits. The answers in each case are represented by the encoding. Note how the encoding depends on the specific choice of questions. We can calculate the average number of bits for the encoding to be:  $(.48) * 1 + (.27) * 2 + (.1 + .08 + .04 + .03) * 4 = 2.02$ bits. This represents the average number of bits we would have to use for each animal if we repeated the game many times. We can also calculate the Shannon variability to be $.27 * \log_2(.27) + .48 * \log_2 (.48) + ... =1.98$ bits. This represents the ideal case, the minimum number of questions required to reach a definite answer. Note that our encoding is already very close to the ideal case.

It is important, at this point, to understand that the technical use of the term information in information theory does not equate to the normal use of the term which refers to knowledge, intelligible data. The bits by themselves, the yeses and the nos, the ones and the zeros, do not provide knowledge. They need the context of the questions and the distribution to become actual information. For example, when opening a jpeg file, the file itself does not contain the instructions of how to read it. If you do not happen to know what a jpeg is and how to read it, you are not going to be able to interpret, to decode, the string of bits into actual intelligible data. The questions, the distribution, the context are considered given, communicated out-of-band through another scheme. As with any semantic content, this cannot be easily formalized and quantified.

The Shannon variability of the distribution, then, has nothing to do with the information the distribution itself holds. The distribution is not what is being encoded. The variability is the information needed to go from the distribution, which is given, to an individual element. It is really the information gap from the population to an element. That is why some people say the entropy is ``lack of information", which is justified because, in a way, it is the information about the elements that the distribution cannot provide. But, again, this is just missing the point of what is being described. If one is not interested in identifying elements there is no ``missing information".

In communication and information theory, information is really encoded information. Communication systems and information processors have no idea whether the source of the data is a digital thermometer or a poet. There is no knowledge per se, just symbol manipulation that may represent different concepts in different contexts.

The term information entropy, then, is misleading for two reasons. First, because it is really not entropy in the thermodynamic sense: it is not defined on states, it does not know about irreversible processes and it is unrelated to maximization at equilibrium. In fact, it is not related to physical systems. Second, it is not really information in the general sense, only in the very narrow technical sense of encoded information within a communication or information system.

So why is it so successfully used in statistical mechanics? Why does it seem to play a fundamental role even for systems far from equilibrium?[CITE]

\section{Connection to Statistical Mechanics\label{csm}}

As we saw in the previous sections, Shannon variability is a very general concept that applies to any type of distribution, no matter what the problem domain. It does not require us to define states, processes, reversibility and therefore it is conceptually different and independent from the entropy in thermodynamic and statistical mechanics which requires them.\footnote{For this reason it is unfortunate that it was given the same name.} It is only when variability is appropriately applied to this particular setting that, though not apparent at first, a connection can be made. To understand this link we need to answer two basic questions. First, what does the Shannon variability represent in the context of statistical mechanics and thermodynamics? In particular, why can it be treated as a state variable and what property of the system is it describing? Second, why does the Shannon variability satisfy the same properties as the entropy? Namely, why does it increase during irreversible evolution, why is it maximized at equilibrium and why is it additive for independent systems?

The first question is straightforward to answer. The macroscopic variables of a system (e.g. volume, pressure, energy) do not give a full description of its parts. If we fix a macrostate and then measure the state of all its microscopic constituents, we will not get the same answer every time. This is true in the case of non-equilibrium (e.g.~we suddenly compress a gas by pushing a piston) and of equilibrium (e.g.~we give enough time to equilibrate). The choice of a macrostate, then, identifies a set of possible microstates that are compatible with it. It is then valid to ask the question: what is the variability of microstates one can find within a given macrostate? This is answered by the Shannon variability, which gives us the number of bits required to identify a microstate given the macrostate. Since the macrostates fully define the distribution of possible microstates, this distribution does not depend on the particular way the macrostate came to pass. It is path independent: it is a state variable. In short, as a macrostate is a distribution over microstates, it will admit as a state variable the Shannon variability, regardless of whether we know the actual distribution and can calculate its value.

Note, though, that the thermodynamic entropy is defined differently. It is defined over a succession of equilibrium states by noting that the integral of the heat exchanged divided by the temperature is path independent. One then chooses a particular state $A$, assigns it an arbitrary entropy value $S_A$ and the entropy for all other states is given by the integral $S(B) = S_A + \int_A^B \frac{dQ}{T}$. At this point, it is not apparent why these two quantities defined independently would be related. What we can say, though, is that all quantities that increase over irreversible processes, are maximized at equilibrium and that are additive for independent systems are precisely the quantities that are linear functions of entropy (see \cite{Giles}). A linear function of entropy, though, simply means a redefinition of $S_A$ and a change of units for either $dQ$ or $T$. Therefore if we can show that the Shannon variability possesses those three properties than it is equivalent to the entropy up to a change in units, which would answer our second question.

Suppose we have an irreversible process, which means we can start from many out-of-equilibrium states which end in a single equilibrium state. During the whole process, some of the macrostate variables are going to stay constant as they characterize the constraints to which the system is subjected during the evolution and will also identify the final equilibrium. For example, if the walls are fixed and insulated, the gas will have a set volume and average energy, which will lead to a unique equilibrium with a given temperature and pressure.\footnote{It can also happen that the irreversible process does not have a single possible equilibrium, as in the case of spontaneous symmetry breaking. New variables will identify the equilibrium which we can think of as new constraints appearing once the initial symmetry is broken, thus reducing to the previous case.} As different out-of-equilibrium macrostates merge during the evolution, they merge the possible microstates that lead to the same macrostate. The variability of microstates within the initial out-of-equilibrium macrostate, then, must be smaller than the variability of the microstates within the final equilibrium state. Irreversible processes increase the Shannon variability within the macrostate, which corresponds to a greater uniformity of the macrostate itself since uniform distributions are the ones that maximize the variability.

To understand why this is maximized at equilibrium, suppose it were not. That is, suppose there were another macrostate which satisfied the same constraints, but for which the Shannon variability were greater. If we prepared such a state and let it equilibrate, it could not evolve to the equilibrium because that has lower Shannon variability. Its equilibrium, then, would be different from the previous one, but this cannot be since we said that those constraints would lead to a single equilibrium state. Therefore the Shannon entropy is maximized for an equilibrium state.

Now suppose we have two independent systems, like two volumes of gasses. As they are independent, picking the microstate of one system does not constrain the microstate of the other. That is, the distribution over microstates are statistically independent. This means that the joint distribution is the product of the marginal distributions and, in this case, the Shannon entropy is additive.

The Shannon variability, then, has all the right properties to establish a conceptually meaningful connection with the thermodynamic/statistical mechanics entropy. It is a state variable that represents the variability of microstates within a macrostate. It increases during irreversible processes, reaches the maximum at equilibrium and it is additive for independent systems. It is indeed an independent concept but when applied to macrostates yields a state variable with the right conceptual characteristics. There are still some mathematical details that need to be worked out to have a complete picture. These details don't change the overall intuitive picture, but they present yet again opportunities for confusion.

The first is the role of the Boltzmann constant $k_B$, which is a conversion factor between the Shannon variability $H = - \sum p_i \log(p_i)$ and the Gibbs entropy $S_G = - k_B p_i \sum \log(p_i)$. This conversion constant should not surprise us, since we only established a linear relationship between variability and entropy. The question is whether this conversion constant itself has a physical meaning, or it is also simply due to a choice of units. The standard relationship between temperature, internal energy and entropy is $T = \frac{\partial U}{\partial S}$. Historically, temperature and internal energy are the quantities that were defined first. If we measure temperature in Kelvin and energy in Joules, the above relationship forces us to measure entropy in Joules/Kelvin. If we changed units, $k_B$ would change accordingly.

However, this is not the only way to define the relationships between those quantities. Another way is to have $\beta = \frac{1}{k_B T} = \frac{\partial S}{\partial U}$. In this setting, $U$ is measured in Joules, $S$ in nats (which are dimensionless), $\beta$ is measured in inverse Joules. Temperature is still measured in Kelvin, but we do not regard it as a primary object. We find this approach, sometimes called entropy first, preferable for a number of reasons, especially in statistical mechanics. First, entropy is invariant under coordinate transformations and therefore it is better to keep it a pure number and not mix it with units of energy, which is not invariant under boost.\footnote{This is essential if one ever hopes to put statistical mechanics into a relativistic setting.} Second, the maximization of entropy at equilibrium guaranties us that $S(U)$ is a single valued function (i.e. the maximum given the value of $U$) and $\frac{\partial S}{\partial U}$ is therefore well defined. On the other hand, $U(S)$ is not in general single valued (e.g. in the Ising model). This means that temperature can be infinite where the curve inverts, where small changes of energy do not change the entropy, and can be negative in the other region. That is, the infinitely cold system is temperature zero, which cannot be achieved, as it warms up temperature rises until it becomes infinite, and as the system becomes even hotter the temperature is negative and increase toward zero, which would correspond to an infinitely hot system. On the other hand, $\beta$ is always finite and cannot be infinite. An infinitely cold system, then would correspond to positive infinite $\beta$, as the system warms up beta decreases, passes from zero, becomes negative and an infinitely hot system corresponds to minus infinite $\beta$: this is a much better behavior for a physically meaningful quantity. Third, when setting up entropy maximization, $\beta$ is quite naturally the Lagrange multiplier associated with energy conservation, therefore mathematically it has a much clearer role. Lastly, one can proceed in both thermodynamic and statistical mechanics with just $\beta$, and convert to temperature only when a value on the Kelvin scale is needed. For these reasons, it seems to us $\beta$ plays the more fundamental role and $k_B$ is the conversion coefficient to go from $\beta$ to temperature measured in Kelvin. Adding and removing the Boltzmann constant from the entropy expression, then, is equivalent to shift from an energy/temperature first approach to an entropy/$\beta$ first approach and should be not seen as physically significant. It is really just fixing a unit.

The other point of possible confusion is that in statistical mechanics there are actually two distributions. A single microstate is itself a distribution of particles over their particle states. There are actually $N$ particles existing at the same time, each with its own particle state. And a macrostate is a probability distribution over microstates. It is important to distinguish between these two distributions, especially because a microstate can be represented in two ways.

The first way is as a distribution over a single copy of phase space, sometimes referred to as $\mu$-space. In this case the distribution represents the fraction of particles that are in that state. The Maxwell-Boltzmann distribution $\rho(q^i, p_i) = \left(\frac{\beta m}{2\pi} \right)^{3/2}e^{-\beta \frac{p_ip^i}{2m}}$ is expressed in this space. On $\mu$-space, the Shannon variability $-\int\rho \log \rho$ does not coincide with the thermodynamic entropy for a simple reason: it does not depend linearly on the number of particles. It is not an extensive quantity. The reason is straightforward: the Shannon entropy works on normalized distributions and gives us the variability of a single element. In this case, the system is composed by $N$ elements, not one. One can use the expression $-N\int\rho \log \rho$ as this represents the variability of $N$ elements.

We want to stress that a distribution on $\mu$-space can only either describe a single microstate where $N$ is very large and therefore the distribution can be assumed to be continuous, or a probability distribution for $N$ identically and independently prepared particles. A macrostate cannot be described by a single distribution on $\mu$-space as this cannot capture correlations. One would need to work with a probability distribution (i.e. the weights represents probabilities) over the space of particle distributions (i.e. the weights represent fractions of total particles) over $\mu$-space or one would obtain erroneous results.\footnote{Unfortunately this happens, for example, in [Jaynes] where he creates the distribution over $\mu$-space by averaging over all microstates. This essentially blends together the probabilities and particle counts losing most of the actual statistical information. Naturally, the entropy thus computed is wrong and found to not work.}

Alternatively, a microstate can be represented as a point over $N$ copies of phase space, sometimes referred to as $\Gamma$-space. It is on this space that the Gibbs entropy is defined and where thermodynamic entropy happens to coincide with the Shannon entropy. Since this is a probability distribution, the microstate corresponds to a single element within the distribution defined by the macrostate. Yet, one has to be careful of two things. The first is that permutations of particles within $\Gamma$-space lead to different points which should not lead to different microstates. This leads to the widely known problem of overcounting which needs to be addressed in the standard way. The second problem is that we should not think of the particles as literally pointlike: they would correspond to delta Dirac distributions over phase-space which have minus infinite Shannon entropy. It is more appropriate, both mathematically and conceptually, to think of indistinguishable particles as identical peaked distributions over $\mu$-space, each contributing the same amount of Shannon entropy.\footnote{Setting to $-\log h$ the entropy corresponding to each degree of freedom of these peaked distributions is a natural way to incorporate the effects of the uncertainty principle.}

The last point of confusion is the invariance of entropy under coordinate transformations. The use of phase-space in statistical mechanics is of paramount importance for one reason and one reason only: it is the only type of manifold where entropy can be defined in a way that is coordinate invariant. The issue is that we do not define fractions/probabilities over continuous variables: we defined densities. For a discrete case, we have a finite amount, like 3\%. For a continuous case we have an infinitesimal contribution, like 3\% per meter. If we switch labels in the discrete case, 3\% remains 3\%. If we switch to kilometers in the continuous case, we have 3000\% per kilometer. The continuous version of the Shannon entropy is affected by a coordinate transformation by the integral of the logarithm of the Jacobian determinant. Any transformation where the Jacobian determinant is not unitary, where the volumes are not preserved, does not preserve the Shannon entropy. This is not a problem for the Shannon entropy in general. It is a problem if we want to give the Shannon entropy any physical meaning.

What happens in phase space is that, under coordinate transformation $\hat{q}^i = \hat{q}^i(q^j)$, the $d\hat{q}^i = \frac{\partial \hat{q}^i}{\partial q^j } dq^j$ vary like vector components while $d\hat{p}_i = \frac{\partial q^j}{\partial \hat{q}^i } dp_j$ vary like covector components. Therefore the areas $d\hat{q}^i d\hat{p}_i = dq^j dp_j$ remain the same. The Jacobian determiants are unitary and the Shannon entropy is invariant. This brings us to another point of confusion: it is often argued that phase space areas are important because they are conserved under Hamiltonian evolution. While they are indeed conserved, that is not the reason why they are important. It is their invariance under coordinate transformation that makes them physically significant. In fact, conservation over time would not even be well defined if the quantity was not invariant and therefore have different values depending on the observer.

This means that we always have to consider distributions over the full phase-space with the appropriate conjugate coordinates if we want to use $-\rho \log \rho$. If we use distributions over position and velocities or over momentum only, the expression would change for those particular variables in those coordinate systems. This can be, again, a source of confusion.\cite{Dunkel}

The last issue is the relationship with the Boltzman entropy $S_B= k_B \log W$. This is actually captured by another standard way to derive the Shannon variability. Suppose we have a sequence $N$ elements, say twenty balls. There are $I$ possible types of elements, say three possible colors: red, green and blue. Suppose $N_i$ are the elements of each possible types, which means $\sum_{i \in I} N_i = N$. Then, given a set of $N_i$, we have $W(N_i) = \frac{N!}{\prod_{i \in I} N_i}$ possible ways to realize that sequence which corresponds to all possible permutations. If all permutations are equally likely, then $\log W$ represents the number of bits needed to identify one of the sequences. Since there are $N$ elements in the sequence, $\frac{1}{N} \log W$ represents the average number of bits needed to identify one element. We now take the limit for large $N$, that is we ask what is the average number of bits needed to identify one element from an infinite sequence given by the weights $p_i = \frac{N_i}{N}$. We find $\frac{1}{N} \log \frac{N!}{\prod_{i \in I} N_i} \approx - \sum p_i \log p_i$. In other words, the Boltzman entropy reduces to the Shannon entropy when counting permutations within large distributions.

These technical details are important to understand the fine details of the links between the different entropies, they do not alter, and should not distract us, from the main conceptual point. The Shannon variability, the thermodynamic entropy, the Boltzmann entropy and the Gibbs entropy, though describing different things, are conceptually related. The variability of the state of the parts within the state of the whole is what the Shannon variability quantifies if applied to the macrostates of thermodynamics and the microstates of statistical mechanics. This variability increases during irreversible evolution, more microstates converge to the same macrostate until equilibrium is reached, where the variability is maximized. The variability of one system does not affect the variability of an independent one and therefore it simply sums. This is not a coincidence: this is what thermodynamic entropy ultimately is about.

\section{Comments on Boltzmann's $H$ Theorem}

[This is for you Gabriele]. In Boltzmann's 1872 paper, he introduced the function that later was called the $H$ function, though it seems he meant to call it [upper case] $\eta$ and it was printed as $E$ (which somehow later became $H$). In the paper Boltzmann describes (in $\mu$-space) a gas of monatomic molecules that has already become spatially uniform and also has the same distribution wrt the particles' kinetic energies $x$ at each space point. He therefore considers a spatially constant distribution function $f(x,t)$ and wants to show that whatever its initial value it will tend in accordance with the Boltzmann equation (which he has previously derived and I won't give here) to the Maxwell distribution $f(x,t)=C\sqrt xe^{-hx}$ as $t\rightarrow\infty$. He defines his function $E$ as follows:
\begin{equation}
E=\int_0^\infty f(x,t)\left\{\textrm{log}\left [{f(x,t)\over \sqrt x}\right ]-1\right\}\textrm dx\label{hfun}
\end{equation}
and aims to prove that it cannot increase and attains its least value when $f(x,t)$ becomes the Maxwell distribution. Boltzmann does not give the argument that leads him to the function (\ref{hfun}), but somewhat later in the paper he notes that the $-1$ in the integrand merely reflects the fact that
$$
\int_0^\infty f(x,t)\textrm dx
$$
is the total number of molecules in unit volume, so that 
\begin{equation}
E=\int_0^\infty f(x,t)\textrm{log}\left [{f(x,t)\over \sqrt x}\right ]\textrm dx\label{hfunb}
\end{equation}
also cannot increase.

Boltzmann does not say how he arrived at the function (\ref{hfun}), but its form is obviously dictated by the form of the Maxwell distribution. I'm terrible at equations, but I assume the $\sqrt x$ in the denominator of the integrand in (\ref{hfunb}) is because the usual Shannon expression is `skewed' by the subsidiary Lagrangian multiplier that fixes the total energy to a definite value. Later in the paper, when considering polyatomic molecules, he arrives by means of arguments I have not yet attempted to understand at the familiar $f\, \textrm{log}\,f$ form.

In his very concise 1860 derivation of his energy distribution function, Maxwell commented that

\begin{quote}\small

It appears from this proposition that the velocities are distributed among the particles according  to the same law as the errors are distributed among the observations in the theory of the ``method of least squares''.

\end{quote}\normalsize

In his much more substantial 1867 paper, Maxwell noted that his 1860 derivation could be seen as `precarious' and gave an alternative proof. However, this does not eliminate the mathematical identity of his distribution with the error distribution. Would you agree that this (somewhat indirectly) establishes another example of variablity within a distribution of things [which I think we might use instead of `elements'; `members' (of a set) is another word we might consider]?


\section{Conclusion}

\bibliographystyle{unsrt}  
\bibliography{bibliography}



\end{document}
