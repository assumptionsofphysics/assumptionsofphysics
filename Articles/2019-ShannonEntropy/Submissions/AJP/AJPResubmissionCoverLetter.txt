Dear Dr. Price:

To address the points you raise, we have added references and modified the introduction to make more explicit that the novel aspect of the work is a more general characterization of the Shannon formula, which can be applied to any field, including information theory and statistical mechanics.  We then lay out the exact relationship of the Shannon formula to the Boltzmann, Gibbs, and von Neumann entropies in statistical mechanics.  We also clarify several issues that appear when the formula is applied to the continuum.

We note that the previously submitted draft had been shortened from our original writing, with references and additional historical context removed, in order to come closer to the AJP article length guidance.  

Regarding specifically the AJP Resource Letter on "Information Theory in Physics," which we now cite, we note that only 15 of the 160 references therein are on general physics rather than purely information theory or on specific applications.  While we have increased the number of citations (e.g. we now cite two rather than one of the historical papers by Jaynes included there), none of these or others we have seen in the literature try to achieve a goal similar to that of our work.  We are not relating information to physics, or trying to understand the nature of the arrow of time. We are identifying a universal characterization of the Shannon formula that can be applied to any field that uses statistics, including information theory and statistical mechanics, which means it is more fundamental.

One cannot truly understand "entropy" if one does not first realize that it is not a fundamental concept, but a combination of others. Our work starts to disentangle one of them.