\documentclass[10pt,twocolumn, nofootinbib]{revtex4-2}
%\documentclass[aps,pra,10pt,twocolumn,floatfix,nofootinbib]{revtex4-1}
%\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amsfonts}

\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	citecolor=blue,
	urlcolor=blue,
	linkcolor=blue
}
\urlstyle{same}
\frenchspacing


\begin{document}

\title{Why classical mechanics fails}
\author{Gabriele Carcassi, Christine A. Aidala}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}

\date{\today}


\begin{abstract}
We present the idea that, contrary to the established view, classical mechanics fails for conceptual reasons, not merely because it does not match experimental evidence. The notion of a classical system stems from two underlying assumptions, perfect isolation and infinitesimal divisibility, that are in contradiction: at some point we will reach a scale for which the parts can no longer be assumed to be isolated. This result defines more cleanly the realm of applicability of classical mechanics, shows what problems quantum mechanics addresses and gives hints as to what other problems will need to be solved by a more general theory.
\end{abstract}

\maketitle

\section{Introduction}

The common view is that classical mechanics is a perfectly consistent theory that just does not happen to be true. It fails simply because experiments do not agree with it.\footnote{CITE: papers that either make the claim, or that discuss how everybody makes that claim} This view is so established, so ingrained, that it is seldom discussed explicitly, and we believe it to be part of the reason why quantum mechanics still feels, after almost a century, so mysterious: how are we going to conceptually appreciate a theory if we do not even know what conceptual problems it is trying to address?

Our argument can be schematically summarized as follows:
\begin{enumerate}
	\item To define a system, we need to define a clear boundary between the system and everything else
	\item The interaction across the boundary participates in the definition of the system
	\item Perfect characterization of a system requires perfect isolation
	\item As we get to smaller and smaller parts, perfect isolation becomes untenable
\end{enumerate}
In a nutshell, classical mechanics fails because the definition of a system is too simplistic. It fails to recognize the role the system/environment interaction plays in the characterization of the system and its state space.

We will analyze these ideas first conceptually and how they translate into inconsistencies between classical mechanics on one side and thermodynamics/statistical mechanics on the other. We will then show how quantum mechanics addresses some of these issues and how it leaves some open, to be solved in future theories.

\section{System/environment boundary}

When defining a physical system, we need to define, implicitly or explicitly, its boundary. Whether we are studying a solid object, an open container of gas, a chemical compound or a single particle, we need, at the very least, to be clear as to what is in the system and what is out. This much should be evident. What may be less evident is that the nature of the interaction at the boundary between the system and the environment is also important, not just in thermodynamics and statistical mechanics, but more broadly to all physical systems. Let us start with the following simple example.

Suppose we want to study the motion of a cannon ball. As the cannon ball moves, air molecules will scatter off its surface. Yet, the effect produced will be negligible. We can therefore describe the state of the cannon ball with a single, infinitely precise, pair of position and momentum. Now suppose we want to study the motion of a speck of dust. This time the effect of the air molecules will not be negligible. We will therefore describe the state of the speck of dust with a probability distribution over position and momentum. Now suppose we want to study the motion of the cannon ball on the surface of the sun. Plasma will scatter off the ball catastrophically, destroying the ball. 

The example illustrates that the system, the interaction at the boundary and the state space for the system are not three independent choices. One may constrain the others. A cannonball at 5000 Kelvin does not exist, or at least not for long. Conceptually, we can think of a property as being intrinsic to the system only if the interaction with the environment leaves it, at least to a good approximation, undisturbed. If it is not left undisturbed, then it is no longer a property of the system itself: it is a shared property between the system and the environment because the nature of both comes into play in its definition.\footnote{CITE: maybe someone in complex systems, emergent properties, ... make similar claims}

While we are used to think this way for thermodynamic quantities, like temperature or pressure, or statistical quantities, like average energy or mean free path, we typically think of mechanical properties, like position and velocity, differently. These, we think, are always intrinsic to the system. However, we have no basis for that distinction. What happens is that, in the vast number of cases, we can sufficiently isolate the system so that the effect of the environment on the motion of the body is negligible. We can take a speck of dust, put it in a vacuum, and the position is now much more stably definable. Therefore, when studying the speck in a fluid, the motion is modeled as fluctuations of an infinitely precise quantity because, at least conceptually, we could instantly remove the air and determine the position of the speck in isolation. But, realistically, we can only go so far. Perfect isolation is impossible.

Apart from practical concerns, any wall we will use as shielding will necessarily have finite temperature and will emit thermal radiation; moreover, gravitational interactions cannot be shielded in the first place.\footnote{CITE: something that goes more into the details as to why gravitational interactions cannot be shielded} Even theoretically, complete isolation is not possible. We cannot, even conceptually, instantly remove all the background interactions. If that is the case, we have to concede that even properties like position and momentum are not intrinsic to the system: they are the characterization of the system together with the environment (e.g. the gravitational field).

One may still argue that these are still circumstantial problems that derive from the incompleteness of our current understanding. We could always imagine that we will be able, in the future, to achieve perfect isolation. But let us explore that possibility in detail. Perfect isolation would mean that no signal, electromagnetic, gravitational or otherwise, would be able to interact with the object. Then, how are we supposed to measure anything about the system? It is experimentally inaccessible. Maybe we are already surrounded by perfectly isolated systems: we have no way of knowing.

This issue uncovers another couple of hidden assumptions. First of all, the observer is always part of the environment as it is not part of the system. The second is that a measurement has to cross the boundary. The act of measurement may interfere with the previous nature of the system/environment interaction, which as we saw participates in defining the properties of the system itself. Therefore the act of measurement may change the nature of the very properties that we are going to measure.

Note that these are definitional issues, not mechanistic issues. It is not about what the system is doing, what the rules are that the most fundamental objects follow. It is about how we define the system in the first place: how we carve up the world into parts that we can study independently. If we take a physical system simply as something we want to study under a set of conditions, and the state to be the experimentally accessible information that is well defined under such conditions, these issues will always crop up. They need to be addressed, even if just by simplifying assumptions, when the system is being defined.

\section{Classical failure}

We are now in a position to understand the failure of classical theories. As we saw before, we can assign intrinsic properties to systems that can be, at least in principle, isolated perfectly. For large scale objects, this is not a problem: small perturbations will have small effects which can, at least at a certain scale, be neglected. But this is an idealization. The infinite precision of a real number is valid only insofar that the idealization holds.

Assuming perfect isolation by itself is not an issue. However, classical mechanics also assumes, implicitly, that this perfect isolation holds for objects of all sizes. With continuous materials, we can take smaller and smaller parts. Classical fields allow for wave packets of ever decreasing energy. The theory implicitly assumes all these objects can be prepared, manipulated and detected independently enough that their state can be perfectly defined. This is the contradiction inherent to classical mechanics: perfect isolation at all scales.

The disconnect is that many take the classical state to be the ``real'' state of the system. But this is more of a philosophical construct. What physics should be concerned with is only the part of reality that is experimentally accessible under suitable conditions (e.g. repeatability, independence from other systems, ...). This does not mean that objects we cannot study experimentally do not exist or are not worth studying. It just means that our physical theories should not make any claims about them as they are not the subject of physics.

Since this is a point of contention,\footnote{CITE: papers that show different view, papers that claim there are different views.} and we do not believe further discussion along these lines will change anyone's perspective, let us switch gears. We are going to argue that a perfect description of the type provided by classical mechanics is inconsistent with thermodynamics and statistical mechanics.

We start by asking: what is the entropy of a single microstate? This is complicated by the fact that we have (at least) two definitions of entropies, and people argue about which is the correct one.\footnote{CITE: papers with different stances on entropy; papers that claim different stances} Therefore let us use both definitions and see what happens.

Recall that a microstate for a classical system of $n$ particles is a point in a $\mathbb{R}^{6n}$ space, that represents all their three-dimensional position and momenta. The fundamental postulate of statistical mechanics states that the entropy is $S = k \log W$, where $W$ measures the  microstates. At first glance, since we have a single microstate, we are inclined to say $W = 1$. This would be true for a discrete state space. However, for a continuum state space this is the wrong answer.\footnote{This is an example where the difference between discrete and continuous spaces plays a fundamentally important role in physics. Unfortunately, too many physicists confuse discrete with countable.} We need to use the phase space volume. Since we have a single point, its volume is zero, therefore we have $S = k \log 0 = -\infty$: the entropy of a single microstate is minus infinity.

Let us now confirm the result with the Gibbs/Shannon entropy $S = -\int \rho \log \rho \, dq^{3n} dp^{3n}$. A single microstate would be described by a delta distribution centered on that microstate: $\delta(q^i-q^i_0, p^i-p^i_0)$. The distribution is zero everywhere, except for the microstate where it is infinite. Therefore the integral returns minus infinity.

No matter our definition, then, the entropy of a single microstate is minus infinity. This certainly does not sound terribly reassuring... but is it a problem?

First of all, this is in direct contradiction with the third law of thermodynamics. While the third law has many different formulations, it always leads to requiring the entropy of a perfect crystal at zero Kelvin to be zero. The idea is that a perfect crystal at zero Kelvin only has one possible configuration: it has one possible microstate. But we have seen that the entropy of a single microstate is minus infinity, not zero. Therefore the result is in contradiction.

Secondly, if we had at our disposal a system with minus infinite entropy, we could use it to circumvent the second law of thermodynamics. Suppose, in fact, we used a heat engine to transform some heat into work. By the second law, this must generate an increase in total entropy $\Delta S$. If we transfer that increase of entropy to our minus infinite entropy system, we reset the entropy of the engine and the reservoirs to the initial state. Yet this leaves the entropy of our system unchanged: minus infinity plus a finite number is still minus infinity.

Since the laws of thermodynamics are considered by some merely phenomenological,\footnote{CITE: papers that claim that thermodynamic laws are only valid on average} let us proceed in a different direction. Note that the entropy is additive for independent systems. Therefore the entropy of the composed system $A+B$ is $S_{A+B} = S_A + S_B$, the sum of the individual entropies. Consider now the empty system $\emptyset$. Composing any system with the empty system will have to give us back the original system $A+\emptyset = A$.\footnote{In mathematical terms, the set of all systems forms a monoid under system combination and the empty system is the identity element.} This means that the entropy of the empty system must be zero. Therefore we have a clear physical meaning for zero entropy that is, at first glance, unrelated to the third law: zero entropy is the entropy of the empty system.

Note that once we say that a system is missing, we have given a complete description of the system. There is one and only one way that a system may be missing: the empty system has only a single (discrete) microstate. Also, there is no more information that can be given about a system after one says that it is missing. An entropy lower than zero would correspond to a description that is more refined, more precise than that of an empty system. But, as we said, no such thing exists. So, from an information perspective, no system can have entropy lower than zero.\footnote{We believe this to be the true form of the third law.} A single classical microstate violates that.

So classical mechanics fails because it allows for the possibility of states that can never exist, those with minus infinite entropy. On practical grounds, we could bypass the second law with all its implications. On theoretical grounds, these ensembles do not respect the third law. On logical grounds, they would provide more information about the system than stating that the system does not exist.

We want to stress here that these entropic arguments are not independent from the isolation arguments. Thermodynamics as a physical theory is exactly about describing what happens across the boundary. Heat and work are energy exchanges by one system to others. Perfect control of a system is effectively Maxwellâ€™s demon in disguise.

Those who are familiar with communication theory are aware that the channel capacity is given by the signal-to-noise ratio.\footnote{CITE: papers that define channel capacity (Shannon?)} In physics terms, the noise is the background interaction of the system with the environment. So that fixes the zero: we cannot use the system to encode more information than the background interaction allows. Saying that we can reduce the interaction with the system as much as we want effectively means setting the noise level to minus infinity, removing the background, perfectly isolating the system. In this light, all of these arguments are actually the same argument: we cannot fully isolate a system, we cannot encode infinite information in a state, we cannot have an infinite entropy differential.

\section{Quantum resolutions and open problems}

While this paper is not about providing a clear and complete understanding of quantum mechanics, we want to give a sense of how the new theory partially solves some of the problems of the old one.

To start, in quantum mechanics all pure states have zero entropy, including the vacuum, the ``empty'' system. Mixed states have positive entropy. We can no longer encode infinite information within the state of a system with continuous variables. Conceptually, the accessible information of the system is quantized by setting a lower limit, a zero, that cannot be crossed. This removes all the inconsistencies with thermodynamics, statistical mechanics and information theory. Conceptually, it signifies there is always a background interaction with the environment that cannot be fully eliminated.

If there is interaction at the system/environment boundary, then the properties of the system can only be defined up to the statistical fluctuations imposed by the interaction. In fact, in quantum mechanics the probabilistic description is baked into the very notion of state, it is not something that can be removed like in classical statistical mechanics.

If the system/environment interaction at the boundary participates in the definition of the state of a system, then states cannot be understood as intrinsic properties of the system that exist before a particular type of interaction has taken place. This is exactly what happens in quantum mechanics: an observable is well defined only when the system is put in determined conditions. A measurement may change those conditions as it is, in general, a change of the system/environment interaction. Contextuality is therefore a feature, not a problem to get rid of, and we should realize it is not even an exclusive feature of quantum mechanics.

In fact, suppose we have an open flask of gas in equilibrium. This would be described by a grand canonical ensemble, identified by its temperature $T$, volume $V$ and chemical potential $\mu$. The number of particles $N$ of the gas will not be well-defined as it fluctuates, and it is described by a probability distribution. If we want a state where $N$ is well-defined, we can simply close the flask and let the system equilibrate. We will obtain a canonical ensemble determined by the same $T$ and $V$ and by the particle number $N$. The number of particles will depend on the fluctuation at the time we closed the flask, therefore we will have a certain probability to have a particular $N$. In short, we start with a state where $\mu$ is well-defined but not $N$, we interact with the system and end up in a state where $N$ is well defined and its value determined by a probability distribution. Note the parallel with quantum mechanics: we start with a state where an observable, say the energy, is well defined and we end in a state where another observable, say position, is well defined with a value determined by a probability distribution. We change context, we change type of ensemble, we change what quantities define the ensemble. The point is that quantum contextuality is not the exception, it is classical \emph{non}-contextuality that is the exception.

For these types of reasons quantum mechanics is more precise than classical mechanics: because it better captures the definition of our system. It captures the effect of a constant minimal interaction of the system  with the environment which classical mechanics simply disregards. If anything, quantum mechanics does not go far enough.

For example, while the accessible information about the system is quantized, the possible values of the observables themselves are not. That is, position and momentum are still modeled by a real quantity. This implicitly assumes that our detectors can be made arbitrarily precise, that the resolution of our equipment can always be made finer than the resolution characterized by the quantum system. But our equipment, supposedly, is made of quantum systems. Therefore, at some point, this assumption has to fail. These types of metrological requirements will ultimately need to be incorporated into a more precise theory.

Another issue is that all pure quantum states have zero entropy and therefore the entropy cannot change during evolution. This makes it impossible to characterize what happens during out-of-equilibrium transitions, when the behavior at the boundary drastically changes. Note that quantum projections are exactly of this type. The temptation is simply to include part of the environment within the system, and then trace it out. This assumes that the interaction between the system and the part of the environment is such that it can be in principle ``switched off'', as in the case of the air with the speck of dust, and therefore does not take part in the definition of the system itself. However, it is conceivable that, while out of equilibrium, the overall system temporarily reaches states where each component cannot be described as a separate independent object. This type of interaction would necessarily influence the definition of the system, and therefore cannot be simply layered on top of the individual components. This would seem out of reach of the current quantum framework.

\section{Conclusion}

We believe these arguments help shed light onto why classical mechanics fails at a conceptual level and pave the way to a better understanding of quantum mechanics. They also give insights into what other problems still need to be addressed at a fundamental level.

We close with the following general remark. Much of the work on the foundations of physics nowadays is geared toward finding the ``true'' objects and rules of nature. \footnote{CITE: papers that propose emergent models of space time, ...} Even in quantum mechanics, a lot of efforts are aimed at finding the true underlying constituents and mechanisms that would explain the quantum world.\footnote{CITE: papers that propose underlying mechanism (e.g. Bohmian approach, GRW)} In this sense, theoretical physics is built bottom-up. We believe this general approach to be problematic.

Experimental physics is built top-down: we start with big objects, and decompose them into smaller and smaller parts. The objects of our research are defined by what parts can be carved out, accessed and manipulated experimentally, independently from one another. This imposes a number of requirements, constraints and assumptions that allow us to define the physical system. These definitional requirements are, therefore, not practical considerations that only experimentalists should be bothered by. They must be at the foundation of our theoretical treatment since, without their stipulation, there is no physical system to study. In this sense, they are more fundamental than the laws the system obeys, because we cannot define the law before having a well-defined system.

We believe it is the inclusion of these definitional requirements and assumptions, which we have started to sketch here and in our more general project Assumptions of Physics, that will ultimately form a more sound foundation for physical theories.



\bibliography{bibliography}


\end{document}