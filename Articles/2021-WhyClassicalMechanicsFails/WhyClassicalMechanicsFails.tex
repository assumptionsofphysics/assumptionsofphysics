\documentclass[10pt,twocolumn, nofootinbib]{revtex4-2}
%\documentclass[aps,pra,10pt,twocolumn,floatfix,nofootinbib]{revtex4-1}
%\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amsfonts}

\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	citecolor=blue,
	urlcolor=blue,
	linkcolor=blue
}
\urlstyle{same}
\frenchspacing


\begin{document}

\title{Why classical mechanics fails}
\author{Gabriele Carcassi, Christine A. Aidala}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}

\date{\today}


\begin{abstract}
We present the idea that, contrary to the established view, classical mechanics fails for conceptual reasons, not merely because it does not match experimental evidence. The notion of a classical system stems from two underlying assumptions, perfect isolation and infinitesimal divisibility, that can never be physically realized at the same time: at some point we will reach a scale for which the parts can no longer be assumed to be isolated. This result defines more cleanly the realm of applicability of classical mechanics, shows what problems quantum mechanics addresses and gives hints as to what other problems will need to be solved by a more general theory.
\end{abstract}

\maketitle

\section{Introduction}

The common view is that classical mechanics is a perfectly consistent theory that just does not happen to be true. It fails simply because experiments do not agree with it.\footnote{CITE: papers that either make the claim, or that discuss how everybody makes that claim} This view is so established, so ingrained, that it is seldom discussed explicitly, and we believe it to be part of the reason why quantum mechanics still feels, after almost a century, so mysterious: how are we going to conceptually appreciate a theory if we do not even know what conceptual problems it is trying to address? [TODO: expand if needed]

We present two arguments, which can be understood as the same argument in different form:
\begin{enumerate}
	\item The entropy of a single microstate over a continuum has minus infinite entropy, which makes classical mechanics incompatible with thermodynamics
	\item Classical mechanics implicitly assumes perfect isolation at all scales, which can only be understood as a simplifying assumption
\end{enumerate}
The link between the two is that the zero entropy level, the lowest physically possible, corresponds (on the continuum) to a minimal uncertainty. In a nutshell, classical mechanics fails because the definition of a system is too simplistic. It fails to recognize the role the system/environment interaction plays in the characterization of the system and its state space.

\section{Entropic minimum}

We will start our investigation with a seemingly innocent question. What is the entropy of a single classical microstate? That is, suppose we have a system composed of $N$ classical particles. A complete configuration, a microstate, is given by the position and momentum of all $N$ particles. Suppose we pick a macrostate that coincides with a single microstate, a pure microstate. What is the entropy associated to that pure microstate?

Many would instinctively think that the answer is zero, but it not the case. Zero would be the correct answer if the space were discrete, but classical phase space is not discrete. To calculate the right answer, we can use the fundamental postulate of statistical mechanics\footnote{CITE} which states that the entropy is $k \log W$, where $W$, the count of microstates, is given by the volume of the region of phase space that represents the macrostate. In our case, the region is a single point, which has volume $0$. Therefore we have $k \log 0 = -\infty$: the entropy of a single microstate is minus infinity. Alternatively, we can use the Gibbs/Shannon entropy $ -\int \rho \log \rho \, dq^{3n} dp^{3n}$.\footnote{CITE} Our macrostate is described by a delta distribution centered on a single microstate: $\delta(q^i-q^i_0, p^i-p^i_0)$. The distribution is zero everywhere, except for the microstate where it is infinite. Therefore the integral returns minus infinity. No matter how we calculate it, the entropy of a single microstate is minus infinity. 

The question now becomes: does it make physical sense for a state to have minus infinite entropy? If not, what are the consequences?

First of all, this is in direct contradiction to the third law of thermodynamics. While the third law has many different formulations, it always leads to requiring the entropy of a perfect crystal at zero Kelvin to be zero.\footnote{This is often taken to mean that a perfect crystal at zero Kelvin is frozen into a single configuration, but, as we saw before, that is not true. Zero entropy corresponds to a cell of unitary volume or to a uniform distribution over a unitary volume. Discrete and continuous spaces work differently at a measure theoretic level, which make both probability and entropy work differently as well.} Since the entropy for a single microstate is minus infinity, there is a clear violation.

Secondly, if we had at our disposal a system with minus infinite entropy, we could use it to circumvent the second law of thermodynamics. Suppose, in fact, we used a heat engine to transform some heat into work. By the second law, this must generate an increase in total entropy $\Delta S$. If we transfer that increase of entropy to our minus infinite entropy system, we reset the entropy of the engine and the reservoirs to the initial state. Yet this leaves the entropy of our system unchanged: minus infinity plus a finite number is still minus infinity.\footnote{This is essentially a redressing of Maxwell's demon. Identifying any variable with infinite precision collapses the phase space volume of possible microstates to zero, giving a state that violates the third law and that would allow to violate the second as well.}

Since the laws of thermodynamics are considered by some merely phenomenological,\footnote{CITE: papers that claim that thermodynamic laws are only valid on average} let us proceed in a different direction. Note that the entropy is additive for independent systems. Therefore the entropy of the composed system $A+B$ is $S_{A+B} = S_A + S_B$, the sum of the individual entropies. Consider now the empty system $\emptyset$. Composing any system with the empty system will have to give us back the original system $A+\emptyset = A$.\footnote{In mathematical terms, the set of all systems forms a monoid under system combination and the empty system is the identity element.} This means that the entropy of the empty system must be zero. Therefore we have a clear physical meaning for zero entropy that is, at first glance, unrelated to the third law: zero entropy is the entropy of the empty system.

Note that once we say that a system is missing, we have given a complete description of the system. There is one and only one way that a system may be missing: the empty system has only a single (discrete) microstate. Also, there is no more information that can be given about a system after one says that it is missing. An entropy lower than zero would correspond to a description that is more refined, more precise than that of an empty system. But, as we said, no such thing exists. So, from a conceptual perspective, no system can have entropy lower than zero.\footnote{In reverse physics, we call this the Principle of Maximal description.} A single classical microstate violates that.

We are in the paradoxical position that classical mechanics is founded on states with perfectly defined continuous quantities that, on thermodynamic grounds, can never be realized. On practical grounds, we could bypass the second law with all its implications. On theoretical grounds, these ensembles do not respect the third law. On logical grounds, they would provide more information about the system than stating that the system does not exist. But what is the physical cause of this contradiction?

Let us look at it from an information theoretic sense. Suppose we use the state of a physical system to encode some data. That is, we choose the state based on the specific values we want to encode. And suppose we let the system reach equilibrium with the environment. The higher the entropy of the final state, the more information will be lost. In other words, we can understand the entropy as an indicator of the disturbance the system-environment interaction placed on the state of the system. Given that the state of a classical system is fully specified by continuous quantities, we would be able to encode an infinite amount of information on a finite system. Conversely, any finite amount of noise introduced by the environment on the system would mean the loss of infinite information. If we imagine that there is a minimum amount of system-environment interaction that cannot be avoided, then this sets the zero of how much information can be encoded or retrieved on a physical system.

Before we explore more what happens across the system-environment boundary, we note that quantum mechanics does solve this problem. The entropy of any pure state in quantum mechanics is zero. The entropy of any mixed quantum state is greater then zero. Therefore there are no quantum states with negative entropy. In this light, we can picture the classical description becoming more and more untenable as entropy decreases. That is, in the same way that we understand non-relativistic mechanics as the limit of relativistic mechanics when velocities are much smaller than $c$, we should understand classical mechanics as the limit of quantum mechanics for states whose entropy is much greater than $0$.

\section{System-environment boundary}

Let us now proceed by studying the system-environment boundary. It should be clear that the very definition of a physical system implies the definition of a boundary. Whether we are studying a solid object, an open container of gas, a chemical compound or a single particle, we must declare what constitutes part of the system and what does not. Moreover, the nature of the interaction between the system and the environment at the boundary is critical for the very definition of state for the system. Let us start with the following simple example.

Suppose we want to study the motion of a cannon ball. As the cannon ball moves, air molecules will scatter off its surface. Yet, the effect produced will be negligible. We can therefore describe the state of the cannon ball with a single, infinitely precise, pair of position and momentum. Now suppose we want to study the motion of a speck of dust. This time the effect of the air molecules will not be negligible. We will therefore describe the state of the speck of dust with a probability distribution over position and momentum. Now suppose we want to study the motion of the cannon ball on the surface of the sun. Plasma will scatter off the ball catastrophically, making the notion of the state of ball meaningless.

The definition of state is contingent upon the system-boundary interaction.\footnote{We use ``system'' and ``environment'' as it mostly evoke the right intuition. However, the same considerations apply to the internal dynamics that is below the sensitivity of our instruments. For example, a proton is described as a single quantum system precisely at those energies for which the internal quark-gluon dynamics can be disregarded. More precisely, ``system'' includes only those properties that are under study given the chosen processes.} If we says that we are describing a cannonball with infinitely precise position and velocity, we are implicitly stating that the we are not at 5000 Kelvin on the surface of the sun. The most precise definition for the state of a system, then, is the one that can be given in the best conditions: when the system-boundary interaction causes the least disruption.

If the state of a system is to be described by infinitely precise quantities, this means that there exists at least one setting for which the system is completely isolated. But complete isolation does not make physical sense. In practice, this is not something that can be realized. But even theoretically possible it is not possible: any wall we will use as shielding will necessarily have finite temperature and will emit thermal radiation; gravitational interactions cannot be shielded in the first place.\footnote{CITE: something that goes more into the details as to why gravitational interactions cannot be shielded?} Moreover, even conceptually complete isolation is problematic. It would mean that no signal, electromagnetic, gravitational or otherwise, would be able to interact with the object. It would be effectively experimentally inaccessible, physically ill defined. This means that there must be a minimal unavoidable interaction between the system and the environment.

If the uncertainty in our measurements or if other sources of noise are great compared with this minimal interaction, then the latter can be disregarded and the state of the system can be modeled as an infinitely precise quantity. The bigger uncertainty ``hides'' the complication of the minimal interaction. But if this minimal source is the dominant part, we cannot model it as some uncertainty over some infinitely precise value: it does not work conceptually. It would mean that, in line of principle, that minimal interaction could be removed, which, by definition, it cannot.\footnote{As an analogy, to the naked eye a star looks like a point in the sky and can be assumed to be as such. Even with several order of magnification, the assumption will hold. However, at some point one reaches a magnification such that the star becomes bigger and the assumption break down. At that point, you can't describe the star as just a single point: it's a more complicated object. Counter-intuitively, the infinitely precise value is the approximation.}

Note that this maps perfectly with the previous entropic argument. The zero entropy limit corresponds to this minimal interaction. Classical mechanics works when the leading source of uncertainty is much greater than the minimal interaction, when the entropy of the states is much greater than zero. As we remove sources of noise and uncertainty, the minimal interaction becomes dominant, and classical mechanics fails.

The problem with classical mechanics is not so much that it assumes perfect isolation, but that it assumes it at all level of scale. With continuous materials, we can take smaller and smaller parts. Classical fields allow for wave packets of ever decreasing energy. The theory implicitly assumes all these objects can be prepared, manipulated and detected independently enough that their state can be perfectly defined. This is the inherent problem with classical mechanics: perfect isolation itself is a simplifying assumption and cannot be taken to always hold. As we reach smaller and smaller scales, study smaller and smaller objects the assumption becomes less and less tenable.

\section{Discussion}

We would like to stress that the arguments we have provided are straight forward physical considerations. There is no mystery. They do not require infinite parallel worlds, special roles of observers, subjective information or many of the other esoteric elements present in various interpretations of quantum mechanics. Just simple considerations about thermodynamics and system-environment interaction. 

Under this light, there is a natural motivation that tells us why quantum mechanics is a statistical theory, and it is so at a fundamental level. What is ``quantized'' is the entropy due to an unavoidable background.\footnote{As discussed in a previous footnote, this may also include decoupling from the internal dynamics of the system.} Identifying a physical system means identifying that experimentally accessible description, that information, that decouples well from everything else, such that we can provide an independent description and rules of evolution. And that description is contingent to a particular system-environment interaction.

An interaction with the system, then, may not only affect the state of the system, but the system-environment interaction. This is, again, nothing unexpected, as these are the type of situations thermodynamics and statistical mechanics describe. In fact, suppose we have an open flask of gas in equilibrium. This would be described by a grand canonical ensemble, identified by its temperature $T$, volume $V$ and chemical potential $\mu$. The number of particles $N$ of the gas will not be well-defined as it fluctuates, and it is described by a probability distribution. If we want a state in which $N$ is well-defined, we can simply close the flask and let the system equilibrate. We will obtain a canonical ensemble determined by the same $T$ and $V$ and by the particle number $N$, and the chemical potential $\mu$ is undefined. The number of particles will depend on the fluctuation at the time we closed the flask, therefore we will have a certain probability to have a particular $N$. In short, we start with a state where $\mu$ is well-defined but not $N$, we interact with the system and end up in a state where $N$ is well defined and its value determined by a probability distribution. Note the parallel with quantum mechanics: we start with a state where an observable, say the energy, is well defined and we end in a state where another observable, say position, is well defined with a value determined by a probability distribution.

The development of a full fledged understanding of quantum mechanics goes beyond the scope of this article. We do want to show, however, that such a straight forward picture, based on tried and true old physical ideas, is possible. We feel that the overall physics community is ``stuck'' thinking that quantum mechanics is fundamentally mysterious, and therefore a new set of concepts are needed. This means that we will never find a down-to-earth, intuitive and obvious explanation. Because we can only find what we are looking for. We choose to believe such an explanation exists, and we just have to clean it up. We do that because, if we don't, we will never find one. We hope that, by showing that the failure of classical mechanics is, in retrospect, reasonable, we can find a ``reasonable interpretation'' of quantum mechanics.

\section{Conclusion}

We have shown that classical mechanics lies on the implicit assumption of perfect isolation at all scale: every fragment of matter, every minuscule wave packet, can be produced, manipulated and measured with infinitesimal precision. This is not only untenable, but it is also in conflict with thermodynamics. The existence of a minimal, unavoidable, interaction with the environment, with the uncertainty associated with it, is ultimately what makes the classical ideas fail.

This is a definitional issue, as the very definition of a system impinges on finding some experimentally accessible properties that decouples from everything else. The insight that nothing can be fully decoupled, then, has to be included at a fundamental level in the definition of a system, and this is what quantum mechanics, albeit in a still unclear way, must be doing. We believe a full understanding of these definitional requirements and constitutive assumptions are needed to give a more sound foundation to all current physical theories, and to open the way for future ones.



\bibliography{bibliography}


\end{document}