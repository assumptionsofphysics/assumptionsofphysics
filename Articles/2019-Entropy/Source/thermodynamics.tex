\documentclass[letterpaper,twocolumn]{article}

\input{../../../include/basicstyle}
\input{../../../include/theorems}
\input{../../../include/logic}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	citecolor=blue,
	urlcolor=blue,
	linkcolor=blue
}
\urlstyle{same}
\usepackage[font=scriptsize]{caption}

\begin{document}

\title{Counting evolutions: a simple foundation for thermodynamics}
\author{Gabriele Carcassi, Christine A. Aidala, et al \\ University of Michigan}

\date{\today}

\maketitle

\begin{abstract}
	In this work we present the core ideas at the heart of a new approach to the foundations of thermodynamics. The premise is to focus on the count of evolutions: the possible ways the state of a system can evolve in time. We see that in a deterministic process the evolutions can never diverge and in a reversible one they can never merge. This means that the count of evolutions per state can never decrease under a deterministic process and it is maximized at equilibrium. We show how this simple idea can be used as a straightforward foundation of thermodyanmics and how it relates to other notions of entropy in physics.
	
	This work is part of Assumptions of Physics, a project that aims to identify a handful of physical principles from which the basic laws can be rigorously derived  (\url{https://assumptionsofphysics.org}).
\end{abstract}


\section{Introduction}

This draft summarizes the status of our work on a new approach to the foundations of thermodynamics and is intended to gather early feedback. Rather then derive thermodynamics bottom-up from a particular microscopic model, we want to derive it top-down from a set of requirements on the dynamics of the system. The results we find are, then, independent on the microscopic dynamics in the sense that they can in principle be realized in different ways. This is both more in line with the aims of the founders of thermodynamics and justifies why similar concepts have been successful in fields beyond physics. Moreover, thermodynamics is found as a specialization of a more general model, which we believe can be used as a common foundation for the different types mechanics.

In a nutshell, we characterize processes in general terms, as the description of all possible ways that the system can evolve in time, its possible evolutions, under a particular set of circumstances, as one can see in figures \ref{fig_single_evolution} through \ref{fig_with_equilibria}. Counting the possible evolutions plays a fundamental role, in much the same way that counting states plays a role in statistical mechanics. In a deterministic and reversible process, all evolutions that start in one state will end in the same state: the evolutions cannot merge or diverge. If the process is deterministic but not reversible, however, the same final state can be reached from different initial states, and the evolution will merge. If the deterministic process reaches an equilibrium, there will be no more merging. We define process entropy to be the logarithm of the count of evolutions per state, which we find to have the basic properties we associate with entropy: it is linear under system composition of independent systems, can never decrease under a deterministic process, increases if the process is non-reversible and is maximized at equilibrium. This gives us a conceptually crisp physically meaningful concept that is tied to irreversibility and equilibria in a very natural way.

There are many technical details that are required to make the above characterization formally rigorous, mostly due to the proper handling of infinities. However, those do not really change the overall intuition, which we believe is the main strength of the approach. Therefore, in this paper, we will only give a conceptual overview leaving the formal details to other. This will leave the key physical insight is more directly accessible, and also result in a narrative that can be used directly in an introductory course in thermodynamics.

\section{States, processes and evolutions}

The first question we want to answer is: how can we characterize processes in the most general terms? We assume we have a system and we want to study its behavior in a given set of circumstances. If the state space $X$ of the system represents all possible configurations the system can be found in, ultimately we want to characterize how the state changes in time. As depicted in figure \ref{fig_single_evolution}, we have a copy of the state space at each moment in time, and we call an \textbf{evolution} a trajectory $x(t)$ that for each moment in time returns the state of the system. 

\begin{figure}[h]
	\includegraphics[width=\columnwidth]{images/Slide1.png}
	\caption{The three sets represent the state space of the same system at three different moments in time. The blue line is an evolution, which tells us the state of the system at all times.}\label{fig_single_evolution}
\end{figure}

Not all evolutions will be possible under all conditions. Therefore we choose to characterize a \textbf{process} simply as the set of allowed evolutions, as we can see in figure \ref{fig_process}. Namely, a process tells us what can and cannot happen in a given set of circumstances. We also assume we have a measure $\mu$ that allows us to determine the size of a set of evolutions, to ``count'' evolutions.\footnote{The precise conceptual and mathematical definition for $\mu$ is source of a number of technical considerations which we (will) address in a separate work. While some issues, like unit definition, are important to the physics, most have to do with the proper handling of infinities or the conditions for existence and uniqueness of the measure. Overall, they would distract from the key insights.} A state $x_t$ at a time $t$ identifies a set of of compatible evolutions $A(x_t)$, those that pass through that state at that time. That is, $x(t) = x_t$. Using $\mu$ we can give a size to that set, which we can write as $\mu(A(x_t))$ or $\mu(x_t)$ for short.

\begin{figure}[h]
	\includegraphics[width=\columnwidth]{images/Slide2.png}
	\caption{A process is characterized by the set of all possible evolutions. The measure $\mu$ allows us to express the size of a set of evolutions. The probability of transitioning from one state to another corresponds to the fraction of evolutions that starts in the first and also ends in the second.}\label{fig_process}
\end{figure}

Note that the probability of transitioning from one state to another during a process is linked to the allowed evolutions and their count. In fact, calculating the probability of finding the state $x_{t+\Delta t}$ at $t+\Delta t$ given $x_t$ at $t$ means determining the fraction of evolutions that pass through $x_t$ that also pass through $x_{t+\Delta t}$. That is:
\begin{equation}
	P(x_{t+\Delta t} | x_t) = \frac{\mu(A(x_t) \cap A(x_{t+\Delta t}))}{\mu(A(x_t))}.
\end{equation}
Conceptually, the ability to count evolutions is equivalent to knowing all possible probability of transition.\footnote{Technically, a single probability measure is not enough to define all these transition at all scales, but the overall intuition remains.}

Having answered the question of how we characterize processes, we can categorize them. For example, we say a process is \textbf{deterministic} if the state at one time in enough to determine the state at all future times. We can see an example of such a process in figure \ref{fig_determinism}. In this case, if two different evolutions happen to reach the same state at a given time, they will also share all future states: evolutions can merge but cannot diverge. The count of evolutions per state over time can never decrease: $
\mu(x(t)) \leq \mu(x(t + \Delta t))$.

Deterministic processes can be characterized by a law of evolution. That is, we can write $x(t+\Delta t) = f(x(t), t)$ given a suitable function $f$ which may, in general, be time dependent. We can do this because $P(x(t+\Delta t) | x(t)) = 0$ for all but one future state. The law of evolution identifies that state. That is, $P(x(t+\Delta t) | x(t)) = 1$ if and only if $x(t+\Delta t) = f(x(t), t)$.

\begin{figure}[h!]
	\includegraphics[width=\columnwidth]{images/Slide3.png}
	\caption{Deterministic process: the state one time determines the state at future times. Evolutions can merge but not diverge in time. The number of evolutions per state cannot decrease.}\label{fig_determinism}
\end{figure}

Conversely, we say a process is \textbf{reversible} if the state at one time is enough to reconstruct the state at all past times. We can see an example of such a process in figure \ref{fig_reversibility}. In this case, if two different evolutions happen to reach the same state at a given time, they will also share all past states: evolutions can diverge but cannot merge. The count of evolutions per state over time can never increase: $
\mu(x(t)) \geq \mu(x(t + \Delta t))$.

\begin{figure}[h!]
	\includegraphics[width=\columnwidth]{images/Slide4.png}
	\caption{Reversible process: the state one time allows one to reconstruct the state at past times. Evolutions can diverge but not merge in time. The number of evolutions per state cannot increase.}\label{fig_reversibility}
\end{figure}

Note that in some literature the ability to reconstruct the past is called retrodictability and does not coincide with thermodynamic reversibility (the ability to undo a change). This distinction will be made precise and clear in the full framework. At this point we want to make it clear that our notion of reversibility will indeed correctly recover thermodynamic reversibility. It will do so without having to assume the existence of a reverse process or a symmetry under time-reversal.

A process that is both deterministic and reversible, like the one in figure \ref{fig_detrev}, will never contain evolutions that split or merge. The count of evolutions per state over time will be a constant: $\mu(x(t)) = \mu(x(t + \Delta t))$.

\begin{figure}[h!]
	\includegraphics[width=\columnwidth]{images/Slide5.png}
	\caption{Determinism and reversibility. Evolutions cannot merge nor split. The number of evolutions per state cannot increase.}\label{fig_detrev}
\end{figure}


To finish, we focus on deterministic processes with equilibria. We can see an example of such a process in figure \ref{fig_with_equilibria}. An equilibrium for a process is a state that, once it is reached, it is never left. In a process with equilibria all evolutions will reach, at some point, an equilibrium. In the out-of-equilibrium phase, evolutions merge until they reach equilibria and can no longer merge. This means that our measure $\mu$ is maximized at equilibria. If we care to characterize only the initial and final state, deterministic evolution simply means that the final equilibrium is a function of the initial state.

Note that the idea of equilibrium as ``nothing changing'' used here is an oversimplification, which does not even work relativistically. Yet, it suffices for now as it captures the intuition correctly.

\begin{figure}[h]
	\includegraphics[width=\columnwidth]{images/Slide6.png}
	\caption{Deterministic process with equilibria. Evolutions keep merging until equilibrium is reached. The number of evolutions per state increase and the maximum is reached at equilibria.}\label{fig_with_equilibria}
\end{figure}

Note that we have found a quantity, $\mu(x(t))$, that can never decrease in a deterministic process, stays the same if the process is reversible and is maximized at equilibrium. We have done so with no assumption on the type of system, the type of forces acting on the system, the internal composition of the system or the internal dynamics of the system. It knows nothing of uncertainty, disorder, information, statistical or probability distribution, phase space volumes. It only knows about evolutions and how to count them. The result is completely general and the justification is very straight forward. It is conceptually a perfect candidate for the thermodynamic entropy, expect for one thing we still need to understand: how it combines under system composition.

Suppose we now have two systems, with their respective state space $X$ and $Y$. The state space of the composite system is the set of all possible pairs $X \times Y$. A process defined on the composite system will come with a measure $\mu_{X \times Y}$ that enables us to count evolutions $\{x(t), y(t)\}$ of the composite system. In general, the state of one system may affect the other system, in which case the evolution of the composite cannot be separated. If the systems are independent, however, the evolution of one system will not influence the other. That is, for each possible evolution $x(t)$ of the first system and $y(t)$ of the second system, there will be a possible evolution $\{x(t), y(t)\}$ on the composite. In other words,
\begin{equation}
	\mu_{X \times Y} = \mu_X \mu_Y
\end{equation}
the measure factorizes. We define the \textbf{process entropy} as $S = \log(\mu)$. Since the logarithm is a monotonic function, the process entropy will preserve monotonicity and maximization of $\mu$ under deterministic process with equilibria. Which means:
\begin{equation}\label{ov_entropy_increases}
S(x(t)) \leq S(x(t + \Delta t))
\end{equation}
In addition, the process entropy will be additive for independent systems. As before there are a number of technical details that need to be addressed, most of all the units for $\mu$ which will influence the 0 point for $S$, which we will address in the full framework.

In general, the process entropy will depend on the process, not just on the state. However, a unique value for process entropy can be assigned to states of equilibrium. Suppose, in fact, that the state $x$ is an equilibrium for two different processes $A$ and $B$. Suppose that $A$ is at equilibrium $x$ and we switch to process $B$. Since $x$ is also an equilibrium for $B$, every evolution of $A$ that ended in $x$ will correspond to an evolution in $B$ that ends in $x$. By symmetry, the converse is also true. Therefore the process entropy of an equilibrium does not depend on the process, only on the equilibrium state. This allows us to assign a unique process entropy to the state. If we have set of states that are equilibria, then, we call \textbf{state entropy} $S(x)$ the process entropy that the system reaches at equilibrium.

To summarize, we found, under most general conditions, a quantity called process entropy, defined as the logarithm of the number of evolutions, that cannot decrease under deterministic processes, cannot increase under reverersible processes, is maximized at equilibrium and is additive for independent systems. Moreover, if $X$ consists of equilibria, there exists an \textbf{equation of state} $S : X \to \mathbb{R}$ which returns the state entropy, which is the process entropy of the equilibrium.

\section{Thermodynamics}

We now have all the basic tools we need to reconstruct thermodynamics, which we characterize as the study of equilibria of composite systems. This is not done in general, but under additional assumptions and definitions that tell us how the equilibrum of the whole relates to the equilibria of the parts. As such, the laws we find are not necessarily universal in nature: they apply to systems that satisfy (or can be assumed to approximately satisfy) the prerequisites.

We say a system is in \textbf{thermodynamic equilibrium} if:
\begin{itemize}
	\item it is at equilibrium
	\item all its parts are in thermodynamic equilibrium
	\item if another system joined with a part of the system in thermodynamic equilibrium is in thermodynamic equilibrium, then the whole is in thermodynamic equilibrium.
\end{itemize}
We say two or more systems are in thermodynamic equilibrium if their composite is in thermodynamic equilibrium. A \textbf{thermodynamic process} is a process with equilibria where the final state is a thermodynamic equilibrium. We again stress that we are not saying that two systems, in general, always allow for thermodynamic equilibria.

To get a feel for the definition, let us rederive the fact that thermodynamic equilibrium is transitive, also known as zeroth law. Suppose system $A$ is in thermodynamic equilibrium with system $B$ and system $B$ is in thermodynamic equilibrium with system $C$. Then the composite $B \times C$ is in thermodynamic equilibrium by definition. Since $A$ is in thermodynamic equilibrium with $B$, a part of $B \times C$, it is also in thermodynamic equilibrium with $B \times C$. Therefore $A \times B \times C$ is in thermodynamic equilibrium. But $A \times C$ is a part of $A \times B \times C$, so it is in thermodynamic equilibrium. Which means $A$ is in thermodynamic equilibrium with $C$.

The characterization of thermodynamic equilibrium we give is manifestly a part-whole relationship. It extends the notion of equilibrium to a composite system so that the equilibrium of the whole and the equilibrium of its parts are essentially the same equilibrium. We find that this kind of part-whole relationships are a good starting point not only for thermodynamics, but also for the other branches of physics. This should not come as a surprise as reductionism (i.e. explaining the whole through its parts) plays an important role in science.

We define a \textbf{thermodynamic system} one whose equilibria can be identified by a set of extensive quantities (i.e. additive under system composition). Moreover, one such quantity is called internal energy which has the property of remaining conserved under any process. We will denote $U$ as the internal energy and $x^i$ as the remaining extensive quantities. For a thermodynamic system, then, the state entropy can be expressed as $S(U, x^i)$ a function of the extensive quantities. This is simply the equation of state in entropic form, which is the starting point of Gibbs formulation of thermodynamics. [CITE] We can therefore define the relationships
\begin{align}
	\beta &= \frac{\partial S}{\partial U} = \frac{1}{k_B T} \;\;\;\;\;\; - \beta X_i = \frac{\partial S}{\partial x^i}
\end{align}
where $k_B$ is Boltzmann's constant. We find:
\begin{align}
dS &= \frac{\partial S}{\partial U} dU + \frac{\partial S}{\partial x^i} dx^i \\
&= \beta dU - \beta X_i dx^i \\
dU &= k_B T dS + X_i dx^i
\end{align}
which are the usual thermodynamic relationships, except that, for us, the state entropy $S$ dimensionless..

Note, again, that the system is identified by quantities that have a clear rule under system composition. We suspect that the existence of extensive quantities can be derived from the composability of thermodynamic equilibria. In fact, we typically construct measurement scales of quantities (such as energy, mass, volume, distance, ...) by preparing unit samples and then adding or dividing them. So it may be possible that, once we assume the ability to combine systems into bigger systems, we will consequently be able to construct extensive quantities without extra assumptions.\footnote{XXX: any work related to this?} Nevertheless, we leave these considerations for later works.

%Under these assumptions/definitions, we can recover the laws of thermodynamics. Let $A$ and $B$ be two thermodynamic system in thermodynamic equilibrium and let $AB$ be the composite. We have:
%\begin{equation}
%\begin{aligned}
%U_{AB} &= U_A + U_B \\
%S_{AB}(U_{AB}) &= S_A(U_A) + S_B(U_B) \\
%&= \sup \{S_A(x) + S_B(y) \, | \, x+y= U_{AB}\} \\
%\frac{1}{kT_A} = \beta_A = \frac{\partial S_A}{\partial U_A} &= \frac{\partial S_B}{\partial U_B} = \beta_B = \frac{1}{kT_B}
%\end{aligned}
%\end{equation}
%which follows the standard textbooks derivations and assumes the new equilibrium is not on a boundary point. In this case, equilibrium is achieved if and only if $T_A$ and $T_B$ are equal. Since equilibria are unique, this will also tell us that all equations of states have to be concave in $U$. If $B$ is in thermal equilibrium with another system $C$ we also have $T_B$ is equal to $T_C$ and therefore $A$ and $C$ are also in thermal equilibrium. This recovers the zeroth law.
%
%There is, however, a problem. We assumed the equilibrium is not a boundary point. It could be that system $A$ reaches its lowest energy state at a finite $T_A$ while $B$ can go to lower temperatures. This would mean that $B$

To derive the first and the second law we need to give a definition for work and heat. We do so by defining two special thermodynamic systems. We say a thermodynamic system $R$ is a \textbf{thermal reservoir} if its equation of state can be sufficiently approximated by $S = \beta U = \frac{U}{k_B T}$. Internal energy is the only state variable and the only way the entropy of the system can change. We call \textbf{heat} the energy exchanged by a thermal reservoir and we note $Q = - \Delta U$ as the energy lost by the reservoir during a thermodynamic process.

We say $M$ is a mechanical system if the state entropy is a constant for all states. For a thermodynamic system that is also mechanical, we therefore have  $S(U, x^i) = k_M$ for all its states. A mechanical system cannot change entropy and all states can be connected through a deterministic and reversible process. We have:
\begin{equation}
\begin{aligned}
dU &= k_B T dS + X_i dx^i = X_i dx^i.
\end{aligned}
\end{equation}
The energy of the mechanical system is effectively \emph{stored} in one of the other state variables and can be later retrieved. We call \textbf{work} the energy exchanged by a mechanical system and we note $W=\Delta U$ as the energy gained by the system during a thermodynamic process.

These two types of systems allow us to study the two ``orthogonal'' directions a change of energy can have: an energy change in the direction of constant entropy and one in the direction of maximal energy change. These definitions makes this manifestly clear. Also note that $\frac{\partial S_M}{\partial U} = 0 = \beta_M = \frac{1}{k_B T_M}$ which means the thermodynamic ``temperature'' of a mechanical system is infinite. While this ``temperature'' has little to do with thermometers, the fact that we can always transform all work into heat but not vice-versa is just a specific case of being able to move energy from higher to lower temperature.

We now consider a system composed of a generic system $A$, a thermal reservoir $R$ and a mechanical system $M$. During a thermodynamic process, because the total energy of the system must be conserved, we have:
\begin{equation}
\begin{aligned}
&\Delta U_A + \Delta U_R + \Delta U_M = 0 \\
&\Delta U_A - Q + W = 0 \\
&\Delta U_A = Q - W.
\end{aligned}
\end{equation}
This recovers the first law.

We now look at the change in entropy, which during a thermodynamic process can never decrease. Keeping in mind $M$ is mechanical, we have:
\begin{equation}
\begin{aligned}
&\Delta S_A + \Delta S_R + \Delta S_M \geq 0 \\
&\Delta S_A \geq - \Delta S_R \\
&\Delta S_A \geq \int \beta_R dQ \\
&\Delta S_A \geq \int \frac{dQ}{k_B T_R}.
\end{aligned}
\end{equation}
which recovers the second law of thermodynamics. This is just a more specialized version of \eqref{ov_entropy_increases} which is valid for any deterministic process.

The third law has many equivalent formulations. [CITE] One of them states that the entropy of any system has a non-negative lower bound which is reached by the lowest energy state. If the ground state is non-degenerate, such as a perfect crystal, its entropy is equal to zero. We can easily recover it in this form.

As state entropy counts the possible evolutions of the system at equilibrium, the lowest possible count $\mu$ is one, which is achieved if the equilibrium can be reached only by one starting condition. In this case, $S=\log(\mu)=0$. This tells us that state entropy is non-negative and therefore is bounded from below. We now have to show that all thermodynamic systems must have a lowest energy state and that it must be a local minima for entropy.

First of all, the existence of unique thermodynamic equilibria forces $S(U)$ to be strictly concave (i.e. negative second derivative). This means that, at least in some region, $S(U)$ must either be a decreasing or increasing function. But since $S$ cannot be negative, $S(U)$ cannot be defined on the whole domain. Therefore $U$ must be bounded at least from above or below. Consequently, the minimum for entropy is reached either at the lowest or highest energy state. Now we show that all thermodynamic systems must have either a lowest energy state or a highest energy state. Suppose, in fact, we had two system, $A$ has no maximum energy while $B$ has no minimum energy. Since $S_A(U_A)$ is concave, it must be a monotonically increasing function of $U_A$. This means that $\beta_A$ is always positive. Conversely, $S_B(U_B)$ must be monotonically decreasing, which means $\beta_B$ is always negative. If we put the two system in contact, the energy of $B$ will keep decreasing and the energy of $A$ will keep increasing and no equilibrium will ever be reached. Therefore all thermodynamic systems either have a minimum or a maximum energy. At this point the difference is a sign convention, so we can assume all thermodynamic systems have a minimum energy.

To finish, we note that a perfect crystalline structure fits the case where the system has only one possible evolution, so it is a zero entropy state. If there is degeneracy, there are different ways the equilibrium can be realized, there are different evolutions that will correspond to the same equilibrium which means the entropy will be greater than zero. This recovers the third law.

The laws of thermodynamics have been recovered, and we have done so only by characterizing how equilibria and state variables behave under system combination. Apart from that, we have made no assumptions on what type of system, the type of forces acting on the system, the internal composition of the system or the internal dynamics of the system. No reference to statistical mechanics or quantum mechanics is needed: the theory stands on its own. This fits the original spirit of the thermodynamics, which was to find a set of laws that would be independent on the ultimate dynamics of the constituents. Additionally, the concepts we used are directly related to the physical explanation, which is not recovered after pages and pages of calculation. The combination of these features makes this approach directly useful, for example, in an introductory physics course: a coherent self-contained explanation can be given in terms of simple definitions.

% \subsubsection{XXX Null states and thermodynamic zero}

% * Use null state as thermodynamic zero.
% * Must have zero entropy.
% * All extensive quantities for the null state must be zero.
% * Extensive quantities can be zero for a propert state (i.e. number of particle of a particular species).
% * Intensive quantities are derivative: they also have an absolute zero.
% * All extensive quantities must be non-negative?

% Sample text

%Zero entropy is usually associated to a non-degenerate ground state and the lower bound on energy is usually thought as a consequence of quantum mechanics. We saw how, in our framework, these requirements stem from the definitions. In fact, we can repeat the argument and find that \emph{all} extensive quantities. We also note that, while one typically focuses on temperature and entropy having an absolute scale, other thermodynamics variable, such as volume and pressure, have a well defined zero. Is there a better way to frame these issues? We believe there is.

%System composition $\times$ is a closed operation (i.e. given any two systems returns another system) that is commutative (i.e. $A \times B = B \times A$) and associative (i.e. $A \times (B \times C) = (A \times B) \times C$). Mathematically it makes sense to ask whether it has an identity (i.e. $A \times 0 = 0 \times A = A$), which we note as $0$ since the operation is commutative (i.e. additive). In fact, while the state space of the composite is the product of the subsystems, system composition in terms of systems is better thought of as an addition: we are putting things together. The identity, then, is naturally the ``null'' system: the empty system. We can keep adding nothing, and the system is unchanged. This makes system composition a commutative monoid, but not a group since there is no ``inverse'' system: the one that combined with the original gives us nothing.\footnote{Note that particles/anti-particle annihilation leaves something: the energy. So the ``anti''-system is not the ``inverse'' system.}

%The null system has only one possible configuration, which we call the null state, so it can only have one possible evolution. The null state of a null system is inherently in equilibrium and the state entropy is zero. In fact, the values for all extensive quantities must be zero since composing the null system with another null system still gives us a null system. Therefore, for example, $U_0 + U_0 = U_0$.

% The intensive quantities are defined as derivatives, which means their zero is also absolute: is the case where changing the extensive quantity leads to no change in entropy. This means that, in thermodynamic, all quantities must be absolute, not only temperature or entropy is absolute, but this tells us that all thermodynamic quantities must be absolute. Volume, pressure, number of particles, we already think of them as having a well defined zero.

%Some confusion comes from energy. In mechanics energy in the form of the Hamiltonian does not have an absolute zero, an arbitrary constant can be added with no ill effect. But rescaling internal energy by a constant $\hat{U}$ would make it no longer an extensive quantities: $(U_A + \hat{U}) + (U_B + \hat{U}) = U_{AB} + 2 \hat{U} \neq U_{AB} + \hat{U}$.\footnote{XXX: someone must have already said this} Now the question is whether

\section{Discussion}

We have shown that thermodynamics can be conceptually rederived by adding few other assumptions/definitions from a general characterization of states, processes and equilibria. Can this approach also offer new insights on classical mechanics, quantum mechanics and statistical mechanics that may lead to a more unified understanding these somewhat disconnected disciplines?

The notion of equilibria would be a place to start. We defined, as it is customary, the state space of a thermodynamic system as consisting only of states of equilibria. We also defined a mechanical system as one for which $S$ is the same for all states. But we said the state entropy is uniquely defined only for state of equilibria. This implicitly assumed that all states of a mechanical systems are equilibria. Is this always true? And in what sense is this true?

Let us first consider the classical phase space of a particle. Qualitatively, we can imagine saying that a particle at rest is in equilibrium. A particle in uniform motion, though, does not seem in equilibrium: the distance is constantly increasing. Yet, there exists a frame in which it is at rest. So, in this a sense, all free particles are at equilibrium. Going one step further, in the presence of gravity, a particle at rest in one frame will not be in uniform motion in another. So it would seem that a particle under gravitational or apparent forces is at equilibrium. What is going on?  The issue here is that, as we noted before, the notion of equilibrium as ``nothing is changing'', which is the notion coming from dynamical systems, is not exactly aligned with the notion of equilibria we need for thermodynamics.\footnote{XXX: cite papers that note the difficulty of definiting temperature relativistically} So we need to answer two questions: what would be a more suitable definition? And why would that definition be tied to the notion of state?

In another work, we more thoroughly investigate characterizing equilibria in terms of system decoupling, which we briefly present here. By system decoupling we mean a process that makes the system independent from other systems, its evolution does not constrain and is not constrained by the evolution of other systems. As we saw before, independence was already critical for defining state entropy. But why should it be connected to states? We typically think of a state as the description of one system and, implicitly, only that system. If the system is not independent, if there are correlations, the description of that system tells us something about the other as well. So if we want a state to give the description of one system alone, we implicitly describing it in the case where the system is independent. System independence is not just a requirement for the process entropy to factorize: it's an implicit requirement to properly talk about a system and its state. But why would independence be tied to a notion of equilibria? The idea is that when we group together quantities into a system, this is not done arbitrarily. For example, we do not group together the position of one ball with the velocity of another. We group quantities that ``go together'', that have to be studied together. More precisely, the ability to define a system impinges upon the ability to prepare it and study it independently from the rest. If we are not able to then what we have is not really a system, just a set of random quantities. So, at least in line of principle, we must have a way to decouple the system from other systems. We must have a process that makes the system independent. Clearly, if the system is already independent, the process should do nothing. If states are descriptions of an independent system, they must be equilibria of these decoupling processes. In this sense, we can say that all states, thermodynamic equilibria or not, are equilibria. Thermodynamic equilibria extends this notion of independence at different level of scale.

Still, one may find strange associating a state entropy to mechanical system. And why would that be constant on all states? We should note that the idea that all states have the same entropy is already embedded in statistical mechanics. For quantum mechanics, in fact, the Von Neumann entropy of every pure state is zero. In classical mechanics, a uniform measure is used to calculate the accessible states; additionally the Gibbs entropy is invariant under translations in phase space. It is also embedded directly in the corresponding mechanics. In both theories, given to states, we can always (at least mathematically) find a deterministic and reversible evolution (i.e. a Hamiltonian evolution or a unitary evolution) that connects the two. So the idea that mechanical systems are associated with a constant entropy, though seldom explicit, can be already deduced from the currently established theories.

Another natural question is: how does the process entropy relates to the fundamental postulate of statistical mechanics, that entropy is the logarithm of the count of states?
Suppose we give the description of a system at a given time (e.g. the energy of the system is within a particular bound). This will identify a set of possible evolutions and therefore a value for the process entropy. Also, this is the sum (or integral) of the process entropy associated with each state not ruled out by that description: every evolution will have to go through one of those states at that time. Now suppose the system is, at that time, independent from the other. Then the process entropy for each state is the state entropy, so the process entropy for that description is the sum (or integral) of the state entropy. Additionally, suppose the system is mechanical, then the process entropy for the description is, up to a constant, the count of states. Entropy as the count of microstates is easily recovered.

Note that this can happen in different scenarios. We could have an independent mechanical system under a deterministic and reversible process. Then counting evolution is the same as counting states, up to a constant, at any moment in time. In another scenario, the process is not deterministic and reversible but is isoentropic, meaning that the process entropy is constant in time. If we have an independent mechanical system, its evolution will necessarily be isoentropic since, whatever state we evolve to, the state entropy will always be the same. Now suppose the set of states compatible with our description mix with each other and only with each other. Each evolution will change state in time, but it will never go outside of the set, so the description works both as a constraint and a description of the dynamic equilibrium. At each time, the same description will correspond to the same states, so the process entropy will be constantly given by the count of states.

Then one may ask: how are these two entropies different? Imagine that the mechanical system starts independent but the dynamics is non-reversible leading the dynamical system to an equilibrium, as in a damped harmonic oscillator. The process entropy at the initial state will coincide with the count of states, but not at later times. As the evolution concentrate around the equilibrium, the number of states become fewer so the number of evolutions per state increases. The entropy of the equilibrium, assuming that it is ever reached, is essentially the size of the basin of attraction at the initial time. The entropy around the equilibrium, then, increases in time which is consistent with the process being irreversible. Conversely, note that the areas in phase space shrink as the available states become fewer and fewer, which would mean entropy as the log of the state count decreases under an irreversible process. It was actually this paradox that pushed into considering evolutions as more fundamental than states and, retrospectively, it is the obvious choice.

This example also show how critical is the notion of independence to define state entropy. The evolution of a damped harmonic oscillator is not independent from the evolution of the systems that are absorbing the dissipated energy. The same description at different times (e.g. the system has reached equilibrium) does not provide exactly the same information on the system or other systems. In these cases, the state entropy does not allow us to compare apples to apples while the process entropy always does.

Having seen how the process entropy relates to the count of states, one may naturally ask: how does it compare with the Gibbs/Shannon entropy? Suppose you cannot distinguish the dynamics of a system under a given time resolution. That is, in principle the system has a well defined trajectory $x(t)$ but you can only measure quantities of the form $A[x(t)] = \int_{t_0}^{t_0 + \Delta t} a(x) dt$ (e.g. average energy, average number of particles, ...). A permutation of states within the trajectory will not affect any of the measurement outcomes. Therefore the best we can do, given enough of those measurements, is to measure the frequency $\rho(x)$ each state is visited within the time resolution. Now we ask, given a specific distribution $\rho(x)$, how many possible evolutions $x(t)$ would that correspond to? That means counting the number of permutations $W[\rho]$ we can construct. The process entropy will be $\log W$. The Shannon/Gibbs entropy is the limit of that expression if we take $x(t)$ to be continuous.

The approach, then, is not simply a nicer retelling of thermodynamics, but it can help us better understand the fundamental connection between all these theories. We stress that we are not arguing that all other notions of entropy should be abandoned in favor of process entropy: quite the contrary. Specific tools are often more useful than general tools precisely because they take advantage of the specific nature of the problem. The point is that if we are more aware of the interplay between the general definition and the more specific one we will not use the latter inappropriately.

\section{Conclusion}

\bibliography{bibliography}


\end{document}