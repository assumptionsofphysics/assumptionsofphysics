\documentclass[letterpaper]{article}

\input{../../../include/basicstyle}
\input{../../../include/theorems}
\input{../../../include/logic}

\begin{document}

\title{TBD}
\author{Gabriele Carcassi, Christine A. Aidala \\ University of Michigan}

\date{\today}

\maketitle

\begin{abstract}
	TBD
\end{abstract}


\section{Introduction}

------------------
Cite other axiomatic works.

Single state space to capture:
* descriptions at all levels
* descriptions at different times
* correlations between descriptions at equal time (equations of state)
* correlations between descriptions at different times (evolution laws)
* different correlations at different description
* granularity/precision of such descriptions

We can't have:
* probability as a native concept
------------------

In the intro, describe only the discrete case as it simpler to understand and already contains all the physics. The continuous case adds the complication of 

Evolution is a temporal sequence of states. Process is a set of possible evolutions. What we want to study is how a statement about a state at a particular time selects a particular set of possible evolutions and therefore selects a set of possible states at a previous or past time.

Define determinism and reversibility in terms of what evolutions exist. Note that a dynamical system is a deterministic system.

Note how the number of states does not always match the number evolutions it is compatible with. Note that under deterministic evolution the number of compatible evolutions can only increase. For dynamical systems, then, the number of evolutions associated to a state can only increase in time and it is stationary for equilibria.

If we compose independent systems, the number of possible states and trajectories is the product. If we take the logarithm, then those numbers are additive. Conceptually the entropy is the logarithm of the number of evolutions compatible with a given state. This article shows how this simpler and more general concept leads, in special cases, to the Boltzman entropy and the Shannon entropy. We will also see how this same concepts is applicable more in general, including the case of non-Hamiltonian systems.

\section{Main points}
\begin{enumerate}
	\item Entropy assigns a size to points. Shannon entropy works on a distribution where each point has the same entropy.
	\item Measure theory differentiate between points and sets. This does not work when combining different level of granularity.
\end{enumerate}

\section{Main section}
\begin{enumerate}
	\item \emph{Experimentally testable descriptions and their logic relationship.} Here the goal is to formalize the basic structure required by a set of experimentally verifiable assertions. Some descriptions are more specific than others, some are compatible with each other, and so on. An evolution is the most specific description, one that tells us what happens at all times.
	\item \emph{Group descriptions by time.} Here the goal is to group description at ``instants of time'', which are taken to be small intervals. A snapshot is the most specific description at an instant, on that tells us everything there is to know about the system at the given time. Determinism and reversibility are defined at this level (i.e. deterministic process is one where the snapshot at one time tells us the snapshots at future times).
	\item \emph{Granularity of the description and counting evolutions.} Here the goal is to quantify how much a description is more refined than another. This can be done by quantifying the possible evolutions that are compatible with each (i.e. the more specific description will be compatible with fewer evolutions). The evolution entropy quantifies the difference in granularity in terms of how many questions (i.e. bits) are needed to go from one description to another. The evolution entropy cannot decrease for a deterministic process and is stationary if the process is also reversible.
	\item Define deterministic evolution: can predict future state. Past state narrower. Number of evolutions increases. Define reversible. Future state narrower. Number of evolution decreases. Det/rev remains the same. How do we quantify that?
	\item Define way to compare granularity. Not a single measure, not everything comparable or finitely comparable. Define probability in terms of measure.
	\item States of the same system must be finitely comparable. Can put a measure on states themselves, which is a count of evolutions. Examples to show that: the measure and state space, in general, change over time. State spaces are stable and can be assigned a measure only at equilibrium.
	\item Define independent systems as those where the state of one does not constrain the state of the other. The measure must be the product.
	
\end{enumerate}

\begin{enumerate}
	\item A thermodynamic system is one that undergoes deterministic evolution with equilibria. There are variables that  
	\item Extensive quantities are somewhat postulated at this point. So is the existence of energy.
	\item Second law come from deterministic system.  Define entropy as the log of the measure, so that it is additive under independent systems. We have: entropy always increases under deterministic evolution; entropy is maximal at equilibrium; entropy is additive for independent system. Entropy is a function of the equilibrium, which is identified by state variables.
	\item Third law comes from states of the same system being finitely comparable. Lowest entropy is "one evolution" in terms of system granularity.
	
\end{enumerate}


\section{Evolutions, states and levels of description}

The overall goal is to describe a generic system as it evolves in time by defining a minimal set of concepts that such a setup must define, thus bringing to light the tacit assumptions underlying state spaces and evolution laws. We want to find notions that are equally valid and useful within different settings (e.g. thermodynamics, dynamical systems, classical mechanics, ...). We provide here an overview of the main features and a firm idea of how they recover the more standard results, leaving a complete formal development to future works.

\subsection{Descriptions and logical relationships}

Our starting point is the \textbf{process domain} $\tdomain[P]$, which is the set that contains all possible descriptions of the system at all levels of detail at all possible times. Depending on the process, it will contain statements like \statement{the average volume of the system between 1 and 2 seconds is between 3 and 4 liters} or \statement{the trajectory of the particle is $y=10 \, m - t^2 \cdot 9.80665 \, m/s^2 $}. It does not matter at this point what the statements themselves are, except they must be, at least in line of principle, defined from experimentally well-defined starting points. The translation of these physical requirements into a formal structure has already been carried out in previous work, and we will briefly summarize them in this section. Mathematically, $\tdomain[P]$ is a $\sigma$-complete Boolean algebra\footnote{A set of statements that is closed under negation (NOT), countable disjunction (OR) and countable conjunction (AND).} and it is the closure of the set of experimentally verifiable statements $\edomain[P] \subseteq \tdomain[P]$.\footnote{The set $\edomain[P]$ will form a Heyting algebra which, to be experimentally reachable, must allow a countable basis. The closure $\tdomain[P]$ is in terms of negation and countable disjunction.}

As $\tdomain[P]$ contains different levels of description, some statements will be more or less specific. For example, we say that \statement{the horizontal position is between 2.5 and 3 meters} is \textbf{narrower} than \statement{the horizontal position is between 2 and 3.5 meters}. Note that whenever the first statement is true, the second one must be true as well. Given two statements $\stmt_1, \stmt_2 \in \tdomain[P]$, we formally capture this relationship by noting $\stmt_1 \narrower \stmt_2$ (read ``is narrower than''). Narrowness can describe relationships at different scales, on different quantities (e.g. \statement{the horizontal position is between 2.5 and 3 meters and the vertical position is between 1 and 1.5 meters} $\narrower$ \statement{the horizontal position is between 2.5 and 3 meters}), constraints between variables at the same time (e.g. \statement{the temperature of the water in the glass is 3.98 $^\circ C$} $\narrower$ \statement{the density of the water in the glass is 1 $g/cm^3$}) or at different times (e.g. \statement{at time 0 s the position is 1 m and the velocity is 1 m/s} $\narrower$ \statement{at time 1 s the position is 2 m}). It is a general tool to capture relationships within the theory which intuitively can also be thought of as implication.\footnote{Technically, implication in classical logic is a truth function as classical logic does not incorporate the idea of different ``possible cases''. What we have is more similar in spirit to semantic consequence in modal logic, though modal logic brings in a lot of undesirable elements we do not want.}

Within our process domain $\tdomain[P]$ we can define the set $E$ of the narrowest possible statements. Its elements give a complete description of our system at all times and therefore we call them \textbf{evolutions} while we call $E \subset \tdomain[P]$ the set of all possible evolutions.

Another logical relationship we want to capture is the idea of whether two statements can be true at the same time. For example, we want to say that \statement{the horizontal position is between 2.5 and 3.5 meters} is \textbf{compatible} with \statement{the horizontal position is between 2 and 3 meters}. In general, given two statements $\stmt_1, \stmt_2 \in \tdomain[P]$ we denote $\stmt_1 \comp \stmt_2$ (read ``is compatible with'') if it is possible for them to be both true. As with narrowness, compatibility can be in terms of different ranges of the same quantities, of different quantities and of constraints at equal or different times.

Finally, we want to capture whether the truth of one or more statements influences the truth of others. For example, we want to say that \statement{the temperature of the water in the glass is 3.98 $^\circ C$} is \textbf{independent} from \statement{the volume of the water in the glass is 10 $cL$} but is not independent from \statement{the density of the water in the glass is 1 $g/cm^3$}. We note $\stmt_1 \indep \stmt_2$ (read ``is independent of'') when two statements are independent.

Every statement $\stmt \in \tdomain[P]$ can now be characterized by the set $A(\stmt) \subseteq E$ of all evolutions compatible with $\stmt$, that is all the evolutions for which $\stmt$ will be true. Mathematically,  $A(\edomain[P])$ is a \textbf{topology} over $E$ while $A(\tdomain[P])$ forms a $\sigma$-algebra over $E$, the \textbf{Borel algebra} of $A(\edomain[P])$. Narrowness and compatibility become set relationships in the $\sigma$-algebra: $\stmt_1 \narrower \stmt_2$ if and only if $A(\stmt_1) \subseteq A(\stmt_2)$ (i.e.~all evolutions for which $\stmt_1$ is true are such that $\stmt_2$ is also true) and $\stmt_1 \comp \stmt_2$ if and only if $A(\stmt_1) \cap A(\stmt_2) \neq \emptyset$ (i.e.~two descriptions are compatible if there is an evolution that is compatible with both). This connection gives a direct physical meaning to these foundational set-based mathematical structures.

\begin{table}[h!]
	\centering
\begin{tabular}[h]{|c|c|p{6cm}|}
	\hline 
	Symbol & Name & Meaning \\ 
	\hline 
	$\tdomain[P]$ & Process domain & The set of all possible descriptions at all times and at all levels of granularity \\ 
	\hline 
	$E \subset \tdomain[P]$ & Possible evolutions & The set of descriptions that give a full account of the process; they correspond to the narrowest statements within $\tdomain[P]$ \\ 
	\hline 
	$A(\stmt)$ & Compatible evolutions & The set of evolutions that are compatible with the statement $\stmt$ \\ 
	\hline 
	$\narrower$ & narrower & A statement is narrower than another $\stmt_1 \narrower \stmt_2$ if the second one is true whenever the first one is \\ 
	\hline 
	$\comp$ & compatible & A statement is compatible with another $\stmt_1 \comp \stmt_2$ if they can be true at the same time \\ 
	\hline 
	$\indep$ & independent & Two statements are independent  $\stmt_1 \indep \stmt_2$ if the truth of one does not influence the truth of the other \\ 
\hline 
\end{tabular} 
	\caption{Process descriptions and their logical relationships}
	\label{table:logic}
\end{table}


\subsection{Time and snapshots}

As a process extends over time and potentially describes multiple systems, we want to organize the statements into partial domains, one for each moment in time relative to a particular system. Given a \textbf{time parameter} $t \in T \subseteq \mathbb{R}$, we can imagine carving out a \textbf{system domain} $\tdomain_t$ that would correspond to all the statements about the system one can make at the given time. In particular, we define \textbf{one-step process} as one that only has an \textbf{initial domain} $\tdomain_{t_0}$ and a \textbf{final domain} $\tdomain_{t_1}$. Note that $\tdomain_t$ is a $\sigma$-complete Boolean sub-algebra of $\tdomain[P]$ since any logical operation on statements at fixed $t$ is also another statement at fixed $t$. The set $X_t \subset \tdomain_t$ of the narrowest statements will give the most precise description at that time; we call these the  possible \textbf{snapshots} of our system at that time. A \textbf{trajectory} is a sequence $x(t)$ of snapshots at each time. A particular evolution will identify a trajectory: that is, given $\stmt[e] \in E$ we can find $x(t)$ such that $\stmt[e] \narrower x(t_0) \AND x(t_1) \AND x(t_2) \AND ... $. The converse is not true: a single trajectory may correspond to multiple evolutions, since the evolutions potentially describe the same system at greater accuracy and/or other systems.

In fact, we should note that, technically, the statements are not defined at an infinitesimal moment $t$, but rather within a finite interval $[t, t + \Delta t]$. This represents the time-scale at which the process will be described, meaning the description within $\tdomain_t$ should not be sensitive to what happens at faster scales. The quantities used to represent states at time $t$, then, should really be thought of as averages within $\Delta t$. In the same way, $\tdomain_t$ will in general represent the system at the chosen level of description, not the narrowest level possible. Statements about the environment or at a finer level of description (i.e. the positions of all molecules for a gas) will not be included.

We should also note that the set of possible snapshots $X_t$ may not be the same at all times and may not correspond to the state of the system. First of all $X_t$ will only include those configurations that are compatible with at least one possible evolution (i.e. $A(x) \neq \emptyset$ for all $x \in X_t$), which may change in time. For example, the set of possible configurations for a system under dissipative processes will shrink as they converge to equilibrium. In quantum mechanics, the possible configurations after a measurement are restricted to the eigenstates, and a different choice of measurement will lead to a different process with different possible evolutions. Secondly, the best possible description may be broader than the full state of the system (e.g. when external interference forces us to give only a statistical account within our $\Delta t$) or narrower (e.g. when there are known correlations to the other degrees of freedom of the environment or of the system itself). In other words, the state space of the system (i.e. the set of all possible complete descriptions of the system and only about that system, independently of the process at hand) is something we'll construct later.\footnote{In other words, starting directly from a state space comes with a set of implicit assumptions about the system which are at the foundations of thermodynamics specifically and physics more in general.}

We say a process is \textbf{deterministic} over the system domains $\{\tdomain_t\}_{t \in T}$ if given the snapshot at one time we can always predict the snapshot at all future times. Formally, let $t_1, t_2 \in T$ such that $t_1 \leq t_2$, for any $x_1 \in X_{t_1} \subset \tdomain_{t_1}$ we can find an $x_2 \in X_{t_2} \subset \tdomain_{t_2}$ such that $x_1 \narrower x_2$. Intuitively, all the evolutions that pass through $x_1$ will also pass through $x_2$. If $x(t)$ is the snapshot trajectory in time, we have $A(x(t)) \subseteq A(x(t + \Delta t))$: the set of compatible evolutions must stay the same or become larger.

Conversely, we say a process is \textbf{reversible} over the system domains $\{\tdomain_t\}_{t \in T}$ if given the snapshot at one time we can always reconstruct the snapshot at all past times. Formally, let $t_1, t_2 \in T$ such that $t_1 \leq t_2$, for any $x_2 \in X_{t_2} \subset \tdomain_{t_2}$ we can find an $x_1 \in X_{t_1} \subset \tdomain_{t_1}$ such that $x_2 \narrower x_1$. In this case, all the evolutions that pass through $x_2$ must have passed through $x_1$. If $x(t)$ is the snapshot trajectory in time, we have $A(x(t)) \subseteq A(x(t - \Delta t))$: the set of compatible evolutions must stay the same or become smaller.

A deterministic and reversible process, then, will mean $x_1$ and $x_2$ are equivalent and that  $A(x(t)) = A(x(t + \Delta t))$: the set of evolutions remains the same. The fact that past and future snapshots are equivalent $x(t) \equiv x(t + \Delta t)$ does not mean they are the same statement in terms of the description at each time: it means there is an if-and-only-if relationship between the descriptions at different times. For example, \statement{at time $t$ the position is $q$ and the velocity is $v$} if and only if \statement{at time $t + \Delta t$ the position is $q + v \Delta t$ and the velocity is $v$}. The differential equation $\dot{v} = 0$ is a short hand for all such relationships.

If we limit ourselves to deterministic processes, with these few definitions, we have already found that the set of possible evolutions ``grows bigger'' under irreversible evolution while it remains the ``the same size'' under reversible evolution. This is the core idea that will recover the second law of thermodynamics.

These definitions naturally link to the idea of a dynamical system and clarify a critical issue. Suppose we have a deterministic process and we pick $t_1, t_2 \in T$. For each $x_1 \in X_1$, the final snapshot $x_2 \in X_2$ must be unique as all snapshots in $X_2$ are incompatible with each other. Therefore we can write a function $f : X_1 \to X_2$ such that $x_1 \narrower f(x_1) \equiv x_2$ that describes that particular step in the process. We call $f$ the \textbf{law of evolution}. Conversely, if the system is reversible, we would be able to find $g : X_2 \to X_1$ such that $x_2 \narrower g(x_2) \equiv x_1$.  We call $g$ the \textbf{law of (inverse) evolution}. Therefore \textbf{a law of (possibly inverse) evolution can always be used to fully characterize deterministic and/or reversible processes}.

We want to stress that if $f$ is invertible, the system is \emph{not} necessarily reversible. In fact, we would have $x_1 \equiv f^{-1}(x_2) \narrower x_2$, which is not the same as $x_2 \narrower g(x_2) \equiv x_1$: narrowness is in the opposite direction. In other words, \textbf{an invertible deterministic system is not necessarily reversible}. For example, consider a damped harmonic oscillator: the dynamics is in principle invertible (we can reconstruct the trajectory) but it is not reversible (as the evolutions bunch together, finite precision knowledge of the initial conditions gives us more information than the same finite precision knowledge of the final conditions). These types of distinctions are crucial and they emerge naturally within the framework.

TODO: picture with deterministic evolution merging "streams" of evolutions, and detrev evolutions not merging

\begin{table}[h!]
	\centering
	\begin{tabular}[h]{|c|c|p{6cm}|}
		\hline 
		Symbol & Name & Meaning \\ 
		\hline 
		$t \in T$ & Time parameter & Time is treated as a real valued parameter within the range $T \subseteq \mathbb{R}$ \\ 
		\hline 
		$\tdomain_t \subset \tdomain[P]$ & System domain & The set of descriptions at a given time \\ 
		\hline 
		$X_t \subset \tdomain_t$ & Possible snapshots & The set of descriptions that give a full account at time $t$; they correspond to the narrowest statements within $\tdomain_t$ \\ 
		\hline 
		$x(t) \narrower x(t + \Delta t)$ & Determinism & Given the snapshot at one time we can predict the snapshot at future times \\ 
		\hline 
		$x(t) \narrower x(t - \Delta t)$ & Reversibility & Given the snapshot at one time we can reconstruct the snapshot at past times \\ 
		\hline 
	\end{tabular} 
	\caption{System domain, snapshots, determinism and reversibility}
	\label{table:states}
\end{table}

\subsection{Granularity and quantifying evolutions}

The next step is to quantify the granularity of the description provided by each statement. Since statements at a finer level of description will put a greater constraint on the set of compatible evolutions, quantifying the granularity of the description of $\stmt$ means quantifying the size of $A(\stmt)$. The statement \statement{at time 0 s the particle is between 0 m and 1 m} is doubly more specific than \statement{at time 0 s the particle is between 0 m and 2 m} precisely because it corresponds to half the evolutions of the second statement.

The naive approach would be to assign a measure over $A(\tdomain[P])$: since this is a $\sigma$-algebra over $E$ and a measure is exactly what one uses to give a size to each set, it would seem we are done. Unfortunately it is not that simple: a single measure can only compare objects of the same dimensionality. If we imagine all evolutions to be points on an $n$-dimensional manifold, if the measure gives finite values for $k$-dimensional regions it will necessarily give measure zero for all regions with lesser dimensionality and infinite measure for those with greater dimensionality. Coarse grained and subsystem descriptions will have different dimensionality as they describe different numbers of degrees of freedom, therefore we need a way to quantify objects at all levels.\footnote{Typically one uses geometrical structures for his purpose. In phase space, the symplectic form allows us to define areas of two-dimensional surfaces, and use those to define volumes or even-dimensional areas. Note that it does not quantify the size of the points, which are implicitly assumed to be of equal size. This is a special case and cannot be the starting point.}

Therefore our starting point will be the ability to compare two statements and decide which one gives a finer description of the process. Formally, we have partial order $\finer$ on $\tdomain[P]$ which, given two statements, tells us whether the description of one is \textbf{finer}, more refined, than the other. Two statements are \textbf{equigranualar}, noted $\stmt_1 \eqgran \stmt_2$, if they provide the same level of description. For example, we can say that \statement{the trajectory of the particle is $y=t \cdot 1 \, m/s$} $\eqgran$ \statement{the trajectory of the particle is $y=t \cdot 2 \, m/s$} and that \statement{the position at time 0 sec is between 0 and 1 m} $\eqgran$ \statement{the position at time 0 sec is between 1 and 2 m}, even though the second pair is infinitely less discerning than the first. Note that if one statement is narrower than the other (i.e. $\stmt_1 \narrower \stmt_2$) then it is also finer than the other (i.e. $\stmt_1 \finer \stmt_2$) while the converse is not necessarily true. 

We say two statements are \textbf{comparable} if one is finer than the other. The order $\finer$ is partial because not all statements are comparable to each other. For example, consider the pressure/volume state space for an ideal gas. Comparing \statement{the pressure is 1 kPa and the volume is between 1 and 2 liters} and \statement{the pressure is between 1 and 2 kPa and the volume is 1 liter} would mean saying whether one liter is bigger or smaller than one kPa.\footnote{In phase space, for example, we cannot in general compare $q^i$ and $p_i$, yet we can always compare areas of phase space} The idea of physical dimension would therefore be something that is formally captured mathematically.

The idea is that geometrical, measure theoretic, information theoretic and probabilistic structures all descend from this more fundamental structure, the partial order that describes this ``information granularity''. We note that all these structures can be used to determine a size and therefore a partial order on the set. It would be possible, given sufficient conditions, to recover those structures from the order itself. The mathematical detail of how exactly this would work (i.e. what conditions are necessary and sufficient) is out of the scope of this work. Here we are interested in how the physical content attached to the partial order gives rise to the other structures.

To recover measures, we pick a statement $\stmt[u] \in \tdomain[P]$, called unit, and construct a \textbf{measure} $\mu_{\stmt[u]}$ such that $\mu_{\stmt[u]}(\stmt[u]) = 1$ and $\stmt_1 \finer \stmt_2$ will mean $\mu_{\stmt[u]}(\stmt_1) \leq \mu_{\stmt[u]}(\stmt_2)$. That is, we can quantify how much more or less precise a statement is compared to a fixed statement taken as a unit. If two statements are such that the measure of one with respect to the other is finite and non-zero, we say they are \textbf{finitely comparable}. We can show that if $\stmt[u]_1$ and $\stmt[u]_2$ are finitely comparable we have $ \mu_{\stmt[u]_1}(\stmt) = \mu_{\stmt[u]_1}(\stmt[u]_2) \mu_{\stmt[u]_2}(\stmt)$. Furthermore, since all evolutions give a complete description of the whole process, we will assume they are all equigranular. Therefore if we pick $\stmt[u] \in E$, then $\mu_{\stmt[u]}(\stmt)=\#(A(\stmt))$ would be the counting measure of evolutions compatible with $\stmt$.\footnote{We still need to understand the necessary and sufficient conditions under which such measures would exist and be unique. For the present work, we assume it can be done.}

We can also define the \textbf{evolution entropy} with respect to $\stmt[u]$ as the quantity $S_{\stmt[u]}(\stmt) = \log \mu_{\stmt[u]}(\stmt)$. This can be thought of as the number of bits, yes/no questions, that separate the level of description of $\stmt[u]$ from the one provided by $\stmt$. Connecting the definitions from above, under a deterministic process we have $S_{\stmt[u]}(x(t)) \leq S_{\stmt[u]}(x(t + \Delta t))$ since $x(t)\narrower x(t+\Delta t)$ means $x(t) \finer x(t+\Delta t)$ and $\mu_{\stmt[u]}(x(t)) \leq \mu_{\stmt[u]}(x(t+\Delta t))$. If the process is determistic and reversible, we have $S_{\stmt[u]}(x(t)) = S_{\stmt[u]}(x(t + \Delta t))$. In other words, \textbf{evolution entropy cannot decrease for a deterministic process and is stationary if the process is reversible as well}.

Quantifying evolutions allows us to define probability. The idea is a process can be realized differently by each possible evolution. The more evolutions a statement is compatible with, the more it is likely to happen. For example, consider a process that implements a coin flip. If the coin (and the process flipping the coin) is fair, we must have that half the evolutions are compatible with \statement{the coin is head} while the other half are compatible with \statement{the coin is tails}.  Therefore we define that the \textbf{probability} of $\stmt_2$ given $\stmt_1$ is
\begin{equation}
	P(\stmt_2 | \stmt_1) = \mu_{\stmt_1}(\stmt_1 \AND \stmt_2) = \frac{\mu_{\stmt[u]}(\stmt_1 \AND \stmt_2)}{\mu_{\stmt[u]}(\stmt_1)}
\end{equation} where $\stmt_1 \AND \stmt_2$ is the logical conjunction (AND) between the two statements and $\stmt[u]$ is finitely comparable to $\stmt_1$. In other words, the probability quantifies the ratio of evolutions compatible with $\stmt_1$ that are also compatible with $\stmt_2$. This can be shown to satisfy the standard axioms of probability.

Suppose that, given a process, we pick $t_1, t_2 \in T$. Given an initial description $\stmt_1 \in \tdomain_{t_1}$ and a final description $\stmt_2 \in \tdomain_{t_2}$, the \textbf{forward transition probability} $P(\stmt_2 | \stmt_1)$ tells us the fraction of evolutions starting in $\stmt_1$ (i.e. that are compatible with $\stmt_1$) that end in $\stmt_2$. In the same way, we can look at the \textbf{backward transition probability}. The transition probabilities are fully determined, through the measure, by the fineness relationship. Conversely, the fineness relationship can be, through the measure, recovered by the probabilities. Therefore \textbf{forward and backward transition probabilities are enough to fully characterize the behavior of a system within a process.}

We say that a process is \textbf{isentropic} if for any possible trajectory $x(t)$ we have $S_{\stmt[u]}(x(t)) = S_{\stmt[u]}(x(t + \Delta t)) \neq 0$. We saw that all deterministic and reversible processes are isentropic, but the converse is not true. We can give, though, a characterization in terms of transition probabilities. Assume $\stmt[u]$, $x(t)$ and $x(t+\Delta t)$ are all finitely comparable. We can write:
\begin{align*}
	\mu_{\stmt[u]}(x(t) &\AND x(t+\Delta t)) \\
	&=  \mu_{\stmt[u]}(x(t+\Delta t)) P(x(t)|x(t+\Delta t)) \\
	&=  \mu_{\stmt[u]}(x(t)) P(x(t+\Delta t)|x(t)) \\
\end{align*}
Therefore
\begin{equation}
\frac{\mu_{\stmt[u]}(x(t+\Delta t))}{\mu_{\stmt[u]}(x(t))} = \frac{P(x(t+\Delta t)|x(t))}{P(x(t)|x(t+\Delta t))}.
\end{equation}
If the forward and backward probabilities are the same, the initial and final snapshots have the same measure. Conversely, if the initial and final snapshot have the same measure, the forward and backward probabilities must be the same. Equal measure corresponds to equal process entropy. Therefore \textbf{isentropic processes are those and only those for which backward and forward probabilities are the same}.


%Note that this approach resolves a problem that causes friction between quantum mechanics and standard probability theory. Given two incompatible states $\psi$ and $\phi$, that is $0 < | \left<\psi | \phi \right> |^2 < 1$, we would like to say that that the probability of one given the other is $P(\psi | \phi) = | \left<\psi | \phi \right> |^2$.

\begin{table}[h!]
	\centering
	\begin{tabular}[h]{|c|c|p{6cm}|}
		\hline 
		Symbol & Name & Meaning \\ 
		\hline 
		$\finer$ & finer & A statement is finer than another $\stmt_1 \finer \stmt_2$ if it provides a more refined description than the second \\ 
		\hline 
		$\eqgran$ & equigranular & A statement is equigranular to another $\stmt_1 \eqgran \stmt_2$ if they provide the same level of detail \\ 
		\hline 
		$\mu_{\stmt[u]}(\stmt)$ & Measure & Quantifies the precision of a statement $\stmt$ in terms of a reference unit statement $\stmt[u]$; it quantifies how many evolutions are compatible with $\stmt$ compared to the evolutions compatible with $\stmt[u]$  \\ 
		\hline 
		$S_{\stmt[u]}(\stmt)$ & Evolution entropy & Defined as $\log \mu_{\stmt[u]}(\stmt)$; represents the number of bits that separate the level of description of $\stmt$ from the one provided by the unit statement $\stmt[u]$; it quantifies the number of questions needed to go from the level of description of $\stmt$ to the level of description of $\stmt[u]$  \\ 
		\hline 
		$P(\stmt_2 | \stmt_1)$ & Probability & Defined as $\mu_{\stmt_1}(\stmt_1 \AND \stmt_2)$; quantifies the fraction of evolutions compatible with $\stmt_1$ that are also compatible with $\stmt_2$  \\ 
		\hline 
		 & Comparable & Two statements are comparable if one is finer than the other \\ 
		\hline 
		 & Finitely comparable & Two statements are finitely comparable if the measure of one in terms of the other is finite and non-zero \\ 
		\hline 
	\end{tabular} 
	\caption{Granularity, measures and probability}
	\label{table:states}
\end{table}

\subsection{States and system independence}

So far we have made no guarantees as to whether each domain $\tdomain_t$ and its snapshots $X_t$ contains \emph{all} descriptions that refer \emph{only} to a particular physical object. Some snapshots may or may not be available at a particular time and/or there may be correlations with other systems. We want the state space to consist of all the possible descriptions of a system, and only of that system, at any point in time for any process. This desire comes with some hidden requirements that we want to bring to light.

First of all, when we say ``all processes'' we can't really mean absolutely all of them, but only the one for which the system is well defined. For example, we can study the motion of a balloon on earth at 20 Celsius. This would not be possible on the surface of the sun as our balloon would not exist. The process has to be such that the system is recognizable to be the same, our descriptions must still make sense. Mathematically, we have a \textbf{state domain} $\tdomain$ of all statement fragments like \statement{the position of the ball is between 2 and 3 meters} together with a surjective map $\iota : \tdomain \times T \twoheadrightarrow \tdomain_t$ that adds \statement{at time t} for each possible time. The map $\iota$ must be a surjection that preserves the logical structure (i.e. logical operations, verifiability, narrowness, compatibility, ...) so that the basic logical relationships determined by the system itself are are valid at all times (i.e. \statement{at time t the position of the ball is between 2 and 3 meters} $\narrower$ \statement{at time t the position of the ball is between 0 and 300 meters} for every $t$).\footnote{We stress that the mathematical requirements are necessary but not sufficient. While the same system must map to the same logical structure but the same logical structure does not necessarily map to the same system (e.g. two distinct system can be represented by the same mathematical structure).}

Note, though, that $\iota( \cdot, t_0)$ in general is not an isomorphism (i.e. a bijection that preserves the logical structure) for two reasons. The first is that not all states will be available at every moment in every process. As we said before, if the process is dissipative the set of possible states will become smaller. In that case, $\iota$ will map all the inaccessible states to $\contradiction$, as they will be impossible. Therefore $\iota$ is not in general a bijection. The second reason is that the statement fragments in $\tdomain$ represent only descriptions about the system itself and nothing else. In a particular process, however, knowing the state of one system might tell us something about other system as well. The presence of correlations/coupling may mean that the precise knowledge of the degrees of freedom of the system may allow us to tell something about the environment or the internal structure (i.e. microstate). Mathematically, $\iota$ may map to statements that are narrower than the original fragment (e.g. we can have $\iota($\statement{position of A is 1 meter}$, t_0) \equiv$ \statement{at time $t_0$ the position of A is 1 meter and the position of B is -1 meter}).

In physics terms, the construction of the state domain requires us to be able to identify and separate the system from all other systems and the environment. We can talk about a book and its state because we can, in many situations, describe and manipulate the book without describing or manipulating other books, the bookcase or the table. If $\tdomain_{t}$ is the domain of our system at time $t$ and $\tdomain[O]_t$ is the domain for another system, we say that the two domains are \textbf{independent} if, taken any two statements $\stmt_d \in \tdomain_{t}$ and $\stmt_o \in \tdomain[O]_{t}$ are independent. In this case there are no correlations: every possible pair of snapshots of each domain is a possible snapshot for the combined system. Therefore if we take $\stmt[u]_d \in \tdomain_{t}$ and $\stmt[u]_o \in \tdomain[O]_{t}$ to be unit statements for the respective measure, we have:
\begin{equation}\label{entropy_sums}
\begin{aligned}
	\mu_{\stmt[u]_d \AND \stmt[u]_o}(\stmt_d \AND \stmt_o)=\mu_{\stmt[u]_d}(\stmt_d) \mu_{\stmt[u]_o}(\stmt_o) \\
S_{\stmt[u]_d \AND \stmt[u]_o}(\stmt_d \AND \stmt_o)=S_{\stmt[u]_d}(\stmt_d) + S_{\stmt[u]_o}(\stmt_o)
\end{aligned}
\end{equation} 
That is, \textbf{for independent systems the process entropy can be seen as the sum of the entropy of each system}.\footnote{Note the direction in which the construction unfolds. We are not defining the entropy on the single systems and then, if they are isolated, the entropy sums. It's the opposite: the entropy is initially defined on the joint system for a particular process as the evolutions describe all systems. If the system is independent of all others, the evolutions are grouped such that their measure factorizes, and we can assign a unique measure to each statement of the independent system, as this will not depend on the description of other systems. If we cannot find this situation, the entropy of the system cannot be defined uniquely on the separate system, but only on the joint. In physics terms, we are unable to treat the description as set of independent degrees of freedom (e.g. describing only the pressure of a gas without the volume, or only the position without the momentum).}

We say we have an \textbf{independent system} if the system is independent from all other systems. In this case, the snapshot is a description of the system and only of the system. The state domain, then, represent descriptions of the system when it is independent. A \textbf{state} $x$ of a system is a snapshot in these conditions and the \textbf{state space} is the set of all possible states.\footnote{There is still the added complication that, even if the system is independent, not all states may be accessible. For example, the thermodynamic states may change depending on the constraint applied to the system (e.g. presence or absence of a magnetic field); in quantum mechanics, the accessible states after a spin measurement will only be the ones along the measured direction. So, in general, the state space stitches together all the possible independent snapshots at all times over all possible processes.} In the same way, the process entropy becomes a property of the system itself and we can define the \textbf{state entropy} $S_{\stmt[u]} : X \to \mathbb{R}$ as the process entropy in these conditions, where $\stmt[u] \in X$ is a state taken as reference. That is:
\begin{equation}
	S_{\stmt[u]}(x) = S_{\iota(\stmt[u], t_0)}(\iota( x, t_0))_{indep}
\end{equation}

We say $\xi^a : X \to \mathbb{R}$ are a set of state variables if they uniquely identify a state. That is, we can find an isomorphism (a bijection that preserve the logical structure) $\phi : \mathbb{R}^n \to X$ such that $\phi(\xi^a(x)) = x$ for all $x \in X$. Then each state space comes equipped with a \textbf{state function} $S=S(\xi^a)$. We'll see that, for thermodynamic systems, this will correspond to the usual thermodynamic entropy. We define a \textbf{purely mechanical} system one for which the state entropy is the same across all states (i.e. $S=k$ where $k$ is a constant, typically chosen to be zero). We'll see that classical phase space and quantum state spaces are state spaces of purely mechanical systems.

Since we can only properly talk about a system when it is independent from the other, what can we do in the other cases? The idea is that we can still talk about a system in that case if we are able, at least in line of principle, to decouple the system from all others while not changing the nature of the system itself. For example, we can talk about a molecule of a gas because we have a way to take such molecule out of the gas. The construction of a state space of a system, then, also requires telling us what all the possible correlated description will go when the system is made independent. Mathematically, we need a deterministic one-step process that takes any initial snapshot of the system to a final state, and does so without disrupting the system (i.e. the relaxation process must take each state to itself). We call this process \textbf{relaxation}.\footnote{The state space is invariant over the relaxation. In other word, the state space is a symmetry of the relaxation process.}

If $x_0$ is the initial snapshot and $x_1$ is the final state, given that the relaxation process is deterministic, we have $x_1= x_1(x_0)$ and 
\begin{equation}
S_{\iota(\stmt[u], t_1)}(x_0) \leq S_{\iota(\stmt[u], t_1)}(x_1) = S_{\iota(\stmt[u], t_1)}(\iota( x, t_1)) = S_{\stmt[u]}(x)
\end{equation}
This means that all snapshots that relax to the same state will have a process entropy lower or equal to the state entropy. Therefore \textbf{the state entropy is the largest possible process entropy for which the description given by the state applies}.\footnote{Mathematically, the relaxation process in needed to compare the process entropies for the correlated descriptions. As the same description can be more or less correlated in different cases, the independent ones work as a reference and we need a way to relate to them.} This does not simply represent a subjective information theoretic consideration (e.g. we know only about the system); it is a physical consideration (e.g. knowledge about one system tells us nothing about the others). As we make the system more independent, there are more possible evolutions the combined system can have, the dynamics of the system is less affected, less correlated, by the dynamics of the other.

If the final state is identified by the set of state variables $\xi^a : X \to \mathbb{R}$, the initial snapshot will be identified by the same quantities since the relaxation process should not, in principle, disturb the system. These quantities, then, are a constraint during relaxation and, additionally, they must be the only constraints. Additional constraints, in fact, would increase the number of final states, meaning the initial set of quantities did not form a full set of state variables. Therefore \textbf{state variables identify both the states of the systems and the constraints during relaxation}.

The takeaway message is that \textbf{entropy maximization is equivalent to requiring system independence}. Maximizing entropy means taking the system apart from the others, so that its dynamics and description is decoupled. We should stress here that the description may not include all possible degrees of freedoms of the object studied (e.g. macrostate variable, position and momentum of the center of mass). Maximizing entropy means requiring independence of that partial description from the rest, effectively decoupling environment, other systems and internal dynamics in one operation. Entropy maximization, more than a law of physics, is a requirement to properly define a part of nature as a system. When we divide nature in independent systems, therefore, we are automatically looking for processes that increase entropy.
 
As we have seen, the definition of a system as a part of nature to be studied independently from the rest comes with physical requirements that provide a rich conceptual and mathematical structure that maps extremely well to the basic ideas in thermodynamics.

\section{Thermodynamic processes}

We now have all the basic tools we need to recover thermodynamic as the study of the relaxation process of composite systems. That is, we have two systems, we put them together and relax the composite system. In light of what discussed before, the composition rule must tell us what composite states are possible and what is the relaxation process. These will be linked because of the double role of the state variables. Thermodynamic describes that link in the case of extensive (i.e. additive) state variables.\footnote{It should be stressed that, in our framework, recovering the laws of thermodynamics does not always mean recovering universal truths about nature. It simply means identify the necessary and sufficient assumptions for which a given system will be suitable described by those laws.}

As we noted before, given the same physical object, we will have a different relaxation process and different state variables. When composing two systems, we are automatically given a relaxation process which is the independent relaxation of each system. The state space, in this case, is simply the set of all possible pairs of states of the components. This means we have constraints within the system which is therefore not necessarily relaxed: the description of the whole is tied to the description of the parts. That is, we don't simply describe what the total energy or the total number of particle is, we also describe the energy and the particles for each part. We now want to remove the constraints within the system and characterize the relaxation process. We call \textbf{equilibrium} the final states of a relaxation process of a composite system with no internal constraints. We call one-step equilibration process a one step process that ends in equilibrium.

\textbf{The first assumption in thermodynamic is that an equilibrium state of the whole is described by a pair of equilibrium states of the parts.} We qualify this as an assumptions as it not always true for all systems and can fail in several ways. For example, removing the wall between two substances may change them both in a third (this usually addressed by changing the state space of the parts to accommodate). More subtly, in the presence of impermeable walls we can characterize the state on either side with a fixed number of particle. When the wall is removed, the state will be characterized by an average number of particles. The state space is not technically the same. If the boundary effects between the two components are non negligible, they will not be independent (i.e. the entropy does not sum). Outside of thermodynamics, an entangled state is certainly a state of a composite system (e.g. two photons) where constraints are put only on the whole (e.g. total spin) and it cannot be described as a simple combination of subsystems' states (e.g. is it not separable). Therefore there are definitely cases that do not follow this assumption. Characterizing all cases in which the assumptions hold (e.g. a ``large'' system in contact with a sufficiently ``large reservoir'') is left for future work.

\textbf{The second assumption in thermodynamic is that constraints are independent, each identified by an extensive (i.e. additive under composition) quantity.} There is good reason to suspect that this assumption is, at least in large part, already contained in the previous one. Under the first assumption, in fact, state entropy is linear under composition. It could be that, assuming the constraints are independent (i.e. the variation on one does not change the others) is enough to construct a linear quantity to describe them. In the end, a linear scale for energy, mass, volume, number of particles, etc.. is typically construct by preparing reference sample and combining them. We leave this investigation for future work.

\textbf{The third assumption in thermodynamic is that one such constraint is the internal energy and its corresponding equilibrium is thermal equilibrium.} This is hope that this too can be already contained in the first assumption. In previous work we have recovered both classical Hamiltonian and quantum unitary evolution under the assumption of deterministic and reversible evolution which, as we have seen, is equivalent to conservation of process entropy. In those settings the notion of energy, in the form of the Hamiltonian, arises naturally so it could be made to arise naturally here as well. Yet, there some technical subtleties in the relationship between the Hamiltonian and the internal energy which we still have to work out.

Under these assumptions, we can recover the laws of thermodynamics. Let $A$ and $B$ be two system in thermal equilibrium and let $AB$ be the composite. We have:
\begin{equation}
	\begin{aligned}
	U_{AB} &= U_A + U_B \\
	S_{AB}(U_{AB}) &= S_A(U_A) + S_B(U_B) \\
	&= \sup \{S_A(x) + S_B(y) \, | \, x+y= U_{AB}\} \\
	\frac{1}{kT_A} = \beta_A = \frac{\partial S_A}{\partial U_A} &= \frac{\partial S_B}{\partial U_B} = \beta_B = \frac{1}{kT_B}
	\end{aligned}
\end{equation}
which follows the standard textbooks derivations. One finds that thermal equilibrium is achieved if and only if $T_A$ and $T_B$ are equal.  If $B$ is in thermal equilibrium with another system $C$ we also have $T_B$ is equal to $T_C$ and therefore $A$ and $C$ are also in thermal equilibrium. This recovers the zeroth law.

We say a system $R$ is a \textbf{thermal reservoir} if its equation of state can be sufficiently approximated by $S = \beta U = \frac{U}{k T}$. Internal energy is the only relevant constraint for the thermal reservoir and the only way the entropy of the system can change. We call \textbf{heat} the energy exchanged by a thermal reservoir and we note $Q = - \Delta U$ as the energy lost by the reservoir during a one-step process.

Let $M$ be a purely mechanical system. Let $U$ be the internal energy and $X^i$ the other variables. We have:
\begin{equation}
\begin{aligned}
S_M&=S_M(W, X^i)=0 \\
dS_M = 0 &= \frac{\partial S_M}{\partial U} dU + \frac{\partial S_M}{\partial X^i} dX^i \\
dW &= \frac{1}{\frac{\partial S_M}{\partial U}}\frac{\partial S_M}{\partial X^i} dX^i \\
P_i &= \frac{1}{\frac{\partial S_M}{\partial U}}\frac{\partial S_M}{\partial X^i} \\
dU &= P_i dX^i.
\end{aligned}
\end{equation}
For a purely mechanical system, a chance in internal energy is always linked to a change one of the other constraints. The energy of the purely mechanical system is \textbf{stored} in one of the constraints. We note a change of internal energy during a one-step equilibration process of a pure mechanical system as $W=\Delta U$.

We now consider a system composed of a generic system $A$, a thermal reservoir $R$ and a purely mechanical system $M$. During a one-step equilibration process, because the total energy of the system must be conserved, we have:
\begin{equation}
\begin{aligned}
&\Delta U_A + \Delta U_R + \Delta U_M = 0 \\
&\Delta U_A - Q + W = 0 \\
&\Delta U_A = Q - W.
\end{aligned}
\end{equation}
This recovers the first law.

We now look at the change in entropy, which can only increase during a one-step process. Keeping in mind $M$ is purely mechanical, we have:
\begin{equation}
\begin{aligned}
&\Delta S_A + \Delta S_R + \Delta S_M \geq 0 \\
&\Delta S_A \geq - \Delta S_R \\
&\Delta S_A \geq \int \beta_R dQ \\
&\Delta S_A \geq \int \frac{dQ}{k T_R}.
\end{aligned}
\end{equation}
which recovers the second law.


As for the second law, we have given a characterization that is far more general than what it is given in thermodynamics. Nevertheless, we can 

% DO WE NEED INTERNAL STATE? On the other side, a state of a system is never really the description of the whole system, but of the part that is relevant to the process at hand. If we study the motion of the balloon, position and velocity may be enough; if we study it as a thermodynamic system, pressure and volume may be enough. The idea is that the state captures the system at a particular level of granularity and the process does not depend, is not sensitive, to the finer dynamics. We say a system is in \textbf{equilibrium} if the state tells us nothing about the 

 The physical meaning that ``all heat is equivalent'' and ``all diathermal walls are equivalent'' is explicit in the fact that the internal energy identifies the constraint.

\section{Math section}

Let $\tdomain$ be the set of all statements about the evolution of the system. This is a $\sigma$-complete Boolean algebra. Let $E \subset \tdomain$ the set of all evolutions, these are the narrowest statements. To each statement $\stmt \in \tdomain$ corresponds a set of evolutions $A(s)$ that are compatible with the statement.

Given two statements $s_1, s_2 \in \mathcal{D}$ we can say whether one is finer than the other $s_1 \finer s_2$. Given any statement $\stmt[u] \in \mathcal{D}$, we have a function $\mu_u : \mathcal{D} \to \mathbb{R}^+\cup \{+\infty \}$ such that:
\begin{enumerate}
	\item $\mu_u$ is a measure
	\item $\mu_u(u) = 1$
	\item $\mu_u(s_1) \leq \mu_u(s_2)$ if $s_1 \finer s_2$
\end{enumerate}
The value $\mu_u(\stmt)$ tells us how many trajectories are compatible with $\stmt$ in terms of how many trajectories are compatible with the unit statement $\stmt[u]$.

The conditional probability $P(\cdot | \cdot) : \tdomain \times \tdomain \to [0,1]$ is defined as $P(\stmt_1 | \stmt_2) = \mu_{\stmt_2}(\stmt_1 \AND \stmt_2)$ and represents the fraction of trajectories that are compatible with $\stmt_2$ that are also with $\stmt_1$.

We define $T \subseteq \mathbb{R}$ to be the range of the time parameter. We define $\tdomain_{t} \subseteq \tdomain$ as the set of statements defined at the time t. Note this does not mean they are statement of only what happens at an instant in time. They can be averages around. It's how we decompose the problem at the desired time resolution. The states at time $t$ are the possibilities of $\tdomain_t$. The process is deterministic over the chosen states if for each $x_0 \in \tdomain_{t_0}$ we can find an $x_1 \in \tdomain_{t_1}$ such that $x_0 \narrower x_1$.  The process is reversible over the chosen states if for each $x_1 \in \tdomain_{t_1}$ we can find an $x_0 \in \tdomain_{t_0}$ such that $x_1 \narrower x_0$.

Things I want to prove:

\begin{prop}
	Let $\tdomain[P]$ be a deterministic process over $\{X_t\}_{t \in T}$. Then $S(x(t))$ is a monotonically increasing function.
\end{prop}

\begin{prop}
	Let $\tdomain[P]$ be a reversible process over $\{X_t\}_{t \in T}$. Then $S(x(t))$ is a monotonically decreasing function.
\end{prop}

\begin{prop}
	Let $\tdomain[P]$ be a deterministic and reversible process over $\{X_t\}_{t \in T}$. Then $S(x(t))$ is constant.
\end{prop}

\begin{prop}
	Let $\tdomain[P]$ be a deterministic process over $\{X_t\}_{t \in T}$. Then $S(x(t))$ is a monotonically increasing function.
\end{prop}

\bibliography{bibliography}


\end{document}