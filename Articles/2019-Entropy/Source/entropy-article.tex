\documentclass[letterpaper]{article}

\input{../../../include/basicstyle}
\input{../../../include/theorems}
\input{../../../include/logic}

\begin{document}

\title{TBD}
\author{Gabriele Carcassi, Christine A. Aidala \\ University of Michigan}

\date{\today}

\maketitle

\begin{abstract}
	TBD
\end{abstract}


\section{Introduction}

------------------
Cite other axiomatic works.

Single state space to capture:
* descriptions at all levels
* descriptions at different times
* correlations between descriptions at equal time (equations of state)
* correlations between descriptions at different times (evolution laws)
* different correlations at different description
* granularity/precision of such descriptions

We can't have:
* probability as a native concept
------------------

In the intro, describe only the discrete case as it simpler to understand and already contains all the physics. The continuous case adds the complication of 

Evolution is a temporal sequence of states. Process is a set of possible evolutions. What we want to study is how a statement about a state at a particular time selects a particular set of possible evolutions and therefore selects a set of possible states at a previous or past time.

Define determinism and reversibility in terms of what evolutions exist. Note that a dynamical system is a deterministic system.

Note how the number of states does not always match the number evolutions it is compatible with. Note that under deterministic evolution the number of compatible evolutions can only increase. For dynamical systems, then, the number of evolutions associated to a state can only increase in time and it is stationary for equilibria.

If we compose independent systems, the number of possible states and trajectories is the product. If we take the logarithm, then those numbers are additive. Conceptually the entropy is the logarithm of the number of evolutions compatible with a given state. This article shows how this simpler and more general concept leads, in special cases, to the Boltzman entropy and the Shannon entropy. We will also see how this same concepts is applicable more in general, including the case of non-Hamiltonian systems.

\section{Main points}
\begin{enumerate}
	\item Entropy assigns a size to points. Shannon entropy works on a distribution where each point has the same entropy.
	\item Measure theory differentiate between points and sets. This does not work when combining different level of granularity.
\end{enumerate}

\section{Main section}
\begin{enumerate}
	\item \emph{Experimentally testable descriptions and their logic relationship.} Here the goal is to formalize the basic structure required by a set of experimentally verifiable assertions. Some descriptions are more specific than others, some are compatible with each other, and so on. An evolution is the most specific description, one that tells us what happens at all times.
	\item \emph{Group descriptions by time.} Here the goal is to group description at ``instants of time'', which are taken to be small intervals. A snapshot is the most specific description at an instant, on that tells us everything there is to know about the system at the given time. Determinism and reversibility are defined at this level (i.e. deterministic process is one where the snapshot at one time tells us the snapshots at future times).
	\item \emph{Granularity of the description and counting evolutions.} Here the goal is to quantify how much a description is more refined than another. This can be done by quantifying the possible evolutions that are compatible with each (i.e. the more specific description will be compatible with fewer evolutions). The evolution entropy quantifies the difference in granularity in terms of how many questions (i.e. bits) are needed to go from one description to another. The evolution entropy cannot decrease for a deterministic process and is stationary if the process is also reversible.
	\item Define deterministic evolution: can predict future state. Past state narrower. Number of evolutions increases. Define reversible. Future state narrower. Number of evolution decreases. Det/rev remains the same. How do we quantify that?
	\item Define way to compare granularity. Not a single measure, not everything comparable or finitely comparable. Define probability in terms of measure.
	\item States of the same system must be finitely comparable. Can put a measure on states themselves, which is a count of evolutions. Examples to show that: the measure and state space, in general, change over time. State spaces are stable and can be assigned a measure only at equilibrium.
	\item Define independent systems as those where the state of one does not constrain the state of the other. The measure must be the product.
	
\end{enumerate}

\begin{enumerate}
	\item A thermodynamic system is one that undergoes deterministic evolution with equilibria. There are variables that  
	\item Extensive quantities are somewhat postulated at this point. So is the existence of energy.
	\item Second law come from deterministic system.  Define entropy as the log of the measure, so that it is additive under independent systems. We have: entropy always increases under deterministic evolution; entropy is maximal at equilibrium; entropy is additive for independent system. Entropy is a function of the equilibrium, which is identified by state variables.
	\item Third law comes from states of the same system being finitely comparable. Lowest entropy is "one evolution" in terms of system granularity.
	
\end{enumerate}


\section{Evolutions, states and levels of description}

The overall goal is to describe a generic system as it evolves in time by defining a minimal set of concepts that such a setup must define, and understand the tacit assumptions underlying state spaces and evolution laws. We want to find notions that can be applied in different settings, being it thermodyamics and/or dynamical systems alike. Then we want to derive general results based on such definitions. Here we provide an overview of the main features, leaving the formal definitions in the (yet to be written) appendix.

\subsection{Descriptions and logical relationships}

Our starting point is the \textbf{process domain} $\tdomain[P]$, which is the set that contains all possible descriptions of the system at all levels of detail at all possible times. Depending on the process, it will contain statements like \statement{the average volume between 1 and 2 seconds is between 3 and 4 liters} or \statement{the trajectory of the particle is $y=10 \, m - t^2 \cdot 9.80665 \, m/s^2 $}. It does not matter at this point what the statements themselves are, except they must be, at least in line of principle, defined from experimentally well-defined starting points. Mathematically, $\tdomain[P]$ is a $\sigma$-complete Boolean algebra\footnote{A set of statements that is closed under negation (NOT), countable disjunction (OR) and countable conjunction (AND).} and it is the closure of the set of experimentally verifiable statements $\edomain[P] \subseteq \tdomain[P]$.\footnote{The set $\edomain[P]$ will form a Heyting algebra which, to be experimentally reachable, must allow a countable basis. The closure $\tdomain[P]$ is in terms of negation and countable disjunction.}

As $\tdomain[P]$ contains different levels of description, some statement will be more or less specific. For example, we say that \statement{the horizontal position is between 2.5 and 3 meters} is \textbf{narrower} than \statement{the horizontal position is between 2 and 3.5 meters}. Note that whenever the first statement is true, the second one must be true as well. Given two statements $\stmt_1, \stmt_2 \in \tdomain[P]$, we formally capture this relationship by noting $\stmt_1 \narrower \stmt_2$. Narrowness can describe relationships at different scales, on different quantities (e.g. \statement{the horizontal position is between 2.5 and 3 meters and the vertical position is between 1 and 1.5 meters} $\narrower$ \statement{the horizontal position is between 2.5 and 3 meters}), constraints between variables at the same time (e.g. \statement{the temperature of the water in the glass is 3.98 C} $\narrower$ \statement{the density of the water in the glass is 1 $g/cm^3$}) or at different times (e.g. \statement{at time 0 s the position is 1 m and the velocity is 1 m/s} $\narrower$ \statement{at time 1 s the position is 2 m}). It is a general tool to capture relationships within the theory which intuitively can also be thought as implication.\footnote{Technically, implication in classical logic is a truth function as classical logic does not incorporate the idea of different ``possible cases''. What we have is more similar in spirit to semantic consequence in modal logic, though modal logic brings in a lot of undesirable elements we do not want.}

Within our process domain $\tdomain[P]$ we can define the set $E$ of the narrowest possible statements. Its elements give a complete description of our system at all times and therefore we call them \textbf{evolutions} while we call $E \subset \tdomain[P]$ the set of all possible evolutions.

Another logical relationship we want to capture is the idea of whether two statements can be true at the same time. For example, we want to say that \statement{the horizontal position is between 2.5 and 3.5 meters} is \textbf{compatible} with \statement{the horizontal position is between 2 and 3 meters}. In general, given two statements $\stmt_1, \stmt_2 \in \tdomain[P]$ we note $\stmt_1 \comp \stmt_2$ if it is possible for them to be both true. As with narrowness, compatibility can be in terms of different ranges of the same quantities, of different quantities and of constraints at equal or different times.

Finally, we want to capture whether the truth of one or more statements incluences the truth of others. For example, we want to say that \statement{the temperature of the water in the glass is 3.98 C} is \textbf{independent} from \statement{the volume of the water in the glass is 10 $cL$} but is not independent from \statement{the density of the water in the glass is 1 $g/cm^3$}. We note $\stmt_1 \indep \stmt_2$ when two statements are independent.

Every statement $\stmt \in \tdomain[P]$ can now be characterized by the set $A(\stmt) \subseteq E$ of all evolutions compatible with $\stmt$, that is all the evolutions for which $\stmt$ will be true. Mathematically,  $A(\edomain[P])$ is a \textbf{topology} over $E$ while $A(\tdomain[P])$ forms a $\sigma$-algebra over $E$, the \textbf{Borel algebra} of $A(\edomain[P])$. Narrowness and compatibility become set realtionships in the $\sigma$-algebra: $\stmt_1 \narrower \stmt_2$ if and only if $A(\stmt_1) \subseteq A(\stmt_2)$ (i.e.~all trajectories for which $\stmt_1$ is true are such that $\stmt_2$ is also true) and $\stmt_1 \comp \stmt_2$ if and only if $A(\stmt_1) \cap A(\stmt_2) \neq \emptyset$ (i.e.~two descriptions are compatible if there is an evolution that is compatbile with both). This connection gives a direct physical meaning to these foundational set-based mathematical structures.

\begin{table}[h!]
	\centering
\begin{tabular}[h]{|c|c|p{6cm}|}
	\hline 
	Symbol & Name & Meaning \\ 
	\hline 
	$\tdomain[P]$ & Process domain & The set of all possible descriptions at all times and at all levels of granularity \\ 
	\hline 
	$E \subset \tdomain[P]$ & Possible evolutions & The set of descriptions that give a full account of the process; they correspond to the narrowest statements within $\tdomain[P]$ \\ 
	\hline 
	$A(\stmt)$ & Compatible evolutions & The set of evolutions that are compatible with the statement $\stmt$ \\ 
	\hline 
	$\narrower$ & narrower & A statement is narrower than another $\stmt_1 \narrower \stmt_2$ if the second one is true whenever the first one is \\ 
	\hline 
	$\comp$ & compatible & A statement is compatible with another $\stmt_1 \comp \stmt_2$ if they can be true at the same time \\ 
	\hline 
	$\indep$ & independent & Two statements are independent  $\stmt_1 \comp \stmt_2$ if the truth of one does not influence the truth of the other \\ 
\hline 
\end{tabular} 
	\caption{Process descriptions and their logical relationships}
	\label{table:logic}
\end{table}


\subsection{Time and snapshots}

As a process extends over time and potentially describes multiple systems, we want to organize the statements into partial domains, one for each moment in time relative to a particular system. Given a \textbf{time parameter} $t \in T \subseteq \mathbb{R}$, we can imagine to carve a \textbf{system domain} $\tdomain_t$ that would correspond to all the statements about the system one can make at the given time. In particular, we define a \textbf{one-step processes} one that only has an \textbf{initial domain} $\tdomain_{t_0}$ and a \textbf{final domain} $\tdomain_{t_1}$. Note that $\tdomain_t$ is a $\sigma$-complete Boolean sub-algebra of $\tdomain[P]$ since any logical operation on statements at fixed $t$ is also another statement at fixed $t$. The set $X_t \subset \tdomain_t$ of the narrowest statements will give the most precise description at that time, we call these the  possible \textbf{snapshots} of our system at that time. A \textbf{trajectory} is a sequence $x(t)$ of snapshots at each time. A particular evolution will identify a trajectory: that is, given $\stmt[e] \in E$ we can find $x(t)$ such that $\stmt[e] \narrower x(t_0) \AND x(t_1) \AND x(t_2) \AND ... $. The converse is not true: a single trajectory may correspond to multiple evolutions, since the evolutions potentially describe the same system at greater accuracy and/or other systems.

In fact, we should note that, technically, the statements are not defined at an infinitesimal moment $t$, rather within a finite interval $[t, t + \Delta t]$. This represents the time-scale at which the process will be described, meaning the description within $\tdomain_t$ should not be sensitive to what happens at faster scales. The quantities used to represent states at time $t$, then, should really be thought as averages within $\Delta t$. In the same way, $\tdomain_t$ will in general represent the system at the chosen level of description, not the narrowest level possible. Statements about the environment or at a finer level of description (i.e. the positions of all molecules for a gas) will not be included.

We should also note that the set of possible snapshots $X_t$ may not be the same at all times and may not correspond to the state of the system. First of all $X_t$ will only include those configurations that are compatible with at least by one possible evolution (i.e. $A(x) \neq \emptyset$ for all $x \in X_t$), which may change in time. For example, the set of possible configurations for a system under dissipative processes will shrink as they converge to equilibrium. In quantum mechanics, the possible configurations after a measurement are restricted to the eigenestates, and a different choice of measurement will lead to a different process with different possible evolutions. Secondly, the best possible description may be broader than the full state of the system (e.g. when external interference forces us to give only a statistical account within our $\Delta t$) or narrower (e.g. when there are known correlations to the other degrees of freedom of the environment or of the system itself). In other words, the state space of the system (i.e. the set of all possible complete descriptions of the system and only about that system, independently of the process at hand) is something we'll construct later.\footnote{In other words, starting directly from a state space comes with a set of implicit assumptions about the system which are at the foundations of thermodynamics specifically and physics more in general.}

We say a process is \textbf{deterministic} over the system domains $\{\tdomain_t\}_{t \in T}$ if given the snapshot at one time we can always predict the snapshot at all future times. Formally, let $t_1, t_2 \in T$ such that $t_1 \leq t_2$, for any $x_1 \in X_{t_1} \subset \tdomain_{t_1}$ we can find an $x_2 \in X_{t_2} \subset \tdomain_{t_2}$ such that $x_1 \narrower x_2$. Intuitively, all the evolutions that pass through $x_1$ will also pass through $x_2$. If $x(t)$ is the snapshot trajectory in time, we have $A(x(t)) \subseteq A(x(t + \Delta t))$: the set of compatible evolutions must stay the same or become larger.

Conversely, we say a process is \textbf{reversible} over the system domains $\{\tdomain_t\}_{t \in T}$ if given the snapshot at one time we can always reconstruct the snapshot at all past times. Formally, let $t_1, t_2 \in T$ such that $t_1 \leq t_2$, for any $x_2 \in X_{t_2} \subset \tdomain_{t_2}$ we can find a $x_1 \in X_{t_1} \subset \tdomain_{t_1}$ such that $x_2 \narrower x_1$. In this case, all the evolutions that pass through $x_2$ must have passed through $x_1$. If $x(t)$ is the snapshot trajectory in time, we have $A(x(t)) \subseteq A(x(t - \Delta t))$: the set of compatible evolutions must stay the same or become smaller.

A deterministic and reversible process, then, will mean $x_1$ and $x_2$ are equivalent and that  $A(x(t)) = A(x(t + \Delta t))$: the set of evolutions remains the same. The fact that past and future snapshots are equivalent $x(t) \equiv x(t + \Delta t)$ does not mean they are the same statement in terms of the description at each time: it means there is an if-and-only-if relationship between the descriptions at different times. For example, \statement{at time $t$ the position is $q$ and the velocity is $v$} if and only if \statement{at time $t + \Delta t$ the position is $q + v \Delta t$ and the velocity is $v$}. The differential equation $\dot{v} = 0$ is a short hand for all such relationships.

If we limit ourselves to deterministic processes, with these few definitions, we have already found that the set of possible evolutions ``grows bigger'' under irreversible evolution while it remains the ``the same size'' under reversible evolution. This is the core idea of the second law.

These definitions naturally link to the idea of a dynamical system and clarify a critical issue. Suppose we have a deterministic process and we pick $t_1, t_2 \in T$. For each $x_1 \in X_1$, the final snapshot $x_2 \in X_2$ must be unique as all snapshots in $X_2$ are incompatible with each other. Therefore we can write a function $f : X_1 \to X_2$ such that $x_1 \narrower f(x_1) \equiv x_2$ that describes that particular step in the process. We call $f$ the \textbf{law of evolution}. Conversely, if the system is reversible, we would be able to find $g : X_2 \to X_1$ such that $x_2 \narrower g(x_2) \equiv x_1$.  We call $g$ the \textbf{law of (inverse) evolution}. Therefore \textbf{a law of (possibly inverse) evolution can always be used to fully characterize deterministic and/or reversible processes}.

We want to stress that if $f$ is invertible, the system is \emph{not} necessarily reversible. In fact, we'd have $x_1 \equiv f^{-1}(x_2) \narrower x_2$, which is not the same as $x_2 \narrower g(x_2) \equiv x_1$: narrowness is in the opposite direction. In other words, \textbf{an invertible deterministic system is not necessarily reversible}. For example, consider a damped harmonic oscillator: the dynamics is in principle invertible (we can reconstruct the trajectory) but it is not reversible (as the evolutions bunch together, finite precision knowledge of the initial conditions gives us more information of the same finite precision knowledge of the final conditions). These types of distinctions are crucial we'd like to stress how it emerges naturally within the framework.

TODO: picture with deterministic evolution merging "streams" of evolutions, and detrev evolutions not merging

\begin{table}[h!]
	\centering
	\begin{tabular}[h]{|c|c|p{6cm}|}
		\hline 
		Symbol & Name & Meaning \\ 
		\hline 
		$t \in T$ & Time parameter & Time is treated as a real valued parameter within the range $T \subseteq \mathbb{R}$ \\ 
		\hline 
		$\tdomain_t \subset \tdomain[P]$ & System domain & The set of descriptions at a given time \\ 
		\hline 
		$X_t \subset \tdomain_t$ & Possible snapshots & The set of descriptions that give a full account at time $t$; they correspond to the narrowest statements within $\tdomain_t$ \\ 
		\hline 
		$x(t) \narrower x(t + \Delta t)$ & Determinism & Given the snapshot at one time we can predict the snapshot at future times \\ 
		\hline 
		$x(t) \narrower x(t - \Delta t)$ & Reversibility & Given the snapshot at one time we can reconstruct the snapshot at past times \\ 
		\hline 
	\end{tabular} 
	\caption{System domain, snapshots, determinism and reversibility}
	\label{table:states}
\end{table}

\subsection{Granularity and quantifying evolutions}

Next step is to quantify the granularity of the description provided by each statement. Since statements at a finer level of description will put a greater constrain on the set of compatible evolutions, quantifying the granularity of the description of $\stmt$ means quantifying the size of $A(\stmt)$. The statement \statement{at time 0 s the particle is between 0 m and 1 m} is doubly more specific than \statement{at time 0 s the particle is between 0 m and 2 m} precisely because it correspond to half the evolutions of the second statement.

The naive approach would be to assign a measure over $A(\tdomain[P])$: since this is is a $\sigma$-algebra over $E$ and a measure is exactly one uses to give a size to each set, it would seem we are done. Unfortunately it is not that simple: a single measure can only compare objects of the same dimensionality. If we imagine all evolutions to be points on an $n$ dimensional manifold, if the measure gives finite values for $k$-dimensional regions it will necessarily give measure zero for all regions with lesser dimensionality and infinite measure for those with greater dimensionality. Coarse grained and subsystem descriptions will have different dimensionality as they describe different numbers of degrees of freedom, therefore we need a way to quantify objects at all levels.\footnote{Typically one uses geometrical structure for his purpose. In phase space, the symplectic form allows us to define areas of two-dimensional surfaces, and use those to define volumes or even-dimensional areas. Note that it does not quantify the size of the points, which are implicitly assumed to be of equal size. This is a special case and cannot be the starting point.}

Therefore our starting point will be the ability to compare two statements and decide which one gives a refined description of the process. Formally, we have partial order $\finer$ on $\tdomain[P]$ which, given two statements, it tells us whether the description of one is \textbf{finer}, more refined, than the other. Two statements are \textbf{equigranualar}, noted $\stmt_1 \eqgran \stmt_2$, if they provide the same level of description. For example, we can say that \statement{the trajectory of the particle is $y=t \cdot 1 \, m/s$} $\eqgran$ \statement{the trajectory of the particle is $y=t \cdot 2 \, m/s$} and that \statement{the position at time 0 sec is between 0 and 1 m} $\eqgran$ \statement{the position at time 0 sec is between 1 and 2 m}, even though the second pair is infinitely less discerning than the first. Note that if one statement is narrower than the other (i.e. $\stmt_1 \narrower \stmt_2$) then it is also finer than the other (i.e. $\stmt_1 \finer \stmt_2$) while the converse is not necessarily true. 

We say two statements are \textbf{comparable} if one is finer than the other. The order is partial because not all statements are comparable to each other. For example, consider the pressure/volume state space for an ideal gas. Comparing \statement{the pressure is 1 kPa and the volume is between 1 and 2 liters} and \statement{the pressure is between 1 and 2 kPa and the volume is 1 liter} would mean saying where one unit liter is bigger or smaller than 1 kPa.\footnote{In phase space, for example, we cannot in general compare $q^i$ and $p_i$, yet we can always compare areas of phase space.}

The idea is that geometrical, measure theoretic, information theoretic and probabilistic structures all descend from this more fundamental structure, the partial order that describes this ``information granularity''. We note that all these structures can be used to determine a size and therefore a partial order on the set. It would be possible, given sufficient conditions, to recover those structure from the order itself. The mathematical detail of how this would exactly work (i.e. what condition are necessary and sufficient) is out of the scope of this work. Here we are interested in how the physical content attached to the partial order give rises to the other structure.

To recover measures, we pick a statement $\stmt[u] \in \tdomain[P]$ and construct a \textbf{measure} $\mu_{\stmt[u]}$ such that $\mu_{\stmt[u]}(\stmt[u]) = 1$ and $\stmt_1 \finer \stmt_2$ will mean $\mu_{\stmt[u]}(\stmt_1) \leq \mu_{\stmt[u]}(\stmt_2)$. That is, we can quantity how more or less precise a statement is compared to a fixed statement taken as a unit. If two statements are such that the measure of one with respect to the other is finite and non-zero, we say they are \textbf{finitely comparable}. We can show that if $\stmt[u]_1$ and $\stmt[u]_2$ are finitely comparable we have $ \mu_{\stmt[u]_1}(\stmt) = \mu_{\stmt[u]_1}(\stmt[u]_2) \mu_{\stmt[u]_2}(\stmt)$. Furthermore, since all evolutions give a complete description of the whole process, we will assume they are all equigranular. Therefore if we pick $\stmt[u] \in E$, then $\mu_{\stmt[u]}(\stmt)=\#(A(\stmt))$ would be the counting measure of evolutions compatible with $\stmt$.\footnote{We still needs to understand the necessary and sufficient conditions under which such measures would exist and be unique. For the present work, we assume it can be done. We know that it is a necessary condition that all evolution must be equigranular. It remains an open question is whether it is sufficient.}

We can also define the \textbf{evolution entropy} with respect to $\stmt[u]$ the quantity $S_{\stmt[u]}(\stmt) = \log \mu_{\stmt[u]}(\stmt)$. This can be thought as the number of bits, yes/no questions, that separate the level of description of $\stmt[u]$ from the one provided by $\stmt$. Connecting the definitions from above, under a deterministic process we have $S_{\stmt[u]}(x(t)) \leq S_{\stmt[u]}(x(t + \Delta t))$ since $x(t)\narrower x(t+\Delta t)$ means $x(t) \finer x(t+\Delta t)$ and $\mu_{\stmt[u]}(x(t)) \leq \mu_{\stmt[u]}(x(t+\Delta t))$. If the process is determistic and reversible, we have $S_{\stmt[u]}(x(t)) = S_{\stmt[u]}(x(t + \Delta t))$. In other words, \textbf{evolution entropy cannot decrease for a deterministic process and is stationary if the process is reversible as well}.

Quantifying evolutions allows us to define probability as well. If we consider all the evolutions compatible with \statement{the coin is fair}, we must have that half of them are compatible with \statement{the coin is head} while the other are compatible with \statement{the coin is tails}.  Therefore we define that the \textbf{probability} of $\stmt_2$ given $\stmt_1$ is
\begin{equation}
	P(\stmt_2 | \stmt_1) = \mu_{\stmt_1}(\stmt_1 \AND \stmt_2) = \frac{\mu_{\stmt[u]}(\stmt_1 \AND \stmt_2)}{\mu_{\stmt[u]}(\stmt_1)}
\end{equation} where $\stmt_1 \AND \stmt_2$ is the logical conjunction (AND) between the two statements and $\stmt[u]$ is finitely comparable to $\stmt_1$. In other words, the probability quantifies the ratio of evolutions compatible with $\stmt_1$ that are also compatible with $\stmt_2$. This can be shown to satisfy the standard axioms of probability.

Suppose that, given a process, we pick $t_1, t_2 \in T$. Given an initial description $\stmt_1 \in \tdomain_{t_1}$ and a final description $\stmt_2 \in \tdomain_{t_2}$, the \textbf{forward transition probability} $P(\stmt_2 | \stmt_1)$ tell us what is the fraction of evolutions starting in $\stmt_1$ (i.e. are compatible to $\stmt_1$)) that end in $\stmt_2$. In the same way, we can look at the \textbf{backwards transition probability}. The probability space is enough to recover the process, which means \textbf{forward and backward transition probabilities can always be used to fully characterize a process.}

We say that a process is \textbf{isoentropic} if for any possible trajectory $x(t)$ we have $S_{\stmt[u]}(x(t)) = S_{\stmt[u]}(x(t + \Delta t)) \neq 0$. We saw that all deterministic and reversible processes are isoentropic, but the converse is not true. We can give, though, a characterization in terms of transition probabilities. Assume $\stmt[u]$, $x(t)$ and $x(t+\Delta t)$ are all finitely comparable. We can write:
\begin{align*}
	\mu_{\stmt[u]}(x(t) &\AND x(t+\Delta t)) \\
	&=  \mu_{\stmt[u]}(x(t)) P(x(t)|x(t+\Delta t)) \\
	&=  \mu_{\stmt[u]}(x(t+\Delta t)) P(x(t+\Delta t)|x(t)) \\
\end{align*}
Therefore
\begin{equation}
\frac{\mu_{\stmt[u]}(x(t))}{\mu_{\stmt[u]}(x(t+\Delta t))} = \frac{P(x(t+\Delta t)|x(t))}{P(x(t)|x(t+\Delta t))}.
\end{equation}
If the forward and backward probabilities are the same, the initial and final snapshots have the same measure. Conversely, if the initial and final have the same measure, the forward and backward probabilities must be the same. Equal measure corresponds to equal process entropy. Therefore \textbf{isoentropic processes are those and only those for which backward and forward probabilities are the same}.


%Note that this approach resolves a problem that causes friction between quantum mechanics and standard probability theory. Given two incompatible states $\psi$ and $\phi$, that is $0 < | \left<\psi | \phi \right> |^2 < 1$, we would like to say that that the probability of one given the other is $P(\psi | \phi) = | \left<\psi | \phi \right> |^2$.

\begin{table}[h!]
	\centering
	\begin{tabular}[h]{|c|c|p{6cm}|}
		\hline 
		Symbol & Name & Meaning \\ 
		\hline 
		$\finer$ & finer & A statement is finer than another $\stmt_1 \finer \stmt_2$ if it provides a more refined description than the second \\ 
		\hline 
		$\eqgran$ & equigranular & A statement is equigranular to another $\stmt_1 \eqgran \stmt_2$ if they provide the same level of detail \\ 
		\hline 
		$\mu_{\stmt[u]}(\stmt)$ & Measure & Quantifies the precision of a statement $\stmt$ in terms of a reference unit statement $\stmt[u]$; it quantifies how many evolutions are compatible with $\stmt$ compared to the evolutions compatible with $\stmt[u]$  \\ 
		\hline 
		$S_{\stmt[u]}(\stmt)$ & Evolution entropy & Defined as $\log \mu_{\stmt[u]}(\stmt)$ represents the number of bits that separate the level of description of $\stmt$ from the one provided by the unit statement $\stmt[u]$; it quantifies the number of questions needed to go from the level of description of $\stmt$ to the level of description of $\stmt[u]$  \\ 
		\hline 
		$P(\stmt_2 | \stmt_1)$ & Probability & Defined as $\mu_{\stmt_1}(\stmt_1 \AND \stmt_2)$ quantifies the ratio of evolutions compatible with $\stmt_1$ that are also compatible with $\stmt_2$  \\ 
		\hline 
		 & Comparable & Two statements are comparable if one is finer than the other \\ 
		\hline 
		 & Finitely comparable & Two statements are finitely if the measure of one in terms of the other is finite and non-zero \\ 
		\hline 
	\end{tabular} 
	\caption{Granularity, measures and probability}
	\label{table:states}
\end{table}

\subsection{States and system independence}

So far we have made no guarantees as to whether each domain $\tdomain_t$ and its snapshots $X_t$ contains \emph{all} descriptions that refer \emph{only} to a particular physical object. Some snapshots may or may not be available at a particular time and/or there may be correlations with other systems. We want the state space to consist of all the possible descriptions of a system, and only of that system, at any point in time for any process. This desire comes with some hidden requirements that we want to bring to light.

First of all, when we say ``all processes'' we can't really mean absolutely all of them, but only the one for which the system is well defined. For example, we can study the motion of a balloon on earth at 20 Celsius. This would not be possible on the surface of the sun as our balloon would not exist. The process has to be such that the system is recognizable to be the same, our descriptions must still make sense. Mathematically, we have a \textbf{state domain} $\tdomain$ of all statement fragments like \statement{the position of the ball is between 2 and 3 meters} together with a surjective map $\iota : \tdomain \times T \twoheadrightarrow \tdomain_t$ that adds \statement{at time t} for each possible time. The map $\iota$ must be a surjection that preserves the logical structure (i.e. logical operations, verifiability, narrowness, compatibility, ...) so that the basic logical relationships determined by the system itself are are valid at all times (i.e. \statement{at time t the position of the ball is between 2 and 3 meters} $\narrower$ \statement{at time t the position of the ball is between 0 and 300 meters} for every $t$).\footnote{We stress that the mathematical requirements are necessary but not sufficient. While the same system must map to the same logical structure but the same logical structure does not necessarily map to the same system (e.g. two distinct system can be represented by the same mathematical structure).}

Note, though, that $\iota( \cdot, t_0)$ in general is not an isomorphism (i.e. a bijection that preserves the logical structure) for two reasons. The first is that not all states will be available at every moment in every process. As we said before, if the process is dissipative the set of possible states will become smaller. In that case, $\iota$ will map all the inaccessible states to $\contradiction$, as they will be impossible. Therefore $\iota$ is not in general a bijection. The second reason is that the statement fragments in $\tdomain$ represent only descriptions about the system itself and nothing else. In a particular process, however, knowing the state of one system might tell us something about other system as well. The presence of correlations/coupling may mean that the precise knowledge of the degrees of freedom of the system may allow us to tell something about the environment or the internal structure (i.e. microstate). Mathematically, $\iota$ may map to statements that are narrower than the original fragment (e.g. we can have $\iota($\statement{position of A is 1 meter}$, t_0) \equiv$ \statement{at time $t_0$ the position of A is 1 meter and the position of B is -1 meter}).

In physics terms, the construction of the state domain requires us to be able to identify and separate the system from all other systems and the environment. We can talk about a book and its state because we can, in many situations, describe and manipulate the book without describing or manipulating other books, the bookcase or the table. If $\tdomain_{t}$ is the domain of our system at time $t$ and $\tdomain[O]_t$ is the domain for another system, we say that the two domains are \textbf{independent} if, taken any two statements $\stmt_d \in \tdomain_{t}$ and $\stmt_o \in \tdomain[O]_{t}$ are independent. In this case there are no correlations: every possible pair of snapshots of each domain is a possible snapshot for the combined system. Therefore if we take $\stmt[u]_d \in \tdomain_{t}$ and $\stmt[u]_o \in \tdomain[O]_{t}$ to be unit statements for the respective measure, we have:
\begin{equation}\label{entropy_sums}
\begin{aligned}
	\mu_{\stmt[u]_d \AND \stmt[u]_o}(\stmt_d \AND \stmt_o)=\mu_{\stmt[u]_d}(\stmt_d) \mu_{\stmt[u]_o}(\stmt_o) \\
S_{\stmt[u]_d \AND \stmt[u]_o}(\stmt_d \AND \stmt_o)=S_{\stmt[u]_d}(\stmt_d) + S_{\stmt[u]_o}(\stmt_o)
\end{aligned}
\end{equation} 
That is, \textbf{for independent systems the process entropy can be seen as the sum of the entropy of each system}.\footnote{Note the direction in which the construction unfolds. We are not defining the entropy on the single systems and then, if they are isolated, the entropy sums. It's the opposite: the entropy is initially defined on the joint system for a particular process as the evolutions describe all systems. If the system is independent of all others, the evolutions are grouped such that their measure factorizes, and we can assign a unique measure to each statement of the independent system, as this will not depend on the description of other systems. If we cannot find this situation, the entropy of the system cannot be defined uniquely on the separate system, but only on the joint. In physics terms, we are unable to treat the description as set of independent degrees of freedom (e.g. describing only the pressure of a gas without the volume, or only the position without the momentum).}

We say we have an \textbf{independent system} if the system is independent from all other systems. In this case, the snapshot is a description of the system and only of the system. The state domain, then, represent descriptions of the system when it is independent. A \textbf{state} $x$ of a system is a snapshot in these conditions and the \textbf{state space} is the set of all possible states.\footnote{There is still the added complication that, even if the system is independent, not all states may be accessible. For example, the thermodynamic states may change depending on the constraint applied to the system (e.g. presence or absence of a magnetic field); in quantum mechanics, the accessible states after a spin measurement will only be the ones along the measured direction. So, in general, the state space stitches together all the possible independent snapshots at all times over all possible processes.} In the same way, the process entropy becomes a property of the system itself and we can define the \textbf{state entropy} $S_{\stmt[u]} : X \to \mathbb{R}$ as the process entropy in these conditions, where $\stmt[u] \in X$ is a state taken as reference. That is:
\begin{equation}
	S_{\stmt[u]}(x) = S_{\iota(\stmt[u], t_0)}(\iota( x, t_0))_{indep}
\end{equation}

We say $\xi^a : X \to \mathbb{R}$ are a set of state variables if they uniquely identify a state. That is, we can find an isomorphism (a bijection that preserve the logical structure) $\phi : \mathbb{R}^n \to X$ such that $\phi(\xi^a(x)) = x$ for all $x \in X$. Then each state space comes equipped with a \textbf{state function} $S=S(\xi^a)$. We'll see that, for thermodynamic systems, this will correspond to the usual thermodynamic entropy. We define a \textbf{purely mechanical} system one for which the state entropy is the same across all states (i.e. $S=k$ where $k$ is a constant, typically chosen to be zero). We'll see that classical phase space and quantum state spaces are state spaces of purely mechanical systems.

Since we can only properly talk about a system when it is independent from the other, what can we do in the other cases? The idea is that we can still talk about a system in that case if we are able, at least in line of principle, to decouple the system from all others while not changing the nature of the system itself. For example, we can talk about a molecule of a gas because we have a way to take such molecule out of the gas. The construction of a state space of a system, then, also requires telling us what all the possible correlated description will go when the system is made independent. Mathematically, we need a deterministic one-step process that takes any initial snapshot of the system to a final state, and does so without disrupting the system (i.e. the relaxation process must take each state to itself). We call this process \textbf{relaxation}.\footnote{The state space is invariant over the relaxation. In other word, the state space is a symmetry of the relaxation process.}

If $x_0$ is the initial snapshot and $x_1$ is the final state, given that the relaxation process is deterministic, we have $x_1= x_1(x_0)$ and 
\begin{equation}
S_{\iota(\stmt[u], t_1)}(x_0) \leq S_{\iota(\stmt[u], t_1)}(x_1) = S_{\iota(\stmt[u], t_1)}(\iota( x, t_1)) = S_{\stmt[u]}(x)
\end{equation}
This means that all snapshots that relax to the same state will have a process entropy lower or equal to the state entropy. Therefore \textbf{the state entropy is the largest possible process entropy for which the description given by the state applies}.\footnote{Mathematically, the relaxation process in needed to compare the process entropies for the correlated descriptions. As the same description can be more or less correlated in different cases, the independent ones work as a reference and we need a way to relate to them.} This does not simply represent a subjective information theoretic consideration (e.g. we know only about the system); it is a physical consideration (e.g. knowledge about one system tells us nothing about the others). As we make the system more independent, there are more possible evolutions the combined system can have, the dynamics of the system is less affected, less correlated, by the dynamics of the other.

The takeaway message is that \textbf{entropy maximization is equivalent to requiring system independence}. Maximizing entropy means taking the system apart from the others, so that its dynamics and description is decoupled. Entropy maximization, more than a law of physics, is a requirement to properly define a part of nature as a system. When we divide nature in independent systems, therefore, we are automatically looking for processes that increase entropy.

\subsection{Thermodynamic processes}





% DO WE NEED INTERNAL STATE? On the other side, a state of a system is never really the description of the whole system, but of the part that is relevant to the process at hand. If we study the motion of the balloon, position and velocity may be enough; if we study it as a thermodynamic system, pressure and volume may be enough. The idea is that the state captures the system at a particular level of granularity and the process does not depend, is not sensitive, to the finer dynamics. We say a system is in \textbf{equilibrium} if the state tells us nothing about the 



\section{Math section}

Let $\tdomain$ be the set of all statements about the evolution of the system. This is a $\sigma$-complete Boolean algebra. Let $E \subset \tdomain$ the set of all evolutions, these are the narrowest statements. To each statement $\stmt \in \tdomain$ corresponds a set of evolutions $A(s)$ that are compatible with the statement.

Given two statements $s_1, s_2 \in \mathcal{D}$ we can say whether one is finer than the other $s_1 \finer s_2$. Given any statement $\stmt[u] \in \mathcal{D}$, we have a function $\mu_u : \mathcal{D} \to \mathbb{R}^+\cup \{+\infty \}$ such that:
\begin{enumerate}
	\item $\mu_u$ is a measure
	\item $\mu_u(u) = 1$
	\item $\mu_u(s_1) \leq \mu_u(s_2)$ if $s_1 \finer s_2$
\end{enumerate}
The value $\mu_u(\stmt)$ tells us how many trajectories are compatible with $\stmt$ in terms of how many trajectories are compatible with the unit statement $\stmt[u]$.

The conditional probability $P(\cdot | \cdot) : \tdomain \times \tdomain \to [0,1]$ is defined as $P(\stmt_1 | \stmt_2) = \mu_{\stmt_2}(\stmt_1 \AND \stmt_2)$ and represents the fraction of trajectories that are compatible with $\stmt_2$ that are also with $\stmt_1$.

We define $T \subseteq \mathbb{R}$ to be the range of the time parameter. We define $\tdomain_{t} \subseteq \tdomain$ as the set of statements defined at the time t. Note this does not mean they are statement of only what happens at an instant in time. They can be averages around. It's how we decompose the problem at the desired time resolution. The states at time $t$ are the possibilities of $\tdomain_t$. The process is deterministic over the chosen states if for each $x_0 \in \tdomain_{t_0}$ we can find an $x_1 \in \tdomain_{t_1}$ such that $x_0 \narrower x_1$.  The process is reversible over the chosen states if for each $x_1 \in \tdomain_{t_1}$ we can find an $x_0 \in \tdomain_{t_0}$ such that $x_1 \narrower x_0$.

Things I want to prove:

\begin{prop}
	Let $\tdomain[P]$ be a deterministic process over $\{X_t\}_{t \in T}$. Then $S(x(t))$ is a monotonically increasing function.
\end{prop}

\begin{prop}
	Let $\tdomain[P]$ be a reversible process over $\{X_t\}_{t \in T}$. Then $S(x(t))$ is a monotonically decreasing function.
\end{prop}

\begin{prop}
	Let $\tdomain[P]$ be a deterministic and reversible process over $\{X_t\}_{t \in T}$. Then $S(x(t))$ is constant.
\end{prop}

\begin{prop}
	Let $\tdomain[P]$ be a deterministic process over $\{X_t\}_{t \in T}$. Then $S(x(t))$ is a monotonically increasing function.
\end{prop}

\bibliography{bibliography}


\end{document}