\documentclass[10pt,twocolumn, nofootinbib]{revtex4-2}

\usepackage{assumptionsofphysics}
\usepackage{tikz}
\usepackage{breakurl}
\usepackage{tcolorbox}

\newcommand\mix{\mathrm{mix}}
\newcommand\component{\mathrm{comp}}
\newcommand\cospan{\mathrm{cospan}}
\newcommand\dist{\mathrm{dist}}
\newcommand\hull{\mathrm{hull}}
\newcommand\support{\mathrm{supp}}
\newcommand\capacity{\mathrm{scap}}
\newcommand\fraction{\mathrm{frac}}
\newcommand\frcap{\mathrm{fcap}}

\newcommand{\ens}[1][e] {\mathsf{#1}} % Ensemble
\newcommand{\Ens}[1][E] {\mathcal{#1}} % Ensemble space

\def\ortho{\perp}
\def\northo{\nperp}
\def\separate{\downmodels}
\def\nseparate{\ndownmodels}

\def\>{\rangle}
\def\<{\langle}

\begin{document}

\title{A non-additive generalization of probability theory \\for quantum mechanics and beyond}
\author{Gabriele Carcassi}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}
\author{Christine A. Aidala}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}

\date{\today}


\begin{abstract}
	We present a physically motivated generalization of probability theory that is suitable for classical mechanics, quantum mechanics and any future physical theory that allows a statistical description. The goal is to put the use of classical and quantum probability in a broader context, and show how the current mathematical structures are likely not suitable to solve the open problems in the foundations of physics. For the more mathematically inclined, we will point to areas where new math or generalization of established math are needed. For the more philosophically incline, we will point to areas where further conceptual work is needed.
	
	
	%Given a generic space of ensembles, we can define the fraction capacity as the maximum fraction of a particular ensemble that can be understood as a mixture of ensembles from a given set. This gives a non-additive (i.e. fuzzy) measure that reduces to a probability (i.e. additive) measure for classical spaces and for quantum measurements. We can also define the state capacity as the exponential of the maximum entropy reachable by a mixture of ensembles from a given set. This is also a non-additive measure and reduces to the Liouville measure in classical mechanics and the Hilbert space dimensions for subspaces of quantum mechanics. Conceptually, it gives us a notion of probability that is both theory and interpretation independent. Mathematically, it gives us a measure theoretic generalization of probability. Physically, it may allow to find new way to understand current theory and tool to investigate new ones. The purpose of this paper is to show the core ideas and present questions that may be developed on the mathematical, physical and philosophical side.
\end{abstract}

\maketitle

\section{TOC}

Overall goal: to create a generalization of probability theory and measure theory for general physical systems.


General question: how do we represent physical states? Different types of representations: talk about examples, ....

-- General problem

For this work: assume we have a locally convex topological vector space V. E is a bounded convex subset of V. Equipped with an entropy. Topology generated by entropic balls?

Separateness and orthogonality

Statistical quantities

Show that they reduce to standard things in classical mechanics and quantum mechanics.

-- Connection to Choquet theory 

Choquet theory - in the case that the closure of the space is compact -> representation through measures. May not be unique

Choquet simplex. Uniqueness of decomposition and separateness/orthogonality.

-- Connection to signed probability

TODO: literature search on use of signed measure in QM. ( e.g. \href{https://arxiv.org/pdf/2302.00118}{this paper})

Characterize signed measures as an affine representation of the ensemble space (i.e. bounded convex subset of a locally convex topological vector space).

Show that the probability is signed if and only if multiple decomposition, non-additive, no Choquet simplex

Signed probability is unique, but does not characterize the convex set (ensemble space). Note: the other measures are defined on the extreme points. Knowing the extreme points means you know the convex space (i.e. take the hull). The non-additive measure does not characterize the extreme points, so it doesn't really characterize the space.

-- Our non-additive measure approach

State capacity

Fraction capacity. Show uniqueness. Also show uniqueness on extreme points.

Fraction capacity as supremum of all possible measures

Additivity of fraction capacity (and state capacity).

Single decomposition and classical spaces and measurement contexts

Show that they reduce to standard things in classical mechanics and quantum mechanics.

-- Connection to fuzzy logic?

Literature search.

Possible connection to
* Choquet integral
* Sugeno integral

? Space of fuzzy measure like we have a space of additive measures?


\section{Introduction}

In the past few decades, there has been a growing interest in developing approaches that generalize the notion of states and physical theories. One basic insight is that statistical/probabilistic concepts often provide a suitable basis for this generalization, and it is in this setting where most of these approaches operate. A key problem, then, is what notion of probability should be used given that the standard measure theoretic Kolmogorovian approach does not generally work in quantum mechanics.

We reformulate the problem in the following way: given the space of all possible statistical ensembles allowed by a statistical theory, what measure theoretic tools can we use to represent each ensemble and the geometric properties of the overall space? Why exactly classical probability measures fail to capture completely quantum states? What suitable generalizations are possible, and what mathematical tools would need to be developed?

In this paper, we will characterize a generic space of ensembles as a convex bounded set of a Hausdorff locally convex topological vector space, upon which an entropy function is defined. These mathematical properties are physically grounded in the ability to experimentally define ensemble (leading to a topological space), to define statistical mixtures (leading to a convex space), to quantify the variability of the elements of the ensembles (leading to an entropy function) and to having enough real valued quantities to identify each ensemble (leading to the continuous embedding into a Hausdorff locally convex topological vector space). The question is, then, what measure theoretic tool can quantify the number of states (as in the classical Liouville measure) or the number of possible outcomes of a measurements (as in the dimension of orthogonal subspaces of an operator)? What measure theoretic tool can represent each ensemble? If we are to define a physically meaningful notion of probability density, both of these questions must be answered.

We will first review some results from Choquet theory, which is, to our knowledge, the most general approach to represent elements of a convex set using additive probability measures. The key insight is that, at least in the compact case, such representation is always possible, but it is not unique. Uniqueness requires the space to be a Choquet simplex, which is a generalization of the standard finite-dimensional simplex and is a particularly shaped subset of a vector space. The space of quantum ensembles is not a simplex for one key reason: it allows multiple decompositions in terms of pure states. These insights will allow to better understand exactly the limits of classical probability to describe ensembles in physical theories.

We will also review some results from the use of quasi-probability measures. While the Wigner function represents the most known instance, there are different choices. Note that the space of signed probability measures over a set $K$ is a vector space, so one can find an affine map between a suitable space of signed probability measures and the embedding vector space. Each ensemble, then, is uniquely represented with a signed probability measure. However, unlike in the Choquet case, the measure is not supported by the set of extreme points of the ensemble space. That is, there is no unique or privileged choice for this representation. Moreover, since the map is at the level of the full vector space, not just the subset with valid ensembles, there is no general way to characterize which subset of signed probability measures correspond to ensembles. Therefore, while signed measures are a powerful tool for calculation in specific problems, they cannot give us a general principled representation of the physical theory.

The above two approaches exhaust what can be done with additive measures, so we turn to the non-additive case. From the entropy, we can define the state capacity, which generalizes the notion of count of states. For each ensemble, we can define the fraction capacity, which generalizes the notion of probability. Both are non-negative sub-additive monotonic set functions, with the second being unit bounded. These measures are always unique and, since they are defined on the ensemble space itself, always have provide a physically meaningful representation. Moreover, in the compact case, these measure can be restricted on the extreme points while retaining uniqueness. (TODO: prove)

The fraction capacity is the supremum of all possible measures for the same ensemble. The classical case exactly corresponds to the simplex case, in which fraction capacity corresponds to a unique measure, and is therefore additive. The state capacity, instead, corresponds to the minimum of the entropy of all possible representations. It recovers the Liouville measure in the classical case and the dimensionality of the Hilbert space in the quantum case.

Having shown that a general approach based on non-additive measures is possible, we lay down a series of questions to explore in the future. The biggest one is whether it is possible to generalize notions such as integration, derivation and expectation to this non-additive case.  It should be noted that non-additive measures are used in the context of decision theory, so there may be a connection between the use of non-additive measures in quantum theory and interdependent complimentary choices.[TODO: find citation that sub-additive is when choices are complementary] Both the Choquet and the Sugeno integral, for example, are defined over a monotonic set-functions, so there is a chance that these tools may turn out to be useful. We see this work as starting this line of research, which will take more effort to investigate.

A final remark before we start. Too often works in the foundations of physics focus on a particular aspect without concern to a broader connection to all relevant mathematical, physical and philosophical aspects. Therefore, it is important to us that this work is a piece of a much larger puzzle, which connects cleanly and directly to  other areas, such as information geometry, functional analysis, quantum logic and so on. It is the hallmark of a true fundamental structure for physics that such connections can be easily made. Unfortunately, the nature of scientific publication requires us to publish a narrow slice of the work to fit the scope of a particular journal. Ironically, this means that the most important motivations for our approach are out of scope of this article, as we will concentrate only to the connections to measure theory.

\section{Ensemble space}

\textbf{Basic requirements for an ensemble space}. It is a cardinal rule in Physical Mathematics that all axioms and mathematical definitions must be grounded in physical requirements. This way we can develop mathematical frameworks that are physically well founded. As part of our larger project, we are developing a general theory of ensemble spaces, the full treatment of which is beyond the scope of this work. We limit ourselves to give the initial premises and the final mathematical characterization, without justifications and proofs.\footnote{More details are available in our ongoing work (cite)}

We start by assuming that physical laws deal with experimentally reproducible relationships, which means that they must describe relationships between statistical ensembles. Conceptually, a statistical ensemble is the collection of outputs, taken at once, of a reproducible preparation procedure for a physical system. In the classical case, this corresponds to a probability distribution over a discrete set of outcomes (e.g. a fair coin) or over phase space (e.g. the symplectic manifold representing position and momentum for a classical particle). In the quantum case, this corresponds to a mixed state (e.g. a positive semi-definite self-adjoint operator of trace one). An ensemble space is the collection of all possible ensembles defined in a physical theory, together with a basic mathematical structure that all physical theories must possess.

An ensemble space $\Ens$ will need to satisfy a three basic requirements. First, it needs to guarantee experimental verifiability. Mathematically, the ensemble space must be a $\mathsf{T}_0$ second countable topological space, where each open set represents an experimentally verifiable statement.\footnote{A verifiable statement is an assertion for which there exists an experimental test that finishes successfully in finite time if and only if the statement is true.} Second, it needs to allow statistical mixtures: if $\ens[a],\ens[b] \in \Ens$ are ensembles, then $p \ens[a] + \bar{p} \ens[b]$ represents the statistical mixture where $\ens[a]$ is taken with probability $p$ and $\ens[b]$ with probability $\bar{p} = 1 - p$. Mathematically, this constrains the ensemble space to be a convex set.\footnote{Only finite mixtures are guaranteed, while the topology will decide whether an infinite mixture $\sum_{i}^{\infty} p_i \ens_i$ converges in the space or not.} Third, an entropy function must be defined that characterizes the variability of the elements within an ensemble.

Additionally, we will require that ensembles can be identified through real valued statistical quantities. \footnote{It is an open question whether all ensemble spaces must possess this property. In a previous work we showed that the dense linear ordering require highly idealized assumptions.} A statistical quantity represents the expectation of a real valued quantity, and is therefore an affine continuous functions $F : \Ens \to \mathbb{R}$. That is, $F(p \ens[a] + \bar{p} \ens[b]) = p F(\ens[a]) + \bar{p} F(\ens[b])$. The last requirement is that there exists a family $F_i$ of statistical quantities that fully identify each ensemble. That is, $F_i(p \ens[a] + \bar{p} \ens[b]) = p F_i(\ens[a]) + \bar{p} F_i(\ens[b])$ and $\ens[a] = \ens[b]$ if and only if $F_i(\ens[a]) = F_i(\ens[b])$ for all $i$.

These requirements interact with each other leading to a rich mathematical structure that connect to many of the structures we use in mathematical physics. In this paper, we will examine only connections to measure theoretic representations.

\textbf{Ensemble spaces as convex sets.} In this work, an ensemble space $\Ens$ is a convex subset of a metrizable second countable locally convex topological vector space $V$.\footnote{The entropy constraints forces the topology to be $\mathsf{T}_1$ the convex space to embed into a vector space. A second countable $\mathsf{T}_1$ TVS is metrizable. The family of statistical quantities induce a family of semi-norms on the space, which must be countable due to second countability, which makes the TVS locally convex.} The entropy is function $S : \Ens \to \mathbb{R}$ that satisfies the following bounds
\begin{equation}
	\begin{aligned}
		p S(\ens[a]) + (1-p) &S(\ens[b]) \leq S(p \ens[a] + (1-p) \ens[b]) \\
		&\leq I(p,\bar{p}) + p S(\ens[a]) + (1-p) S(\ens[b])
	\end{aligned}
\end{equation}
where $I(p,\bar{p}) = - p \log p - \bar{p} \log(\bar{p})$.\footnote{In the larger work, we can show that the Shannon entropy is the only upper bound possible.}  Intuitively, the variability (i.e. the entropy) cannot decrease during mixing, and will have an upper bound when mixing two ensembles whose instances are always distinguishable.

\textbf{Orthogonality and separatedness.} We will say that two ensembles are orthogonal, noted $\ens[a] \ortho \ens[b]$, when their mixture gives a maximal entropy increase. This will recover the notion of orthogonality for both classical and quantum spaces. Another notion of distinction between two ensembles is in terms of the mixtures: two ensembles are separate, noted $\ens[a] \separate \ens[b]$, they cannot be expressed as a mixture of a common ensemble. That is, there is no $\ens[c]$ such that $\ens[a] = \alpha \ens[c] + \bar{\alpha} \ens[d]$ and $\ens[b] = \beta \ens[c] + \bar{\beta} \ens$ from some $\alpha, \beta \in (0,1]$ and $\ens[d],\ens \in \Ens$. In classical spaces, these two notions coincide: two ensembles are orthgonal and separate if and only if the support of the respective probability distributions overlap on a non-measure-zero set. In quantum mechanics, these two notions are different: any two pure states are separate but are not orthogonal.

To summarize, a repeatedly verifiable physical theory requires ensembles, which must be defined experimentally, allow statistical mixing and provide an entropy. Additionally, we require the existence of enough statistical quantities to identify each ensemble. Under these assumptions, an ensemble space $\Ens$ is a convex set of a metrizable second countable locally convex topological vector space $V$. We now turn to the question of how we can represent both the entropy and each ensemble in terms of some measure theoretic structure.

\section{Probability measures and Choquet theory}

\textbf{Classical probability measures.} In the classical case, ensembles are defined as probability measures of the space of all possible states $X$. In the discrete case, like for roll of a die, $X$ is a set endowed with the discrete topology\footnote{Physically, the discrete topology corresponds to the ability to experimentally verify or falsify each case. That is, we are able to recognize whether a particular die landed or didn't land on a particular face.} and, consequently, the power set as the $\sigma$-algebra. In the continuous case, $X$ will be classical phase space endowed with the usual topology of a manifold\footnote{Physically, the topology of a manifold corresponds to the ability to label each possible case through a set of continuous variable which are verifiable up to finite but arbitrarily small precision. That is, we are able to tell whether a state is within a particular range of position and momentum.} and the standard Borel sets as the $\sigma$-algebra. In either case, a probability measure is a map $p : \Sigma_X \to [0,1]$ that assign to each event of the $\sigma$-algebra (i.e. to an empirical statements) the probability to find it to be true. The probability measure is additive, meaning that if two events $A, B \in \Sigma_X$ are mutually exclusive (i.e. $A \cap B = \emptyset$) then the probability of the union is the sum of the probability of the respective events:
\begin{equation}
	p(A\cup B) = p(A) + p(B).
\end{equation}

\textbf{Absolute continuity of probability measures.} While each classical ensemble is a probability distribution, not all probability distribution can be thought as a valid ensemble. For example, we could have a probability measure to be wholly defined over a single point of phase space, which would correspond to the ability to prepare a system in the same exact place with the same exact momentum. Not only this is not feasible experimentally, which violates the whole premise of having objects that map to physically meaningful entities, but it would have to have entropy equal minus infinity. To address this, we add a requirement that is both mathematically natural and physically meaningful. Let $\mu : \Sigma_X \to [0, + \infty]$ the measure on $X$ that quantifies the count of states in each region. In the discrete case this is simply the counting measure (i.e. three possible faces of the die corresponds to three possible states), while in the continuous case this corresponds to the Liouville measure (i.e. volumes in phase space correspond to the count of states). This measure is additive over mutually exclusive events. The requirement is that we can assign non-zero probability only to events that have a non-zero count of states. That is, if for a particular event $A$ we have $p(A) \neq 0$, then we must have that $\mu(A) \neq 0$ as well. Mathematically, $p$ is absolutely continuous with respect to $\mu$, and this is exactly the case in which a probability density $\rho = \frac{dp}{d\mu}$ can be defined.\footnote{In the general case, we can still assign non-zero probability to a point of a manifold, provided that the topology and the measure $\mu$ consistently allow for it. Physically, it means that those points are ``special'' both in terms of experimental verifiability and entropy/count of states. While the general framework allows for it, it is not crucial for our discussion here.} This is another example in which a physical requirement (probability must be zero if there are no possible states), once properly spelled out (absolute continuity), justifies common physical intuition (we assume there is a probability density).

% Intro paragraph that says quickly how to connect to Choquet theory and its limitations. It should be a quick summary of the whole section

\textbf{Choquet theory.} Having reviewed the use of probability measures in classical mechanics, we can ask the more general question: when can we represent ensembles in a generic ensemble space as a probability measure? A very similar question is the subject of Choquet theory.

First, we have to define what it means to represent an ensemble with a probability measure. As a first step, let $E$ be a convex subset of a locally convex space $V$. We say that $\ens \in E$ is represented by a probability measure $p : \Sigma_{E} \to [0,1]$ if $f(\ens) = \int_E f dp$ for every continuous linear functional $f$ on $V$. Physically, instead of describing our ensemble $\ens$ as a point in our ensemble space identified by the value of all possible statistical variables $f(\ens)$, we represent it as a probability measure $p$ and all the associated expectation values.

In this first step, we are using a probability measure on $E$, so we are defining a probability distribution over ensembles. As a second step, we want the probability measure to be defined over the possible cases, the sample space. To do that, Choquet theory identifies each possible case with the Dirac measure associated to that case, which is an extreme point of the convex set $E$. The result is the following:
\begin{thrm}[Choquet]
	Let $E$ be a metrizable compact convex subset of a locally convex space $V$, and let $\ens \in E$. Then there exists a probability measure $\mu_{\ens}$ on $X$ which represents $\ens$ and is supported by the extreme points of $X$.
\end{thrm}

Note that while an ensemble space $\Ens$ is a metrizable convex subset of a locally convex TVS $V$, it is not necessarily compact. As we saw, the Dirac measures should not be considered valid ensembles in the continuum case. Therefore Choquet theory cannot be applied to our general case. When we are drawing the parallel, we will implicitly assume that $E$ is a compact set that is also the closure of $\Ens$ in an appropriate convex TVS $V$. Many interesting cases fall into this restriction: the ensemble spaces of classical probability distribution over finitely many discrete cases fall in this restriction; the subset of ensembles over finite ranges of position and momentum; the ensemble spaces of all finite dimensional quantum systems. When Choquet theory applies, then, we are able to always characterize an ensemble in terms of a probability distributions over its, maybe idealized, most refined ensembles.

The previous theorem does not, however, guarantee uniqueness. The above theorem, in fact, applies in the quantum case. The Bloch ball, in fact, is a compact convex subset and any point can be seen as a convex combination of points on the surface. The center, which represent the maximally mixed state, can be thought as an equal mixture of any two opposite points, or a uniform distribution over the whole surface. The fact that any ensemble can be seen as a mixture of a set of pure states, then, is not a property of classical spaces. The property of classical spaces is that this representation is unique.

Mathematically, the probability measure is unique if and only if $E$ is a Choquet simplex, which is a generalization of the finite dimensional simplex. The characterization of the Choquet simplex is fairly technical, so we will simply define the Choquet simplex by the property of having a single decomposition in terms of extreme points. The classical case, that is when the ensemble space coincide with the space of all probability measures over a set of states, is a Choquet simplex.

% space is metrizable https://proofwiki.org/wiki/Birkhoff-Kakutani_Theorem/Topological_Vector_Space

The core insight from Chouqet theory, then, is that both in the classical and quantum case we can represent ensembles as a probability distribution over a set of states. However, only in the classical case this representation is unique. In the quantum case, mixed states can be decomposed into pure states in multiple ways, and it is this multiple decomposition that makes the ensemble space not a simplex, and therefore the space not classical. Unique representation of ensembles is a desirable property, and standard probability measures are not going to provide it in general.

\section{Quasiprobability}

%Goal of this section: quasi-probability representations are allowed by the embedding in a topological vector space. Choice of representations are guided by what observables are interesting for a specific problem.

%Examples: Wigner, Husimi, Glauber P, SIC-POVM (space not made of spectra of operators), Kirkwood-Dirac (two-time correlation function - observables not necessarily taken at the same time )

%Things to say: some people think about it as weakining axioms (cite papers on negative probability). It's more than that. Points describe events that are not necessarily mutually exclusive. TODO: example of two Wigner function/Husimi Q would be better that have no overlap but they are not orthogonal and/or two functions that have overlap but are orthogonal.

%Thing to say: all these representations are useful because they preserve the linear structure, meaning that convex combinations of ensembles are convex combinations of quasi-probability; expectation values are linear. Check, but it should be true.

% 1 describe quasiprobability approaches; go through examples; point out that the quasi-probability can be negative; that one can recover expectations;

\textbf{Quasi-probability representations.} Another way to represent quantum ensembles is through the use of quasiprobability distributions, of which the Wigner function is probably the most widely known. The Wigner transform $W : \Ens \to L^1(\mathbb{R}^2, \mathbb{R}) $, defined as [CITATION NEEDED]
\begin{equation}
	W(\rho) = W_{\hat{\rho}} (q, p) = 2 \int_{-\infty}^{\infty} \langle x + y | \rho | x - y \rangle e^{-2 i p y / \hbar} \; \mathrm dy,
\end{equation}
takes a mixed state $\rho$ and returns a function of phase space that integrates to one, like a probability density, but can have small negative regions. Recall that an affine combination is a linear combination where all coefficients sum to one but are not necessarily non-negative. Therefore, in general, quasi-probability distributions can be understood as affine combinations. Remarkably, expectation values of polynomial of position and momentum in symmetric ordering, coincide with expectations over the Wigner function. For example
\begin{equation}
	\tr \left( \frac{1}{2}[XP+PX] \rho \right) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xp \; W_{\rho} (q, p) dq dp \; .
\end{equation}
Moreover, the transform is an affine map: $W(p \rho_1 + \bar{p} \rho_2 ) = p W(\rho_1) + \bar{p} W(\rho_2)$.

In some problems, especially in quantum optics, normal ordering and anti-normal ordering are more useful. The Glauber P representation and the Husimi Q representation work like the Wigner function except that the expectations of polynomials recover the respective order. In other circumstances, quasi-probability distributions over observables different from position and momentum are more useful. For example, the Kirkwood-Dirac is used to represent two-time correlation functions over observables defined at different times. Quasi-probability representations are also used over discrete elements that may not even correspond to eigenstates of a particular operator. For example, the SIC-POVM represents quantum states as affine combinations of pure states $P_i$ that are ``equally distant'' from each other, which means that no two elements are orthogonal to each other. Each ensemble $\rho$ can then be represented as
\begin{equation}
	\rho = \sum_{n=1}^{d^2} \lambda_i P_i = \sum_{n=1}^{d^2} \left[ (d^2 + 1) p_i - \frac{1}{d} \right] P_i \; ,
\end{equation}
where $p_i = \tr \left(P_i \rho\right)$. [CITATION NEEDED]

It should be clear that there are many ways to represent quantum states via quasi-probability distributions, and that the choice of representation is a matter of convenience for the problem at hand. All representations, though, have the following features in common. First, we have a map $W : \Ens \to Q(Z,\mathbb{R})$ that given each mixed state $\rho \in \Ens$ returns the corresponding quasi-probability function $w(z) \in Q(Z,\mathbb{R})$, where $Q(Z,\mathbb{R}) \subset L^1(Z,\mathbb{R})$ is the function space used by the specific representation. The space $Z$, in fact, can change from representation to representation: it is $\mathbb{R}^{2n}$ for a Wigner function, $\mathbb{C}^n$ for the Husimi Q and $[1, d^2] \subset \mathbb{N}$ for the SIC-POVM. All representations preserves convex combinations: $W(\sum_i p_i \rho_i) = \sum_i p_i W(\rho_i)$. That is, a statistical mixture of quasi-probability distributions represents the statistical mixture of the corresponding states. Since the map is linear, and expectation values are linear, one can express expectation values as integrals or sums over the quasi-probability $w(z)$. 

% 2 Note that it's not about negative probability; disjoint not mutually exclusive

\textbf{Not about negative probability.} The inclusion of negative probability is often advertised as the main difference between classical probability and quasi-probability. [CITATION NEEDED] Sometimes these negative probabilities are justified as arising from constraints on probability distributions defined on different contexts. This is a mischaracterization. For example, the Husimi Q representation provides us with a non-negative quasi-probability function. That is, the Husimi Q is a non-negative density distribution that integrates to one, yet it is not a classical probability distribution.\footnote{Since the Husimi Q is defined as $Q_{\rho} ( \alpha ) = \frac{1}{\pi} \langle \alpha | \rho | \alpha \rangle$, it is proportional to the probability of measuring a coherent (i.e. Gaussian state) $| \alpha \rangle$, which is non-negative. } Similarly, a SIC-POVM characterizes the Bloch ball by four pure states that form a tetrahedron inscribed in the ball. Since we can also inscribe the Bloch ball inside a tetrahedron, we can represent all states of a two-state quantum system by a convex combination of the vertices, leading to a non-negative quasi-probability distribution over four elements.

Conversely, we can use signed measures to represent classical distributions. Suppose we have a two state classical discrete space, which means the space of all possible ensembles is spanned by $[p, (1-p)]$. Alternatively, we can write it as $p[1,0] + (1-p)[0,1]$ which highlights that we are writing each possible probability distribution in terms of the two possible cases. But we could also express each probability distribution in terms of other two distributions, as in $\left(2p - \frac{1}{2}\right)\left[\frac{3}{4}, \frac{1}{4}\right] + \left(\frac{3}{2} - 2p\right)\left[\frac{1}{4}, \frac{3}{4}\right]$. The coefficients now span between $\left[-\frac{1}{2}, \frac{3}{2}\right]$, can be negative, but still sum to one. That is, in terms of this new affine basis, we need to use a signed probability.

Since we can represent classical states with signed probability measures, and quantum states with non-negative probability measure, the difference between classical probability and quantum quasi-probability does not lie in the use of negative probability. Instead, the crucial difference is that in classical probability all points of the sample space, and therefore disjoint regions, represent mutually exclusive events, and all mutually exclusive events are represented by disjoint regions. Mutually exclusive events and be prepared and observed independently from the others (i.e. they are orthogonal in the sense that they maximize entropy increase during mixture); we can assign some probability to some region without having to assign it to another disjoint region.\footnote{The only constraint is that the probability over the whole space is one.} This is not the case for quasi-probability in quantum mechanics.

For example, the eigenstates $| m \rangle$ of the harmonic oscillator are orthogonal, mutually exclusive events. Their corresponding Wigner functions are
\begin{equation}
	W_m (q, p) = \frac{(-1)^m}{\pi} e^{-(q^2+p^2)} L_m (2(q^2 + p^2)).
\end{equation}
Their support is the whole $\mathbb{R}^2$ since the Laguerre polynomials $L_m$ have finitely many zeros, and therefore are not disjoint. Conversely, in a SIC-POVM we take $d^2$ pure states $P_i$ in a $d$ dimensional space, which means they cannot be all orthogonal to each other. For a Bloch sphere, the four corners of the tetrahedron are not all opposite to each other. Therefore, disjoint sets of $P_i$ do not correspond to mutually exclusive events. 

While negative probability is a red herring, we want to understand two things. When can we use classical probability? Are quasi-probability representation specific to quantum mechanics, or are they more general? Fortunately, the ensemble space framework allows us to answer these questions directly.

% 3 These representations are possible because of the vector space structure of the ensemble space, and therefore something similar will be possible in ANY physical theory

\textbf{Ensemble space embedding and signed measures.} We have already seen that we can represent ensembles using classical probability if and only the ensemble space is a simplex. Since any probability measure is possible, we can assign probability to two disjoint areas independently. This answers the first question.

For the second question, let $Z$ be the space over which we are defining our quasi-probability distributions.
Note that, in general, $Z$ is not the space of states. Wigner functions, for example, are defined over phase space, which does not represent quantum states. Let $M^{\pm}(Z)$ the space of all bounded signed measures over $Z$. Since the linear combination of two bounded signed measure is a bounded signed measure, $M^{\pm}(Z)$ is a vector space. Now let $M_1^{\pm}(Z)$ be the space of signed probability measures. While it is not a subspace of $M(Z)$, it does not include the zero measure, it is an affine subspace therefore, up to the choice of an arbitrary origin, it is a vector space.

Recall that an ensemble space must be a convex subset of a vector space. If we are able to represent ensembles as quasi-probability distribution over $Z$, then, $M_1^{\pm}(Z)$ will be isomorphic to the embedding vector space $V$ as topological vector spaces.\footnote{The question of whether such an $Z$ always exists is still open for us. A possible conjecture is that $Z$ could be constructed from the statistical quantities $F_i$ that fully identify each ensemble in $\Ens$.} In this case, any element $v\in V$ can be represented uniquely by a signed probability measure $\mu_{v}$, regardless of whether it is an actual ensemble or not. Since statistical quantities $F_i$ are linear functional, we will have $F_i(v) = \int_Z F_i(z) d\mu_{v}$. In other words, there is nothing special about quantum mechanics that allows quasi-probability representations. It is the fact that the ensemble space embeds into a vector space, a general fact for all theories, that allows such representations.

It should be clear that, since an ensemble space is a strict subset of the embedding vector space, not all affine measure will correspond to an ensemble space. Whether all probability measures correspond to a valid ensemble, or whether signed probability measures are even used depends on how the mapping is done in each particular case. For example, there is no simple characterization of which Wigner functions correspond to valid quantum states and which do not.

% 4 pros and cons - can choose a repretation for the problem at hand; no fundamental representation; boundaries ore not well defined (i.e. which functions represnt valid states)

\textbf{Non-uniquess of quasi-probability.} 

This alternative way of looking at measure theoretic representations, then, is consistent with the previous one and fits in our general framework. Since quantum ensemble spaces are not simplexes, classical probability does not apply. The different bounds one gets between classical and quantum mechanics depend on the different shape of the respective ensemble spaces.

Quasi-probability distributions are very useful in practice, particularly because one can construct them to fit particular problems. Once one has identified a set of observables that are the most natural ones to define a problem, one can find a representation that is specifically suited for the case. While the quasi-probability cannot be interpreted as probability, it still provides a useful tool to describe and understand the problem.

However, this practical convenience is also a drawback for finding a more general approach to probability in spaces beyond classical mechanics. Crucially, the space $Z$ over which the quasi-probability measure are defined is not unique, and it is not defined by the structure of the ensemble space and, as a consequence, cannot tells us which quasi-probability measure represent valid ensembles and which do not.

As a comparison, in the previous case all probability measures were given over a uniquely defined set of points, the extreme points of the ensemble space, but the probability measure may not be unique. In this case, the pseudo-probability associated to each ensemble is unique, but the space over which the pseudo-probability is defined is not.

\section{Fraction capacity}

TODO: intro

Given that mixing coefficients have a more general applicability than probability of outcomes, let us concentrate on the following problem: given an ensemble $\ens \in \Ens$ and a (Borel) set of ensembles $A \subseteq \Ens$, what fraction of $\ens$ can be constructed by a mixture of $A$? How much of $\ens$ can be explained as coming from preparations corresponding to $A$? For example, let $\Ens$ be the space of all probability distributions for a six-sided die. Let $\ens_{123456}$ be the uniform distribution over all outcomes. Let $A_1 = \{\ens_1\}$ where $\ens_1$ represents outcome one with 100\% probability. Since we can write $\ens_{123456} = \frac{1}{6} \ens_{1} + \frac{5}{6} \ens_{23456}$, where $\ens_{23456}$ represents a uniform distributions over the five outcomes, $1/6$ of $\ens$ can be constructed from $A$, but no more. Similarly, if $A_{12} = \{\ens_{1},\ens_{2}\}$, we can write $\ens = \frac{1}{3} \left(\frac{1}{2} \ens_1 + \frac{1}{2} \ens_2 \right)  + \frac{2}{3} \ens_{3456}$, so $1/3$ of $\ens$ can be constructed from $A$, but no more. Note that, given the uniform distribution, $1/6$ is exactly the probably for the event $A_1$ and $1/3$ the probability for event $A_{12}$. This gives the basic insight for our definitions.

Given a target ensemble $\ens \in \Ens$ and an arbitrary ensemble $\ens[a] \in \Ens$, we define the \textbf{fraction} of $\ens[a]$ in $\ens$ to be
\begin{equation}
	\fraction_{\ens}(\ens[a]) = \sup(\{ p \in [0,1] \, | \, \exists \, \ens_1 \in \Ens \text{ s.t. }  \ens = p \ens[a] + \bar{p} \ens_1 \}).
\end{equation}
The fraction is always well-defined because we can always write $\ens = 0 \ens[a] + 1 \ens$, therefore it must be zero or greater. This quantity tells us how much of ensemble $\ens$ can be constructed from $\ens[a]$. 

We now extend this idea from a single ensemble $\ens[a] \in \Ens$ to a Borel set of ensembles $A \subset \Ens$. Given that the $\hull(A)$ is the set of all possible convex combinations (i.e. mixtures),\footnote{Since we work with convex sets within a topology, the hull is both the closed under the topology and convex combinations. That is, $\hull(A)$ is the smallest closed and convex set that contains $A$} we define the \textbf{fraction capacity} of $A$ for $\ens$ to be
\begin{equation}
	\frcap_{\ens}(A) = \sup(\fraction_{\ens}(\hull(A))\cup\{0\}).
\end{equation}
This returns the biggest fraction of $\ens$ that can be achieved with a mixture of elements of $A$.\footnote{The name fraction capacity is chosen to both signify the ability of the set $A$ to contain $\ens$ (i.e. fraction capacity of one means $\ens$ is within the convex combinations of $A$) and to indicate that it will be a non-additive monotonic measure, which are called ``capacities'' in some literature. TODO \href{ https://link.springer.com/book/10.1007/978-3-319-03155-2}{cite} }

One can then show that the fraction capacity $\frcap_{\ens} : \Sigma_{\Ens} \to [0,1]$ is a set function that satisfies the following:
\begin{enumerate}
	\item non-negative and unit bounded - $\frcap_{\ens}(A) \in [0,1]$
	\item monotone - $A \subseteq B \implies \frcap_{\ens}(A) \leq \frcap_{\ens}(B)$
	\item sub-additive - $\frcap_{\ens}(A \cup B) \leq \frcap_{\ens}(A) + \frcap_{\ens}(B)$
	\item continuous from below and above - $\frcap_{\ens}(\lim\limits_{i \to \infty} A_i) = \lim\limits_{i \to \infty} \frcap_{\ens}(A_i)$ for any increasing or decreasing sequence $\{A_i\}$.
\end{enumerate}

The first property readily comes from the range of the fraction. For the second, note that $\hull$ and $\sup$ are both monotone. For sub-additivity, note that the fraction of two components can at most sum during mixing. For the last, increasing and decreasing sequences strictly add or remove possible mixtures, therefore they will lead to increasing or decreasing sequences of real numbers bounded between zero and one. The limit of these sequences must agree with the fraction capacity of the limit. If not for the lack of additivity over disjoint sets, the fraction capacity would be a probability measure.

Our setup is very similar to what happens in Choquet theory. The fraction capacity, in fact, is not just defined on the extreme points, but it is defined on the whole space of ensembles. In the classical discrete case, for example, if $A \subset \Omega \subset \Ens$ is an event, then $\frcap_{\ens}(A)$ will return the probability of event $A$; if $P \subset \Ens$ represents all the Poisson distributions with integer coefficient, $frcap_{\ens}(P)$ will return how much of $\ens$ can be expressed is a mixture of those distributions. Similarly, the probability measures in Choquet theory are, in general, defined on the whole ensemble space, and then one finds those particular ones that have support on the extreme points.

We can establish a direct connection between the fraction capacity and Choquet theory, because the ability to express an ensemble $\ens$ as the convex combination of $A$ is precisely the ability to find a measure $\mu$ that has support within $A$. That is, $\ens = \sum_{i=1}^{\infty} p_i \ens[a]_i$ if an only if the probability measure defined by $\mu(\{\ens[a]_i\}) = p_i$ represents $\ens$.\footnote{TODO: add proof somewhere} Consequently, if $M_{\ens}$ is the set of all probability measures that represent $\ens$ as defined in Choquet theory, then $\frcap_{\ens}(A) = \sup(\{\mu(A) \, | \, \mu \in M_{\ens}\})$.\footnote{TODO: prove somewhere}

As we saw, the classical case corresponds to a simplex in Choquet theory. This means that each element of the ensemble space is represented by a unique probability measure over the extreme points. In that case, the fraction capacity extended on the extreme points will also be additive. In the non-classical case, the closure of the ensemble space will not be a simplex, and there will be multiple representations, which will lead to a non-additive measure. Suppose, in fact, that $\mu$ and $\nu$ are two distinct measures that represent the same ensemble $\ens$. Then there will be a set $A$ such such that $\mu(A) \neq \nu(A)$. Suppose, without loss of generality, that $\mu(A) > \nu(A)$ We will also have $\mu(A^{\complement}) = 1- \mu(A) < 1 - \nu(A) = \nu(A^{\complement})$. Since the fraction capacity for a set cannot be less than the measure on that set, we have $\frcap_{\ens}(A \cup A^{\complement})=\frcap_{\ens}(X) = 1 = \mu(A) + \mu(A^{\complement}) < \mu(A) + \nu(A^{\complement}) \leq \frcap_{\ens}(A) + \frcap_{\ens}(A^{\complement})$. That is, the fraction capacity is additive over disjoint sets of extreme points in and only in the classical case. In other words, the fraction capacity reduces to a classical probability distribution over pure states in the classical case.

\section{State capacity}

We now want to introduce a measure theoretic representation of the entropy that generalizes the notion of count of states. Since entropy characterizes the variability of the preparations of the ensemble, greater variability means the ensemble is spread over more cases. Therefore the count of distinguishable states must be a monotonic function of the entropy. It has been shown[cite Hall] under very general conditions that the exponential of the entropy is the only function to have the appropriate features. One required feature, that we prove in general, is that the exponential of the entropy is subadditive (i.e. the count of states associated to a mixture cannot exceed the combined count of states associated to both mixtures) and it is additive for orthogonal ensembles (i.e. no overlap in possible instances) in a specific linear combination.

Given a Borel set of ensembles $A \subseteq \Ens$, we define the \textbf{state capacity} of $A$ to be
\begin{equation}
\capacity(A) = \sup(2^{S(\hull(A))}\cup\{0\}).
\end{equation}
This returns the exponential of the biggest entropy achievable with a mixture of elements of $A$.

One can then show that the fraction capacity $\capacity_{\ens} : \Sigma_{\Ens} \to [0,\infty]$ is a set function that satisfies the following:
\begin{enumerate}
	\item non-negative - $\capacity(A)\geq 0$
	\item monotone - $A \subseteq B \implies \capacity(A) \leq \capacity(B)$
	\item sub-additive - $\capacity(A \cup B) \leq \capacity(A) + \capacity(B)$
	\item additive over orthogonal sets - $\capacity(A \cup B) = \capacity(A) + \capacity(B)$ if $A \ortho B$
	\item continuous from below and above - $\capacity(\lim\limits_{i \to \infty} A_i) = \lim\limits_{i \to \infty} \capacity(A_i)$ for any increasing or decreasing sequence $\{A_i\}$.
\end{enumerate}

The first property readily comes from the range of the exponential. For the second, note that $\hull$, exponential and $\sup$ are all monotone. For sub-additivity, the exponential of the entropy itself is sub-additive, and it is additive over orthogonal ensembles. For the last, since the entropy itself  is continuous, this property carries over to the set function.

Again we note that the non-additive measure we recover is very well-behaved, and connects directly with important quantities. When Choquet theory applies, state capacity can be extended to the extreme points. In the classical case, disjoint sets of extreme points will be orthogonal, meaning that the state capacity there restricted is an additive measure. In the discrete case, every point is associated to zero entropy, and since $2^0=1$, the state capacity reduces to the counting measure. In the continuous case, gives a subset of phase space $A$, the maximum entropy is reached by the uniform distribution $\rho_A$, which means we recover $\log_2 \capacity(A) = S(\rho_A)$, the fundamental postulate of statistical mechanics. Similarly, in quantum mechanics, given an orthogonal subspace $A$, the maximum entropy is recovered by the maximally mixed state, which means $\capacity(A) = 2^{\dim(A)}$.

The state capacity, then, recovers the number of distinguishable states. In classical mechanics, all states are perfectly distinguishable. In quantum mechanics, only orthogonal state are distinguishable, and therefore the state capacity recovers the number of orthogonal states.

\section{Connection to fuzzy logic}



\section{Possible generalization of calculus}

As we pointed out, the state capacity and the fraction capacities are always well-defined, even in the cases where, potentially, Choquet theory does not apply because the closure of the ensemble space is not compact.

General problems:
1) Can we always find extreme points?
2) Can we generalize to the non-compact case? Can \href{https://en.wikipedia.org/wiki/Krein%E2%80%93Milman_theorem#convex_compactness}{convex compactness} help?
3) In the classical case, both measures become additive on the extreme points. Standard calculus is used. Is there a non-additive version of calculus that generalizes?

\section{Conclusion}



\section*{Acknowledgments}
This paper is part of the ongoing \textit{Assumptions of Physics} project \cite{aop-book}, which aims to identify a handful of physical principles from which the basic laws can be rigorously derived. This article was made possible through the support of grant \#62847 from the John Templeton Foundation.


\bibliography{bibliography}

\newcommand{\pj}[1] {\underbar{$#1$}}


\end{document}
