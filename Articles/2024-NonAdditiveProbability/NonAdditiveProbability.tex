\documentclass[10pt,twocolumn, nofootinbib]{revtex4-2}

\usepackage{assumptionsofphysics}
\usepackage{tikz}
\usepackage{breakurl}
\usepackage{tcolorbox}

\newcommand\hull{\mathrm{hull}}
\newcommand\stcap{\mathrm{scap}}
\newcommand\fraction{\mathrm{frac}}
\newcommand\frcap{\mathrm{fcap}}

\newcommand{\ens}[1][e] {\mathsf{#1}} % Ensemble
\newcommand{\Ens}[1][E] {\mathcal{#1}} % Ensemble space

\def\ortho{\perp}
\def\northo{\nperp}
\def\separate{\downmodels}
\def\nseparate{\ndownmodels}

\def\>{\rangle}
\def\<{\langle}

\begin{document}

\title{A non-additive generalization of probability theory \\for quantum mechanics and beyond}
\author{Gabriele Carcassi}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}
\author{Christine A. Aidala}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}

\date{\today}


\begin{abstract}
	We present a physically motivated generalization of probability theory that is suitable for classical mechanics, quantum mechanics and any future physical theory that allows a statistical description. The goal is to put the use of classical and quantum probability in a broader context, and show how the current mathematical structures are likely not suitable to solve the open problems in the foundations of physics. For the more mathematically inclined, we will point to areas where new math or generalization of established math are needed. For the more philosophically incline, we will point to areas where further conceptual work is needed.
	
	
	%Given a generic space of ensembles, we can define the fraction capacity as the maximum fraction of a particular ensemble that can be understood as a mixture of ensembles from a given set. This gives a non-additive (i.e. fuzzy) measure that reduces to a probability (i.e. additive) measure for classical spaces and for quantum measurements. We can also define the state capacity as the exponential of the maximum entropy reachable by a mixture of ensembles from a given set. This is also a non-additive measure and reduces to the Liouville measure in classical mechanics and the Hilbert space dimensions for subspaces of quantum mechanics. Conceptually, it gives us a notion of probability that is both theory and interpretation independent. Mathematically, it gives us a measure theoretic generalization of probability. Physically, it may allow to find new way to understand current theory and tool to investigate new ones. The purpose of this paper is to show the core ideas and present questions that may be developed on the mathematical, physical and philosophical side.
\end{abstract}

\maketitle

\section{TOC}

Overall goal: to create a generalization of probability theory and measure theory for general physical systems.


General question: how do we represent physical states? Different types of representations: talk about examples, ....

-- General problem

For this work: assume we have a locally convex topological vector space V. E is a bounded convex subset of V. Equipped with an entropy. Topology generated by entropic balls?

Separateness and orthogonality

Statistical quantities

Show that they reduce to standard things in classical mechanics and quantum mechanics.

-- Connection to Choquet theory 

Choquet theory - in the case that the closure of the space is compact -> representation through measures. May not be unique

Choquet simplex. Uniqueness of decomposition and separateness/orthogonality.

-- Connection to signed probability

TODO: literature search on use of signed measure in QM. ( e.g. \href{https://arxiv.org/pdf/2302.00118}{this paper})

Characterize signed measures as an affine representation of the ensemble space (i.e. bounded convex subset of a locally convex topological vector space).

Show that the probability is signed if and only if multiple decomposition, non-additive, no Choquet simplex

Signed probability is unique, but does not characterize the convex set (ensemble space). Note: the other measures are defined on the extreme points. Knowing the extreme points means you know the convex space (i.e. take the hull). The non-additive measure does not characterize the extreme points, so it doesn't really characterize the space.

-- Our non-additive measure approach

State capacity

Fraction capacity. Show uniqueness. Also show uniqueness on extreme points.

Fraction capacity as supremum of all possible measures

Additivity of fraction capacity (and state capacity).

Single decomposition and classical spaces and measurement contexts

Show that they reduce to standard things in classical mechanics and quantum mechanics.

-- Connection to fuzzy logic?

Literature search.

Possible connection to
* Choquet integral
* Sugeno integral

? Space of fuzzy measure like we have a space of additive measures?


\section{Introduction}

In the past few decades, there has been a growing interest in developing approaches that generalize the notion of states and physical theories. One basic insight is that statistical/probabilistic concepts often provide a suitable basis for this generalization, and it is in this setting where most of these approaches operate. A key problem, then, is what notion of probability should be used given that the standard measure theoretic Kolmogorovian approach does not generally work in quantum mechanics.

We reformulate the problem in the following way: given the space of all possible statistical ensembles allowed by a statistical theory, what measure theoretic tools can we use to represent each ensemble and the geometric properties of the overall space? Why exactly classical probability measures fail to capture completely quantum states? What suitable generalizations are possible, and what mathematical tools would need to be developed?

In this paper, we will characterize a generic space of ensembles as a convex bounded set of a Hausdorff locally convex topological vector space, upon which an entropy function is defined. These mathematical properties are physically grounded in the ability to experimentally define ensemble (leading to a topological space), to define statistical mixtures (leading to a convex space), to quantify the variability of the elements of the ensembles (leading to an entropy function) and to having enough real valued quantities to identify each ensemble (leading to the continuous embedding into a Hausdorff locally convex topological vector space). The question is, then, what measure theoretic tool can quantify the number of states (as in the classical Liouville measure) or the number of possible outcomes of a measurements (as in the number of orthogonal subspaces of an operator)? What measure theoretic tool can represent each ensemble? If we are to define a physically meaningful notion of probability density, both of these questions must be answered.

We will first review some results from Choquet theory, which is, to our knowledge, the most general approach to represent elements of a convex set using additive probability measures. The key insight is that, at least in the compact case, such representation is always possible, but it is not unique. Uniqueness requires the space to be a Choquet simplex, which is a generalization of the standard finite-dimensional simplex and is a particularly shaped subset of a vector space. The space of quantum ensembles is not a simplex for one key reason: it allows multiple decompositions in terms of pure states. These insights will allow to better understand exactly the limits of classical probability to describe ensembles in physical theories.

We will also review some results from the use of quasi-probability measures. While the Wigner function represents the most known instance, there are different choices. Note that the space of signed probability measures over a set $K$ is a vector space, so one can find an affine map between a suitable space of signed probability measures and the embedding vector space. Each ensemble, then, is uniquely represented with a signed probability measure. However, unlike in the Choquet case, the measure is not supported by the set of extreme points of the ensemble space. That is, there is no unique or privileged choice for this representation. Moreover, since the map is at the level of the full vector space, not just the subset with valid ensembles, there is no general way to characterize which subset of signed probability measures correspond to ensembles. Therefore, while signed measures are a powerful tool for calculation in specific problems, they cannot give us a general principled representation of the physical theory.

The above two approaches exhaust what can be done with additive measures, so we turn to the non-additive case. From the entropy, we can define the state capacity, which generalizes the notion of count of states. For each ensemble, we can define the fraction capacity, which generalizes the notion of probability. Both are non-negative sub-additive monotonic set functions, with the second being unit bounded. These measures are always unique and, since they are defined on the ensemble space itself, always have provide a physically meaningful representation. Moreover, in the compact case, these measure can be restricted on the extreme points while retaining uniqueness. (TODO: prove)

The fraction capacity is the supremum of all possible measures for the same ensemble. The classical case exactly corresponds to the simplex case, in which fraction capacity corresponds to a unique measure, and is therefore additive. The state capacity, instead, corresponds to the minimum of the entropy of all possible representations. It recovers the Liouville measure in the classical case and the dimensionality of the Hilbert space in the quantum case.

Having shown that a general approach based on non-additive measures is possible, we lay down a series of questions to explore in the future. The biggest one is whether it is possible to generalize notions such as integration, derivation and expectation to this non-additive case.  It should be noted that non-additive measures are used in the context of decision theory, so there may be a connection between the use of non-additive measures in quantum theory and interdependent complimentary choices.[TODO: find citation that sub-additive is when choices are complementary] Both the Choquet and the Sugeno integral, for example, are defined over a monotonic set-functions, so there is a chance that these tools may turn out to be useful. We see this work as starting this line of research, which will take more effort to investigate.

A final remark before we start. Too often works in the foundations of physics focus on a particular aspect without concern to a broader connection to all relevant mathematical, physical and philosophical aspects. Therefore, it is important to us that this work is a piece of a much larger puzzle, which connects cleanly and directly to  other areas, such as information geometry, functional analysis, quantum logic and so on. It is the hallmark of a true fundamental structure for physics that such connections can be easily made. Unfortunately, the nature of scientific publication requires us to publish a narrow slice of the work to fit the scope of a particular journal. Ironically, this means that the most important motivations for our approach are out of scope of this article, as we will concentrate only to the connections to measure theory.

\section{Ensemble space}

Conceptually, a statistical ensemble is a collection of outputs, taken at once, of a reproducible preparation procedure for a physical system. In the classical case, this corresponds to a probability distribution over a discrete set of outcomes (i.e. a fair coin) or over phase space (i.e. the symplectic manifold representing position and momentum for a classical particle). In the quantum case, this corresponds to a mixed state (i.e. a positive semi-definite self-adjoint operator of trace one). An ensemble space is the collection of all possible ensembles defined in a physical theory, together with a basic mathematical structure that all physical theories must possess. The development of a full theory of ensembles goes beyond the scope of this work, and we refer to the appendix and our ongoing work (cite).

For the purpose of this paper, we will assume an ensemble space $\Ens$ is a convex subset of a Hausdorff locally convex topological vector space $V$. The topology is the physically natural topology, where each open set represents a verifiable statement. The convex structure represents statistical mixing. For example, $p \ens[a] + \bar{p} \ens[b]$ represents the statistical mixture of two ensembles $\ens[a]$ and $\ens[b]$ with probability $p$ and $\bar{p} = 1 - p$. Only finite mixtures are guaranteed, while the topology will decide whether an infinite mixture $\sum_{i}^{\infty} p_i \ens_i$ converges in the space or not. We also assume that there is a countable family of affine continuous functions $F_i : \Ens \to \mathbb{R}$ that represent statistical quantities and fully identify each ensemble. That is, $F_i(p \ens[a] + \bar{p} \ens[b]) = p F_i(\ens[a]) + \bar{p} F_i(\ens[b])$ and $\ens[a] = \ens[b]$ if and only if $F_i(\ens[a]) = F_i(\ens[b])$ for all $i$.\footnote{It is an open question whether all ensemble spaces must possess this property.} For example, we can imagine $F_i$ representing the expectation of all polynomials of position and momentum.\footnote{This requires all probability distributions to converge to zero at infinity faster than any polynomial.} Mathematically, these statistical quantities can be used to define semi-norms on the space, giving us a locally convex topology.

Additional, each ensemble space $\Ens$ is equipped with a continuous entropy function $S : \Ens \to \mathbb{R}$. The entropy of a statistical mixture satisfies the following bounds
\begin{equation}
	\begin{aligned}
		p S(\ens[a]) + (1-p) &S(\ens[b]) \leq S(p \ens[a] + (1-p) \ens[b]) \\
		&\leq I(p,\bar{p}) + p S(\ens[a]) + (1-p) S(\ens[b])
	\end{aligned}
\end{equation}
where $I(p,\bar{p}) = - p \log p - \bar{p} \log(\bar{p})$.\footnote{In the larger work, we can show that the Shannon entropy is the only upper bound possible.} The bounds come from assuming that the entropy quantifies the variability of each ensemble. Intuitively, the variability cannot decrease during mixing, and will have an upper bound when mixing two ensembles that are non-overlapping, or orthogonal. In fact, we define two ensembles $\ens[a] \ortho \ens[b]$ to be orthogonal when they mixture gives a maximal entropy increase. This does recover the notion of orthogonality in the corresponding classical and quantum vector spaces of statistical ensembles. Lastly, mixtures preserve orthogonality in the sense that mixing non-overlapping ensembles still gives non-overlapping ensembles. More precisely, $\ens[a] \ortho \ens[b]$ and $\ens[a] \ortho \ens[c]$ if and only if $\ens[a] \ortho p \ens[b] + \bar{p} \ens[c]$ for any $p \in (0,1)$.

To summarize, the above mathematical structure represents the requirements that ensembles must be experimentally well-defined, allow statistical mixture, can be identified with statistical quantities and provide a well-defined entropy.\footnote{While the full physical justification of the above mathematical structure is beyond the scope of this work, it is important to us that such justification exists and this physically straight forward. This is the essence of what we call Physical Mathematics. We will summarize some of these justifications in the appendix, leaving the full and more updated discussion in our work in progress.} We now turn to the question of how we can represent both the entropy and each ensemble in terms of some measure theoretic structure.

\section{Additive measure and Choquet theory}

Since ensemble spaces are a convex subset of a locally convex topological vector space, they can be connected to Choquet theory. First of all, let $E$ be a convex subset of a locally convex space $V$. We say that $\ens \in E$ is represented by a probability measure $\mu : \Sigma_{E} \to [0,1]$ if $f(\ens) = \int_X f d\mu$ for every continuous linear functional $f$ on $V$. The Choquet theorem states:
\begin{thrm}[Choquet]
	Let $E$ be a metrizable compact convex subset of a locally convex space $V$, and let $\ens \in E$. Then there exists a probability measure $\mu_{\ens}$ on $X$ which represents $\ens$ and is supported by the extreme points of $X$.
\end{thrm}

First, let us understand the physical significance. An ensemble $\ens$ can be seen as a probability measure $\mu$ over the ``pure states'' (i.e. the extreme points of $E$) if the value for every statistical quantity $f$ for $\ens$ can be recovered as an expectation over the pure states using the probability measure. That is, all statistical properties of $\ens$ can be recovered by the probability measure. This is what allows to think go $\ens$ as a probability distribution. Note, however, that the Choquet theorem does not guarantee uniqueness. The probability measure is unique if and only if $E$ is a Choquet simplex, which is a generalization of the finite dimensional simplex. The characterization of the Choquet simplex is fairly technical, so we will simply define the Choque simplex by the property of having a single decomposition in terms of extreme points.

We should now look at the requirements of the theorem, to see whether we can always apply it to an ensemble space. First of all, and ensemble space $\Ens$ is a convex subset of a convex sapce $V$. Moreover, since every Hausdorff first-countable topological vector space is metrizable, $\Ens$ is metrizable. However, $\Ens$ is not necessarily compact. In fact, $\Ens$ is not necessarily a closed subset of $V$. Consider the space of probability measures over $\mathbb{R}$. Since $\mathbb{R}$ is not compact, the space of probability measure over $\mathbb{R}$ cannot be compact.\footnote{TODO: Check with mathematicians} Moreover, the set of extreme points is given by the Dirac measures (i.e. probability one over a single point), which would have minus infinite entropy, and therefore cannot be part of the ensemble space. Therefore the theorem does not apply in general.

We can apply Choquet theorem in the case where the closure of $\Ens$ in $V$ is a compact set $E$, which will also be convex\footnote{The closure of a convex set is convex. TODO: finalize the proof} and metrizable. In this case, we are guaranteed that all ensembles allows a representation through a probability on points that may corresponds idealized ensembles (i.e. the Dirac measure) that are not, even conceptually, realizable. Many interesting cases fall into this restriction: the ensemble spaces of classical probability distribution over finitely many discrete cases fall in this restriction; the subset of ensembles over finite ranges of position and momentum; the ensemble spaces of all finite dimensional quantum systems.

Note that ensembles over classical spaces will be represented by a unique probability measure, therefore the corresponding closure of the ensemble space is a Choquet simplex. However, quantum mixed states can be decomposed in multiple ways in terms of pure states, and therefore the closure of a quantum ensemble space is not a simplex.

% space is metrizable https://proofwiki.org/wiki/Birkhoff-Kakutani_Theorem/Topological_Vector_Space

Chouqet theory tells, in the realm where it can be applied, that classical probability fails in quantum mechanics precisely because the ensemble space of a quantum system allows multiple decomposition.

\section{Quasiprobability}

Another way to represent quantum ensembles is through the use of quasiprobability distributions. Given a space $X$, a measure $\mu$ is affine if $\mu(X)$. All probability measures are affine measures, but the reverse is not true as affine measures can be negative on particular regions. The Wigner function, for example, can be negative over small regions of phase space.

Given a space $X$, the space of all affine measure is a vector space, not just a convex subset as in the case of the space of probability measures. This means that, given an ensemble space, we may find a space $X$ such that the space of affine measures over $X$ is isomorphic to the embedding vector space $V$ as topological vector spaces. In this case, any element $v\in V$ can be represented uniquely by an affine measure $\mu_{v}$, regardless of whether they are actual ensembles or not.\footnote{The question of whether such an $X$ always exists is left open. We suspect a link exists between the statistical quantities $F_i$ that fully identify each ensemble and the space $X$.} Because the statistical quantities $F_i$ are affine functions, we will have $F_i(v) = \int_X F_i d\mu_{v}$.

In general, since the ensemble space is bounded along every direction, not all affine measure will correspond to an ensemble space. Whether all probability measure correspond to a valid ensemble, or whether affine measures are even used, correspond to relationships between $\Ens$ and the space of probability measures $M_p(X)$. If $\Ens \subset M_p(X)$, then all states are represented by a probability measure. However, this is not a ``probability'' in the standard sense of the word. For example, the Husimi Q representation is always non-negative at every point, but two points to do not represent mutually exclusive events as they are not orthogonal (i.e. their inner product is non-zero and they do not maximize the entropy increase during mixture). Mathematically, the issue is that the points $X$ of the Husimi Q representation are not the extreme points of the closure ensembles space $\overline{\Ens}$. In general, $\Ens$ and $M_p(X)$ will overlap, but neither will contain the other. For example, in the Wigner representation the only pure states (i.e. the extreme points of $\Ens$) represented by a probability measure are the Gaussian wave-packets. This means that any other non-negative Wigner function will be in the convex hull of Gaussian wave-packets, and therefore will allow a decomposition in terms of them.

Note that if $\overline{\Ens} = M_p(X)$, then the space of the extreme point is exactly the space of Dirac measures, and therefore $X$ corresponds to the pure states. Alternatively, either one ensemble is represented by an affine measure, or not all probability measures can be understood as mixtures of extreme points. If $\overline{\Ens}$ is compact, then this is exactly when $\overline{\Ens}$ is not a simplex and multiple decompositions are allowed.

This alternative way of looking at measure theoretic representations, then, is consistent with the previous one. While quasiprobability distributions are very useful in practice, we want to stress that there is no single choice for $X$. To solve a specific problem, this may actually be useful as one can choose the most convenient $X$ for the problem. To develop a general theory for physics, however, this is not useful as each representation is not general enough.

\section{Fraction capacity}

TODO: intro

Given that mixing coefficients have a more general applicability than probability of outcomes, let us concentrate on the following problem: given an ensemble $\ens \in \Ens$ and a (Borel) set of ensembles $A \subseteq \Ens$, what fraction of $\ens$ can be constructed by a mixture of $A$? How much of $\ens$ can be explained as coming from preparations corresponding to $A$? For example, let $\Ens$ be the space of all probability distributions for a six-sided die. Let $\ens_{123456}$ be the uniform distribution over all outcomes. Let $A_1 = \{\ens_1\}$ where $\ens_1$ represents outcome one with 100\% probability. Since we can write $\ens_{123456} = \frac{1}{6} \ens_{1} + \frac{5}{6} \ens_{23456}$, where $\ens_{23456}$ represents a uniform distributions over the five outcomes, $1/6$ of $\ens$ can be constructed from $A$, but no more. Similarly, if $A_{12} = \{\ens_{1},\ens_{2}\}$, we can write $\ens = \frac{1}{3} \left(\frac{1}{2} \ens_1 + \frac{1}{2} \ens_2 \right)  + \frac{2}{3} \ens_{3456}$, so $1/3$ of $\ens$ can be constructed from $A$, but no more. Note that, given the uniform distribution, $1/6$ is exactly the probably for the event $A_1$ and $1/3$ the probability for event $A_{12}$. This gives the basic insight for our definitions.

Given a target ensemble $\ens \in \Ens$ and an arbitrary ensemble $\ens[a] \in \Ens$, we define the \textbf{fraction} of $\ens[a]$ in $\ens$ to be
\begin{equation}
	\fraction_{\ens}(\ens[a]) = \sup(\{ p \in [0,1] \, | \, \exists \, \ens_1 \in \Ens \text{ s.t. }  \ens = p \ens[a] + \bar{p} \ens_1 \}).
\end{equation}
The fraction is always well-defined because we can always write $\ens = 0 \ens[a] + 1 \ens$, therefore it must be zero or greater. This quantity tells us how much of ensemble $\ens$ can be constructed from $\ens[a]$. 

We now extend this idea from a single ensemble $\ens[a] \in \Ens$ to a Borel set of ensembles $A \subset \Ens$. Given that the $\hull(A)$ is the set of all possible convex combinations (i.e. mixtures),\footnote{Since we work with convex sets within a topology, the hull is both the closed under the topology and convex combinations. That is, $\hull(A)$ is the smallest closed and convex set that contains $A$} we define the \textbf{fraction capacity} of $A$ for $\ens$ to be
\begin{equation}
	\frcap_{\ens}(A) = \sup(\fraction_{\ens}(\hull(A))\cup\{0\}).
\end{equation}
This returns the biggest fraction of $\ens$ that can be achieved with a mixture of elements of $A$.\footnote{The name fraction capacity is chosen to both signify the ability of the set $A$ to contain $\ens$ (i.e. fraction capacity of one means $\ens$ is within the convex combinations of $A$) and to indicate that it will be a non-additive monotonic measure, which are called ``capacities'' in some literature. TODO \href{ https://link.springer.com/book/10.1007/978-3-319-03155-2}{cite} }

One can then show that the fraction capacity $\frcap_{\ens} : \Sigma_{\Ens} \to [0,1]$ is a set function that satisfies the following:
\begin{enumerate}
	\item non-negative and unit bounded - $\frcap_{\ens}(A) \in [0,1]$
	\item monotone - $A \subseteq B \implies \frcap_{\ens}(A) \leq \frcap_{\ens}(B)$
	\item sub-additive - $\frcap_{\ens}(A \cup B) \leq \frcap_{\ens}(A) + \frcap_{\ens}(B)$
	\item continuous from below and above - $\frcap_{\ens}(\lim\limits_{i \to \infty} A_i) = \lim\limits_{i \to \infty} \frcap_{\ens}(A_i)$ for any increasing or decreasing sequence $\{A_i\}$.
\end{enumerate}

The first property readily comes from the domain of the fraction. For the second, note that $\hull$ and $\sup$ are both monotone. For sub-additivity, note that the fraction of two components can at most sum during mixing. For the last, increasing and decreasing sequences strictly add or remove possible mixtures, therefore they will lead to increasing or decreasing sequences of real numbers bounded between zero and one. The limit of these sequences must agree with the fraction capacity of the limit. If not for the lack of additivity over disjoint sets, the fraction capacity would be a probability measure.

Our setup is very similar to what happens in Choquet theory. The fraction capacity, in fact, is not just defined on the extreme points, but it is defined on the whole space of ensembles. In the classical discrete case, for example, if $A \subset \Omega \subset \Ens$ is an event, then $\frcap_{\ens}(A)$ will return the probability of event $A$; if $P \subset \Ens$ represents all the Poisson distributions with integer coefficient, $frcap_{\ens}(P)$ will return how much of $\ens$ can be expressed is a mixture of those distributions. Similarly, the probability measures in Choquet theory are, in general, defined on the whole ensemble space, and then one finds those particular ones that have support on the extreme points.

We can establish a direct connection between the fraction capacity and Choquet theory, because the ability to express an ensemble $\ens$ as the convex combination of $A$ is precisely the ability to find a measure $\mu$ that has support within $A$. That is, $\ens = \sum_{i=1}^{\infty} p_i \ens[a]_i$ if an only if the probability measure defined by $\mu(\{\ens[a]_i\}) = p_i$ represents $\ens$.\footnote{TODO: add proof somewhere} Consequently, if $M_{\ens}$ is the set of all probability measures that represent $\ens$ as defined in Choquet theory, then $\frcap_{\ens}(A) = \sup(\{\mu(A) \, | \, \mu \in M_{\ens}\})$.\footnote{TODO: prove somewhere}

As we saw, the classical case corresponds to a simplex in Choquet theory. This means that each element of the ensemble space is represented by a unique probability measure over the extreme points. In that case, the fraction capacity extended on the extreme points will also be additive. In the non-classical case, the closure of the ensemble space will not be a simplex, and there will be multiple representations, which will lead to a non-additive measure. Suppose, in fact, that $\mu$ and $\nu$ are two distinct measures that represent the same ensemble $\ens$. Then there will be a set $A$ such such that $\mu(A) \neq \nu(A)$. Suppose, without loss of generality, that $\mu(A) > \nu(A)$ We will also have $\mu(A^{\complement}) = 1- \mu(A) < 1 - \nu(A) = \nu(A^{\complement})$. Since the fraction capacity for a set cannot be less than the measure on that set, we have $\frcap_{\ens}(A \cup A^{\complement})=\frcap_{\ens}(X) = 1 = \mu(A) + \mu(A^{\complement}) < \mu(A) + \nu(A^{\complement}) \leq \frcap_{\ens}(A) + \frcap_{\ens}(A^{\complement})$. That is, the fraction capacity is additive over disjoint sets of extreme points in and only in the classical case. In other words, the fraction capacity reduces to a classical probability distribution over pure states in the classical case.



\section{Old introduction}

The goal of this paper is to show that, in the same way that classical probability can be axiomatized through standard measure theory, quantum probability, and in fact any probability used a physical theory, can be axiomatized through non-additive (i.e. fuzzy) measure theory. We will define a generalized concept of probability that is ultimately based on physically motivated axioms, giving the physicist a clear operational motivation to our definitions, the mathematicians a formally well-defined mathematical structure and the philosopher a unified framework in which probability can be interpreted in the context of all physical theories. We are not going to be able to develop a full theory here, as there are still a number of conceptual and technical challenges to be solved that will require work in physics, mathematics and philosophy. However, we will show that there are enough results that point in the same direction, and therefore make it reasonable to invest time and effort to work on the open problems.

The material presented is part of a larger work that aims to coalesce ideas and results from different branch of mathematics and physics. For those interested, the current state of the research is openly available in an open document that will be routinely updated as experts from different fields review and help us improve the overall theory.

In a nutshell, we will start by arguing that physical theory must at least be able to talk about statistical ensembles, and that ensembles must allow the preparation of statistical mixture. Given a target ensemble $\ens$ and a set of ensemble $A$, we define the fraction capacity of $A$ for $\ens$ as the biggest fraction of $\ens$ that can be constructed from another set of ensembles $A$. That is, the highest $p$ such that we can express our ensemble as $\ens = p \ens[a] + (1-p) \ens[b]$, where $\ens[a]$ is a mixture of elements of $A$. For example, if $\ens$ is the uniform distributions for a roll of a six-faced die, and $A$ includes only two ensembles with $100\%$ probability for the outcomes $3$ and $4$ respectively, then the faction capacity of $A$ for $\ens$ will be $\frac{1}{3}$. Note that this coincides with the probability of the event ``$3$ or $4$''. The fraction capacity is a non-negative, unit bounded, sub-additive and continuous set function, which is therefore like a probability measure except for the additivity, which is recovered for classical mechanics and over quantum measurements.

Similarly, given a set $A$ of ensembles we define the \textbf{state capacity} as the exponential of the highest entropy obtainable with a mixture of $A$. Note that the highest entropy of a subspace in both classical and quantum statistical mechanics is given by the uniform distribution, therefore the state capacity recovers the count of states (i.e. the Liouville volume) in classical mechanics and the dimensionality of the Hilbert space in quantum mechanics. The state capacity is also a non-negative, sub-additive and continuous set function. This points to a possible generalization of measure theoretic calculus to a sub-additive case that would be guaranteed to work on all physical theories.

\section{Ensemble spaces}

Note that our goal is not to construct an abstract probability theory that applies to everything, but rather to construct a specific probability theory that is guaranteed to apply to all physical systems. Therefore we do not to define an axiomatic theory of probability and apply it to physical systems, but rather characterize basic properties of physical states and from those build a theory of probability that fits those definitions. Whether it is applicable to decision theory, economics, or psychology is out of scope. As is common in our larger project, we start from axioms that, in our opinion, represent requirements of scientific practice. We call these \textbf{constitutive assumptions} as they are essential to proceed with the scientific endeavor. The advantage is that the realm of applicability and physical interpretation will be clear from the start. 

We start from requiring a physical theory to allow statistical descriptions: at the very least, it will provide a notion of ensemble and describe which ensembles that are allowed by the theory. An ensemble can be understood as the infinite collection of the outputs of a particular preparation procedure. The ensemble space of classical mechanics, for example, is the space of probability distributions on phase space. In quantum mechanics, instead, it is the space of density operators. While the ensemble space for each theory will in principle be different, they will all share common features: they must allow measurable properties of the system, probability distributions for outcomes of said properties, characterize reversible and irreversible processes through an entropy function, and so on. We are looking for necessary minimal basic requirements upon which these common tools can be built.

At least two arguments can be made to show that any physical theory must allow statistical descriptions, i.e.~provide an ensemble space. First, note that physical laws are never about specific instances but rather regularities. They are statements of the type ``whenever we prepare this, we can measure that.'' This means that, ultimately, they are about ensembles. Second, if a physical theory is to be testable experimentally, and repeatedly so, it must be in terms of ensembles because this is what we prepare in practice and this is what, in the end, we characterize experimentally. Moreover, repeatability requires the ability to always test ``one more time,'' which implies that ensembles are infinite collections, and therefore idealization. The approach, then, can most likely be given firm conceptual grounds, which we leave as an open but likely solvable philosophical problem.

We have identified three basic requirements on ensembles: first, they have to be identifiable experimentally; second, they need to allow statistical mixtures; lastly, they need a well-defined entropy. Let us go through these items one by one. We leave the full details to the working document (TODO: decide how to call it).

\subsection{Experimental verifiability}
The first requirement is that ensembles must be connected to experimental verification. That is, we must have enough \textbf{verifiable statements} at our disposal to define ensembles and tell them apart. By verifiable statement we mean a statement for which an experimental test is available that terminates in finite time if and only if the statement is true. For example, the statement ``the mass of the photon is less than $10^{-13} \, eV$'' is verifiable, while ``the mass of the photon is exactly $0 \, eV$'' is not verifiable due to its infinite precision. Based on previous work, (CITE) this mathematically requires an ensemble space to be a $T_0$ second-countable topological space, where open sets represent verifiable statements and Borel sets represent statements associated with a test, but without guarantee of termination.

The topological structure, then, represents the most foundational structure for a physical theory. It tells us why functions are ``well-behaved,'' which simply means they have to be topologically continuous in the ``natural topology'' induced by this requirement. It tells why probability is assigned to Borel sets (i.e. all statements associated with tests) and why the Banach-Tarski paradox does not apply in physics (i.e. non-Borel sets are physically ill-defined as they are not connected to experimental verification). Similarly, it tells us that sets with cardinality greater than that of the continuum are not physically relevant, as they cannot be given a $T_0$ second-countable topology.

In the case of ensembles, typical verifiable statements will be ``the average energy of the particle is between 3 and 4 $eV$,'' ``the probability of getting heads is between 49 and 51 percent,'' ``the probability distribution for the position is a Gaussian of mean 0 $m$ and 1 $m$ standard deviation within 1\%.'' Note that these are not results of single-shot measurements, and therefore verifiable statements are distinctly different from observables in quantum mechanics. In the same manner, these are not measurements that extract one bit of information, as there is no requirement of termination in the negative case.

\begin{tcolorbox}[colback=white, colframe=black]
	Experimental verifiability $\Rightarrow$ An ensemble space $\Ens$ is a $T_0$ second-countable topological space.
\end{tcolorbox}


\subsection{Statistical mixtures}
Another basic requirement is the ability to prepare a \textbf{mixture} of two ensembles. If $\ens[a], \ens[b] \in \Ens$ are two ensembles, and $p \in [0,1]$ a weight, then $\ens = p \ens[a] + (1-p) \ens[b] \in \Ens$ is the ensemble that describes a process that selects the first ensemble over the second $p$ percent of the times. Mathematically, the ensemble space is endowed with a \textbf{convex structure}. Moreover, since the mixing operation must be consistent with experimental verifiability, it will be topologically continuous, and the ensemble space will be a topological convex space.

Ultimately, the convex structure is responsible for all linear structures we have in physics. In face, the entropy and its bounds, defined later, forces the ensemble space to be the subset of a convex subset of a vector space. That is, the convex structure is ``invertible.''\footnote{If we fix $\ens$, $\ens[a]$ and $p$ then, if it exists, there is only one $\ens[b]$ such that $\ens = p \ens[a] + (1-p) \ens[b]$. In the context of convex spaces, this property is called ``cancellative.''} What is still not clear is whether the ensemble space embeds continuously in a topological vector space. While topological vector spaces are well established in the literature, topological convex spaces are not. 

Regardless of the topology, we are already equipped to understand what negative probability is. Suppose we have a real vector space of dimensions $n$, then $n$ linearly independent vectors are enough to express every point as a linear combination. We can also take an additional vector and express every point as an affine combination, that is a linear combination where the coefficients sum to one, though some can still be negative. Since every ensemble space, including the space of mixed states in quantum mechanics, is a subset of a real vector space, we can choose a linearly dependent set of states that span the whole space, and every mixed state can be expressed as an affine combination, as a pseudo-probability. However, the boundaries of the ensemble space will determine which affine combinations are valid states (i.e. they end up within the ensemble space), and there is no requirement that these boundary are conveniently expressible in terms of affine combinations.

The convex structure, however, allows us to ask whether two ensembles have a \textbf{common component}. That is, then can be seen as different statistical mixture that have some part in common. Two ensemble are separate, noted $\ens[a] \separate \ens[b]$, if they do not have a common component. This allows to characterize the space in terms of statistical decomposition, and see how this differs from other decompositions.

\begin{tcolorbox}[colback=white, colframe=black]
	Statistical mixture of ensembles $\Rightarrow$ An ensemble space $\Ens$ is a topological convex space.
\end{tcolorbox}

\subsection{Entropy}
The last requirement is that each ensemble must have a well defined \textbf{entropy}. That is, there is a scalar function $S : \Ens \to \mathbb{R}$ that satisfies the following:
\begin{enumerate}
	\item strictly concave: $S(p \ens[a] + (1-p) \ens[b]) - (p S(\ens[a]) + (1-p) S(\ens[b]) ) \geq 0$
	\item bounded from above: $S(p \ens[a] + (1-p) \ens[b]) - (p S(\ens[a]) + (1-p) S(\ens[b]) ) \leq - p \log p - (1-p) \log(1-p)$
\end{enumerate}
The first tells us that, during mixing, the entropy cannot decrease, and it stays the same if and only we are mixing an ensemble with itself. The second tells us that the most the entropy can increase is given by the choice between the two ensembles.\footnote{The expression for the Shannon entropy is actually derived instead of imposed axiomatically.} We define two ensembles to be \textbf{orthogonal} when they maximally increase the entropy during mixing. This recovers the standard notion of orthogonality: two classical ensembles are orthogonal when they have disjoint support, and therefore $\int_M \rho_1 \rho_2 d\mu = $; two quantum ensembles are orthogonal when they are defined on orthogonal subspaces, and therefore $\tr(\rho_1 \rho_2) = 0$.

The entropy is ultimately responsible for all geometrical structure in physics. In classical mechanics, the geometry is essentially defined by phase space volumes and areas in each degree of freedom (DOF). The entropy of uniform distributions is given by the logarithm of the volume, meaning that given the volume we are able to calculate the entropy and given the entropy we are able to reconstruct the volume. Similarly, uniform marginals will recover DOF areas. In quantum mechanics, the inner product (i.e. the Born rule) allows us to calculate the entropy of the mixture of two pure states, and given the entropy we can recover the Born rule.

An interesting insight is that the entropy, being strictly concave, has a negative defined Hessian. The negation of the Hessian, then, is a symmetric positive defined function of two infinitesimal variations, and can serve as a metric tensor on the affine structure given by the mixing coefficients. In both classical and quantum mechanics this recovers the Fisher-Rao metric.

Another crucial feature is that two ensembles maximize the entropy increase if and only if they are orthogonal: two classical ensembles maximize the entropy increase when they have disjoint support, and therefore $\int_M \rho_1 \rho_2 d\mu = 0$; two quantum ensembles when they are defined on orthogonal subspaces, and therefore $\tr(\rho_1 \rho_2) = 0$. This corresponds to the case when the two ensemble are made of instances that are mutually exclusive. Therefore we define two ensembles to be \textbf{orthogonal}, noted $\ens[a] \ortho \ens[b]$, if their mixture leads to a maximal entropy increase. The following additional axiom is valid for orthogonal ensembles
\begin{enumerate}
	\setcounter{enumi}{2}
	\item mixtures preserve orthogonality: $\ens[a] \ortho \ens[b]$ and $\ens[a] \ortho \ens[c]$ if and only if $\ens[a] \ortho p \ens[b] + \bar{p} \ens[c]$ for any $p \in (0,1)$
\end{enumerate}
as mixing elements distinguishable from those of ensemble $\ens[a]$, still gives elements distinguishable from ensemble $\ens[a]$.

It can be shows that orthogonal ensembles are separate, but the converse is not true. In quantum mechanics, for example, all pure states are separate (they cannot be decomposed and therefore cannot have components in common) but not all are orthogonal. In classical spaces, however, all separate ensembles are also orthogonal. Understanding the relationships between these two properties, then, is key in understanding the difference between classical and quantum mechanics, and what can happen in other physical theories.

\begin{tcolorbox}[colback=white, colframe=black]
	Existence of entropy $\Rightarrow$ Strictly concave function whose upper bound defines orthogonality. Statistical mixtures preserve orthogonality.
\end{tcolorbox}

\subsection{Interplay between structures}

Before introducing the actual measures, we want to note that there is a delicate balance between these structures which is important at both a conceptual level and at a more mathematical level. Part of the difficulty is to recognize which standard modes of understanding the problem should be kept, as the they generalize nicely, and which should be discarded. Let us go through some key examples.

Those that focus on the convex/linear structure may be tempted to close the space under infinite convex/linear combinations. It would be very convenient mathematically, and, in fact, this is what happens in Hilbert spaces. However, this closure leads yo physically ill-defined systems, such as a particle with an infinite average position or undefined energy.(CITE Hilbert) It is the connection to experimental verification, the topology, that will tell us which limits are allowed and which are not. Therefore, the lack of infinite convex is a crucial feature of the space, and convex sets in ensemble spaces are to be understood as closed convex sets, so that closure on the topology includes those, and only those, infinite mixtures that are physically meaningful.

If we focus on probability, we are tempted to extend the intuition from classical mechanics and it is useful to understand exactly why and where it fails. On a convex structure we can define the extreme points, those that cannot be further decomposed. Therefore it is tempting to define \textbf{pure states} as those ensembles that cannot be further decomposed into other ensembles. For example, the pure states for a six-sided die are the ensembles perfectly prepared respectively on the six possible outcomes. All ensembles are convex combinations $\sum_{i=1}^{6} p_i \ens_i = 1$ of those pure states $\ens_i$. This is exactly how classical discrete spaces work and much of our intuition for probability comes from there.

This approach, however, already becomes problematic in classical mechanics. Since we can take uniform distributions over any region of phase space, we can imagine to take a sequence of more and more refined distributions around a value of position and momentum. Therefore we may be tempted to say that the points of phase space represent perfectly prepared systems, and every other probability is a distribution on these space. This is physically unsound. First of all, we cannot consistently prepare position and momentum with infinite precision, therefore those ensembles do not exist even exist as an idealization. Secondly, the entropy of those ensembles, if they existed, would be minus infinity, which is a problem for any thermodynamic treatment. To make this intuition consistent with the math would require each point in phase space to be topologically isolated, which would extend the $\sigma$-algebra to the power set of $\mathbb{R}^n$, which means there would be no consistent way to define a volume. The fact that the math breaks down is simply a symptom that our physical problem is ill-specified.

In quantum mechanics, things are even worse. Pure states are indeed extreme points of the convex set. However, we also have discrete spectra. Here the temptation is, like we ``did'' in classical mechanics, to consider wave-functions all concentrated at a single value of position. While one may think this is the analogue of classical mechanics, it is not. We can, in fact, take a wave-function that is uniformly distributed on every finite range of position, and we can imagine to shrink that range. However, this is a sequence of pure states. All pure states have zero entropy, and the reduced spread on position will mean an increased spread on momentum. That is, we are not providing a sequence of ensembles at ever greater prevision, like in the classical case. In other words, the eigenstates of position in quantum mechanics, even if we wanted to say they were actual states, they would not be the limit of infinitesimal decomposition (i.e. a limit in the convex structure) but rather the limit of a path over pure states (i.e. a topological limit).

The key insight is that \textbf{mixing coefficients and probabilities are different in general} and they coincides only when a particular ensemble can be understood as a statistical mixtures of perfect preparations of a given quantity (i.e. of orthogonal ensembles). This always happens in the classical case and over measurement contexts in the quantum case, but is not the general case. For example, an equal mixture of spin up and spin left, for example, does not correspond to a 50\% probability of measuring spin up. The key to develop a general theory of probability, then, is to characterize the mixing coefficients first and understand when they recover probabilities of outcomes.

To recap, the topological structure captures experimental verifiability and is responsible for the limits and the connection to measureable quantities. The convex structure captures statistical mixing and is responsible for all linear structures of physical theories. The entropic structure captures the variability within an ensemble and is ultimately responsible for all geometric structures. Since the physical problem is indivisible, there is an interplay across these structure which goes beyond this article and we refer to our larger work.



\section{Statistical properties}

\section{Beyond real valued quantities}


\section{State capacity}

\section{Quantization}
* 3 pick 2

\section{Quantizing space-time}
* we need a non-additive measure on degrees of freedom
*

\section{Conclusion}



\section*{Acknowledgments}
This paper is part of the ongoing \textit{Assumptions of Physics} project \cite{aop-book}, which aims to identify a handful of physical principles from which the basic laws can be rigorously derived. This article was made possible through the support of grant \#62847 from the John Templeton Foundation.


\bibliography{bibliography}

\newcommand{\pj}[1] {\underbar{$#1$}}


\end{document}
