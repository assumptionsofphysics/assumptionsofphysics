\documentclass[10pt,twocolumn, nofootinbib]{revtex4-2}

\usepackage{assumptionsofphysics}
\usepackage{tikz}
\usepackage{breakurl}
\usepackage{tcolorbox}
\usepackage{scrextend}	% for environment ``labeling''

\newcommand\mix{\mathrm{mix}}
\newcommand\component{\mathrm{comp}}
\newcommand\cospan{\mathrm{cospan}}
\newcommand\dist{\mathrm{dist}}
\newcommand\hull{\mathrm{hull}}
\newcommand\support{\mathrm{supp}}
\newcommand\capacity{\mathrm{scap}}
\newcommand\fraction{\mathrm{frac}}
\newcommand\frcap{\mathrm{fcap}}

\newcommand{\ens}[1][e] {\mathsf{#1}} % Ensemble
\newcommand{\Ens}[1][E] {\mathcal{#1}} % Ensemble space

\def\ortho{\perp}
\def\northo{\nperp}
\def\separate{\downmodels}
\def\nseparate{\ndownmodels}

\def\>{\rangle}
\def\<{\langle}

\begin{document}

\title{A non-additive generalization of probability theory\\for quantum mechanics and beyond}
\author{Gabriele Carcassi}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}
\author{Christine A. Aidala}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}

\date{\today}


\begin{abstract}
	We present a physically motivated generalization of probability theory that is suitable for classical mechanics, quantum mechanics, and any future physical theory that allows a statistical description. The goal is to put the use of classical and quantum probability in a broader context and show how the current mathematical structures are likely not suitable to solve the open problems in the foundations of physics. For the more mathematically inclined, we will point to areas where new math or generalization of established math is needed. For the more philosophically inclined, we will point to areas where further conceptual work is needed.

	%Given a generic space of ensembles, we can define the fraction capacity as the maximum fraction of a particular ensemble that can be understood as a mixture of ensembles from a given set. This gives a non-additive (i.e. fuzzy) measure that reduces to a probability (i.e. additive) measure for classical spaces and for quantum measurements. We can also define the state capacity as the exponential of the maximum entropy reachable by a mixture of ensembles from a given set. This is also a non-additive measure and reduces to the Liouville measure in classical mechanics and the Hilbert space dimensions for subspaces of quantum mechanics. Conceptually, it gives us a notion of probability that is both theory and interpretation independent. Mathematically, it gives us a measure theoretic generalization of probability. Physically, it may allow to find new way to understand current theory and tool to investigate new ones. The purpose of this paper is to show the core ideas and present questions that may be developed on the mathematical, physical and philosophical side.
	
\end{abstract}

\maketitle

\tableofcontents

%\section{TOC}
%
%Overall goal: to create a generalization of probability theory and measure theory for general physical systems.
%
%General question: how do we represent physical states? Different types of representations: talk about examples, ....
%
%-- General problem
%
%For this work: assume we have a locally convex topological vector space V. E is a bounded convex subset of V. Equipped with an entropy. Topology generated by entropic balls?
%
%Separateness and orthogonality
%
%Statistical quantities
%
%Show that they reduce to standard things in classical mechanics and quantum mechanics.
%
%-- Connection to Choquet theory 
%
%Choquet theory - in the case that the closure of the space is compact -> representation through measures. May not be unique
%
%Choquet simplex. Uniqueness of decomposition and separateness/orthogonality.
%
%-- Connection to signed probability
%
%TODO: literature search on use of signed measure in QM. ( e.g. \href{https://arxiv.org/pdf/2302.00118}{this paper})
%
%Characterize signed measures as an affine representation of the ensemble space (i.e. bounded convex subset of a locally convex topological vector space).
%
%Show that the probability is signed if and only if multiple decomposition, non-additive, no Choquet simplex
%
%Signed probability is unique, but does not characterize the convex set (ensemble space). Note: the other measures are defined on the extreme points. Knowing the extreme points means you know the convex space (i.e. take the hull). The non-additive measure does not characterize the extreme points, so it doesn't really characterize the space.
%
%-- Our non-additive measure approach
%
%State capacity
%
%Fraction capacity. Show uniqueness. Also show uniqueness on extreme points.
%
%Fraction capacity as supremum of all possible measures
%
%Additivity of fraction capacity (and state capacity).
%
%Single decomposition and classical spaces and measurement contexts
%
%Show that they reduce to standard things in classical mechanics and quantum mechanics.
%
%-- Connection to fuzzy logic?
%
%Literature search.
%
%Possible connection to
%* Choquet integral
%* Sugeno integral
%
%? Space of fuzzy measure like we have a space of additive measures?


\section{Introduction}

In the past few decades, there has been a growing interest in developing approaches that generalize the notion of states and physical theories. One basic insight is that statistical/probabilistic concepts often provide a suitable basis for this generalization, and it is in this setting where most of these approaches operate. A key problem, then, is what notion of probability should be used, given that the standard measure theoretic Kolmogorovian approach does not generally work in quantum mechanics.

We reformulate the problem in the following way: given the space of all possible statistical ensembles allowed by a statistical theory, what measure theoretic tools can we use to represent each ensemble and the geometric properties of the overall space? Why exactly do classical probability measures fail to capture completely quantum states? What suitable generalizations are possible, and what mathematical tools would need to be developed?

In this paper, we will characterize a generic space of ensembles as a convex bounded set of a Hausdorff locally convex topological vector space, upon which an entropy function is defined. These mathematical properties are physically grounded in the ability to experimentally define ensemble (leading to a topological space), to define statistical mixtures (leading to a convex space), to quantify the variability of the elements of the ensembles (leading to an entropy function) and to having enough real valued quantities to identify each ensemble (leading to the continuous embedding into a Hausdorff locally convex topological vector space). The question is, then, what measure theoretic tool can quantify the number of states (as in the classical Liouville measure) or the number of possible outcomes of a measurement (as in the dimension of orthogonal subspaces of an operator)? What measure theoretic tool can represent each ensemble? If we are to define a physically meaningful notion of probability density, both of these questions must be answered.

We will first review some results from Choquet theory, which is, to our knowledge, the most general approach to represent elements of a convex set using additive probability measures. The key insight is that, at least in the compact case, such a representation is always possible, but it is not unique. Uniqueness requires the space to be a Choquet simplex, which is a generalization of the standard finite-dimensional simplex and is a particularly shaped subset of a vector space. The space of quantum ensembles is not a simplex for one key reason: it allows multiple decompositions in terms of pure states. These insights will allow to better understand exactly the limits of classical probability to describe ensembles in physical theories.

We will also review some results from the use of quasi-probability measures. While the Wigner function represents the most known instance, there are different choices. Note that the space of signed probability measures over a set $K$ is a vector space, so one can find an affine map between a suitable space of signed probability measures and the embedding vector space. Each ensemble, then, is uniquely represented with a signed probability measure. However, unlike in the Choquet case, the measure is not supported by the set of extreme points of the ensemble space. That is, there is no unique or privileged choice for this representation. Moreover, since the map is at the level of the full vector space, not just the subset with valid ensembles, there is no general way to characterize which subset of signed probability measures corresponds to ensembles. Therefore, while signed measures are a powerful tool for calculation in specific problems, they cannot give us a general principled representation of the physical theory.

The above two approaches exhaust what can be done with additive measures, so we turn to the non-additive case. From the entropy, we can define the state capacity, which generalizes the notion of the count of states. For each ensemble, we can define the fraction capacity, which generalizes the notion of probability. Both are non-negative sub-additive monotonic set functions, with the second being unit bounded. These measures are always unique and, since they are defined on the ensemble space itself, always provide a physically meaningful representation. Moreover, in the compact case, these measures can be restricted to the extreme points while retaining uniqueness. (TODO: prove)

The fraction capacity is the supremum of all possible measures for the same ensemble. The classical case exactly corresponds to the simplex case, in which fraction capacity corresponds to a unique measure and is therefore additive. The state capacity, instead, corresponds to the minimum of the entropy of all possible representations. It recovers the Liouville measure in the classical case and the dimensionality of the Hilbert space in the quantum case.

Having shown that a general approach based on non-additive measures is possible, we lay down a series of questions to explore in the future. The biggest one is whether it is possible to generalize notions such as integration, derivation, and expectation to this non-additive case.  It should be noted that non-additive measures are used in the context of decision theory, so there may be a connection between the use of non-additive measures in quantum theory and interdependent complementary choices.[TODO: find citation that sub-additive is when choices are complementary] Both the Choquet and the Sugeno integral, for example, are defined over a monotonic set-functions, so there is a chance that these tools may turn out to be useful. We see this work as starting this line of research, which will take more effort to investigate.

A final remark before we start. Too often, works in the foundations of physics focus on a particular aspect without concern for a broader connection to all relevant mathematical, physical, and philosophical aspects. Therefore, it is important to us that this work is a piece of a much larger puzzle, which connects cleanly and directly to other areas, such as information geometry, functional analysis, quantum logic, and so on. It is the hallmark of a true fundamental structure for physics that such connections can be easily made. Unfortunately, the nature of scientific publication requires us to publish a narrow slice of the work to fit the scope of a particular journal. Ironically, this means that the most important motivations for our approach are out of scope of this article, as we will concentrate only on the connections to measure theory.

\section{Ensemble space}

Conceptually, a statistical ensemble is a collection of outputs, taken at once, of a reproducible preparation procedure for a physical system. In the classical case, this corresponds to a probability distribution over a discrete set of outcomes (i.e. a fair coin) or over phase space (i.e. the symplectic manifold representing position and momentum for a classical particle). In the quantum case, this corresponds to a mixed state (i.e. a positive semi-definite self-adjoint operator of trace one). An ensemble space is the collection of all possible ensembles defined in a physical theory, together with a basic mathematical structure that all physical theories must possess. The development of a full theory of ensembles goes beyond the scope of this work, and we refer to the appendix and our ongoing work (cite).

For the purpose of this paper, we will assume an ensemble space $\Ens$ is a convex subset of a Hausdorff locally convex topological vector space $V$. The topology is the physically natural topology, where each open set represents a verifiable statement. The convex structure represents statistical mixing. For example, $p \ens[a] + \bar{p} \ens[b]$ represents the statistical mixture of two ensembles $\ens[a]$ and $\ens[b]$ with probability $p$ and $\bar{p} = 1 - p$. Only finite mixtures are guaranteed, while the topology will decide whether an infinite mixture $\sum_{i}^{\infty} p_i \ens_i$ converges in the space or not. We also assume that there is a countable family of affine continuous functions $F_i : \Ens \to \mathbb{R}$ that represent statistical quantities and fully identify each ensemble. That is, $F_i(p \ens[a] + \bar{p} \ens[b]) = p F_i(\ens[a]) + \bar{p} F_i(\ens[b])$ and $\ens[a] = \ens[b]$ if and only if $F_i(\ens[a]) = F_i(\ens[b])$ for all $i$.\footnote{It is an open question whether all ensemble spaces must possess this property.} For example, we can imagine $F_i$ representing the expectation of all polynomials of position and momentum.\footnote{This requires all probability distributions to converge to zero at infinity faster than any polynomial.} Mathematically, these statistical quantities can be used to define semi-norms on the space, giving us a locally convex topology.

Additionally, each ensemble space $\Ens$ is equipped with a continuous entropy function $S : \Ens \to \mathbb{R}$. The entropy of a statistical mixture satisfies the following bounds
\begin{equation}
	\begin{aligned}
		p S(\ens[a]) + (1-p) &S(\ens[b]) \leq S(p \ens[a] + (1-p) \ens[b]) \\
		&\leq I(p,\bar{p}) + p S(\ens[a]) + (1-p) S(\ens[b])
	\end{aligned}
\end{equation}
where $I(p,\bar{p}) = - p \log p - \bar{p} \log(\bar{p})$.\footnote{In the larger work, we can show that the Shannon entropy is the only upper bound possible.} The bounds come from assuming that the entropy quantifies the variability of each ensemble. Intuitively, the variability cannot decrease during mixing, and will have an upper bound when mixing two ensembles that are non-overlapping, or orthogonal. In fact, we define two ensembles $\ens[a] \ortho \ens[b]$ to be orthogonal when their mixture gives a maximal entropy increase. This does recover the notion of orthogonality in the corresponding classical and quantum vector spaces of statistical ensembles. Lastly, mixtures preserve orthogonality in the sense that mixing non-overlapping ensembles still gives non-overlapping ensembles. More precisely, $\ens[a] \ortho \ens[b]$ and $\ens[a] \ortho \ens[c]$ if and only if $\ens[a] \ortho p \ens[b] + \bar{p} \ens[c]$ for any $p \in (0,1)$.

To summarize, the above mathematical structure represents the requirements that ensembles must be experimentally well-defined, allow statistical mixture, can be identified with statistical quantities and provide a well-defined entropy.\footnote{While the full physical justification of the above mathematical structure is beyond the scope of this work, it is important to us that such justification exists and this physically straight forward. This is the essence of what we call Physical Mathematics. We will summarize some of these justifications in the appendix, leaving the full and more updated discussion in our work in progress.} We now turn to the question of how we can represent both the entropy and each ensemble in terms of some measure theoretic structure.

\section{Additive measure and Choquet theory}

[This section has to make clear in what extra conditions on ensemble spaces Choquet theory can be applied.]

% Intro paragraph that says quickly how to connect to Choquet theory and its limitations. It should be a quick summary of the whole section
Since ensemble spaces are a convex subset of a locally convex topological vector space, they can be connected to Choquet theory.

% Quick review of Choquet theory
Let us first give a summary of the main results in Choquet theory that are interesting for our purposes. First of all, let $E$ be a convex subset of a locally convex space $V$. We say that $\ens \in E$ is represented by a probability measure $\mu : \Sigma_{E} \to [0,1]$ if $f(\ens) = \int_E f d\mu$ for every continuous linear functional $f$ on $V$. The Choquet theorem states:
\begin{thrm}[Choquet]
	Let $E$ be a metrizable compact convex subset of a locally convex space $V$, and let $\ens \in E$. Then there exists a probability measure $\mu_{\ens}$ on $X$ which represents $\ens$ and is supported by the extreme points of $X$.
\end{thrm}

% Physical meaning of representation with a measure.
First, let us understand the physical significance. An ensemble $\ens$ can be seen as a probability measure $\mu$ over the ``pure states'' (i.e. the extreme points of $E$) if the value for every statistical quantity $f$ for $\ens$ can be recovered as an expectation over the pure states using the probability measure. That is, all statistical properties of $\ens$ can be recovered by the probability measure. This is what allows us to think of $\ens$ as a probability distribution. Note, however, that the Choquet theorem does not guarantee uniqueness. The probability measure is unique if and only if $E$ is a Choquet simplex, which is a generalization of the finite dimensional simplex. The characterization of the Choquet simplex is fairly technical, so we will simply define the Choquet simplex by the property of having a single decomposition in terms of extreme points.


% Limitation of applying Choquet theory to our case
We should now look at the requirements of the theorem to see whether we can always apply it to an ensemble space. First of all, an ensemble space $\Ens$ is a convex subset of a convex space $V$. Moreover, since every Hausdorff first-countable topological vector space is metrizable, $\Ens$ is metrizable. However, $\Ens$ is not necessarily compact. In fact, $\Ens$ is not necessarily a closed subset of $V$. Consider the space of probability measures over $\mathbb{R}$. Since $\mathbb{R}$ is not compact, the space of probability measures over $\mathbb{R}$ cannot be compact.\footnote{TODO: Check with mathematicians} Moreover, the set of extreme points is given by the Dirac measures (i.e. probability one over a single point), which would have minus infinite entropy, and therefore cannot be part of the ensemble space. Therefore, the theorem does not apply in general.

% Define "Choquet space" as those spaces where we can apply Choquet theory
We can apply Choquet's theorem in the case where the closure of $\Ens$ in $V$ is a compact set $E$, which will also be convex\footnote{The closure of a convex set is convex. TODO: finalize the proof} and metrizable. In this case, we are guaranteed that all ensembles allow a representation through a probability on points that may correspond to idealized ensembles (i.e. the Dirac measure) that are not, even conceptually, realizable. Many interesting cases fall into this restriction: the ensemble spaces of classical probability distributions over finitely many discrete cases fall in this restriction; the subset of ensembles over finite ranges of position and momentum; the ensemble spaces of all finite dimensional quantum systems.

Note that ensembles over classical spaces will be represented by a unique probability measure, therefore the corresponding closure of the ensemble space is a Choquet simplex. However, quantum mixed states can be decomposed in multiple ways in terms of pure states, and therefore, the closure of a quantum ensemble space is not a simplex.

% space is metrizable https://proofwiki.org/wiki/Birkhoff-Kakutani_Theorem/Topological_Vector_Space

Chouqet theory tells us, in the realm where it can be applied, that classical probability fails in quantum mechanics precisely because the ensemble space of a quantum system allows multiple decompositions.

\section{Quasiprobability}

%Goal of this section: quasi-probability representations are allowed by the embedding in a topological vector space. Choice of representations are guided by what observables are interesting for a specific problem.

%Examples: Wigner, Husimi, Glauber P, SIC-POVM (space not made of spectra of operators), Kirkwood-Dirac (two-time correlation function - observables not necessarily taken at the same time )

%Things to say: some people think about it as weakining axioms (cite papers on negative probability). It's more than that. Points describe events that are not necessarily mutually exclusive. TODO: example of two Wigner function/Husimi Q would be better that have no overlap but they are not orthogonal and/or two functions that have overlap but are orthogonal.

A measure $\mu: \Omega_X \to \mathbb{R}$ over a sample space $X$ with $\sigma$-algebra $\Omega_X$ is called a probability measure if it satisfies the following three axioms\footnote{TODO: These should be stated earlier.}
\begin{labeling}{Non-negativity:}
	\item[Non-negativity:] $\forall x \in \Omega_X: \mu(x) \ge 0$
	\item[Normalization:] $\mu(X) = 1$
	\item[$\sigma$-additivity:] $\mu ( \bigcup_i x_i ) = \sum_i \mu ( x_i )$ for any countable sequence of disjoint sets $x_i \in \Omega_X$.
\end{labeling}

%Thing to say: all these representations are useful because they preserve the linear structure, meaning that convex combinations of ensembles are convex combinations of quasi-probability; expectation values are linear. Check, but it should be true.

% 1 describe quasiprobability approaches; go through examples; point out that the quasi-probability can be negative; that one can recover expectations;

\textbf{Quasi-probability representations.} Another way to represent quantum ensembles is through the use of quasiprobability distributions. While the Wigner function is probably the most widely known, there are in fact several different approach. The Wigner function represent quantum states, both pure and mixed, as a density function over position and momentum. This function integrates to one over the full space, like a probability density, but can have small negative regions, unlike a probability density. Recall that an affine combination is a linear combination where all coefficients sum to one but are not necessarily non-negative. Therefore, in general, quasi-probability distributions can be understood as affine combinations.

For the Wigner function, the expectation of polynomials of position and momentum over phase space recovers expectations of polynomials of position and momentum in symmetric order (?). (Definitions and features?) In some problems, especially in quantum optics, normal ordering and anti-normal ordering are more useful. The Glauber P representation and the Husimi Q representation work like the Wigner function except that the expectations of polynomials recover the respective order. In other circumstances, quasi-probability distributions over observables different from position and momentum are more useful. For example, the Kirkwood-Dirac is used to represent two-time correlation functions over observables defined at different times. Quasi-probability representations are also used over discrete elements that may not even correspond to eigenstates of a particular operator. For example, the SIC-POVM represents density matrices as affine combinations of pure states that are ``equally distant'' from each other, which means that no two elements are orthogonal to each other.

It should be clear that there are many ways to represent quantum states via quasi-probability distributions, and that the choice of representation is a matter of convenience for the problem at hand. All representations, though, have the following features in common. First, we have a map $W : \Ens \to Q(Z,\mathbb{R})$ that given each mixed state $\rho \in \Ens$ returns the corresponding quasi-probability function $w(z) \in Q(Z,\mathbb{R})$, where $Q(Z,\mathbb{R})$ is the function space used by the specific representation. The representation preserves convex combinations: $W(\sum_i p_i \rho_i) = \sum_i p_i W(\rho_i)$. That is, a statistical mixture of quasi-probability distributions represents the statistical mixture of the corresponding states. Since the map is linear, and expectation values are linear, one can express expectation values as integrals or sums over the quasi-probability $w(z)$. 

% 2 Note that it's not about negative probability; disjoint not mutually exclusive

\textbf{Not about negative probability.} It is very common to conceive the main difference between classical probability and these quasi-probabilities as the inclusion of negative probability. Sometimes these negative probabilities are justified as arising from constraints on probability distributions defined on different contexts. This is a mischaracterization. For example, the Husimi Q representation provides us with a non-negative quasi-probability function. That is, the Husimi Q is a non-negative density distribution that integrates to one, yet it is not a probability distribution.

The crucial difference is that in classical probability all points of the sample space represent mutually exclusive events. That is, they can be prepared and observed independently from the others. Therefore, any two events that are disjoint are also mutually exclusive. This is not the case for quasi-probability in quantum mechanics. Husimi Q represents a state as decomposed over coherent states, which cannot represent mutually exclusive as they are not orthogonal. Similarly, we can imagine to embed the Bloch ball into a tetrahedron, which means every point in the Bloch ball could be represented as a convex combination of extreme points, meaning that they would be represented by a non-negative quasi-probability distribution over four elements. However, none of the four elements represent a valid state.

% 3 These representations are possible because of the vector space structure of the ensemble space, and therefore something similar will be possible in ANY physical theory

For example, affine measures do not necessarily satisfy non-negativity. The Wigner function is the probability density of such a measure and can be negative over small regions of phase space.

Given a space $X$, the space of all affine measures is a vector space, not just a convex subset as in the case of the space of probability measures. This means that, given an ensemble space, we may find a space $X$ such that the space of affine measures over $X$ is isomorphic to the embedding vector space $V$ as topological vector spaces. In this case, any element $v\in V$ can be represented uniquely by an affine measure $\mu_{v}$, regardless of whether they are actual ensembles or not.\footnote{The question of whether such an $X$ always exists is left open. We suspect a link exists between the statistical quantities $F_i$ that fully identify each ensemble and the space $X$.} Because the statistical quantities $F_i$ are affine functions, we will have $F_i(v) = \int_X F_i d\mu_{v}$.

In general, since the ensemble space is bounded along every direction, not all affine measures will correspond to an ensemble space. Whether all probability measures correspond to a valid ensemble, or whether affine measures are even used, corresponds to relationships between $\Ens$ and the space of probability measures $M_p(X)$. If $\Ens \subset M_p(X)$, then all states are represented by a probability measure. However, this is not a ``probability'' in the standard sense of the word. For example, the Husimi Q representation is always non-negative at every point, but two points do not represent mutually exclusive events as they are not orthogonal (i.e. their inner product is non-zero and they do not maximize the entropy increase during mixture). Mathematically, the issue is that the points $X$ of the Husimi Q representation are not the extreme points of the closure ensemble space $\overline{\Ens}$. In general, $\Ens$ and $M_p(X)$ will overlap, but neither will contain the other. For example, in the Wigner representation, the only pure states (i.e. the extreme points of $\Ens$) represented by a probability measure are the Gaussian wave-packets. This means that any other non-negative Wigner function will be in the convex hull of Gaussian wave-packets, and therefore will allow a decomposition in terms of them.

Note that if $\overline{\Ens} = M_p(X)$, then the space of the extreme point is exactly the space of Dirac measures, and therefore $X$ corresponds to the pure states. Alternatively, either one ensemble is represented by an affine measure, or not all probability measures can be understood as mixtures of extreme points. If $\overline{\Ens}$ is compact, then this is exactly when $\overline{\Ens}$ is not a simplex and multiple decompositions are allowed.


% 4 pros and cons - can choose a repretation for the problem at hand; no fundamental representation; boundaries ore not well defined (i.e. which functions represnt valid states)

This alternative way of looking at measure theoretic representations, then, is consistent with the previous one. While quasiprobability distributions are very useful in practice, we want to stress that there is no single choice for $X$. To solve a specific problem, this may actually be useful as one can choose the most convenient $X$ for the problem. To develop a general theory for physics, however, this is not useful as each representation is not general enough.


\section{Fraction capacity}

TODO: intro

Given that mixing coefficients have a more general applicability than probability of outcomes, let us concentrate on the following problem: given an ensemble $\ens \in \Ens$ and a (Borel) set of ensembles $A \subseteq \Ens$, what fraction of $\ens$ can be constructed by a mixture of $A$? How much of $\ens$ can be explained as coming from preparations corresponding to $A$? For example, let $\Ens$ be the space of all probability distributions for a six-sided die. Let $\ens_{123456}$ be the uniform distribution over all outcomes. Let $A_1 = \{\ens_1\}$ where $\ens_1$ represents outcome one with 100\% probability. Since we can write $\ens_{123456} = \frac{1}{6} \ens_{1} + \frac{5}{6} \ens_{23456}$, where $\ens_{23456}$ represents a uniform distributions over the five outcomes, $1/6$ of $\ens$ can be constructed from $A$, but no more. Similarly, if $A_{12} = \{\ens_{1},\ens_{2}\}$, we can write $\ens = \frac{1}{3} \left(\frac{1}{2} \ens_1 + \frac{1}{2} \ens_2 \right)  + \frac{2}{3} \ens_{3456}$, so $1/3$ of $\ens$ can be constructed from $A$, but no more. Note that, given the uniform distribution, $1/6$ is exactly the probability for the event $A_1$ and $1/3$ the probability for event $A_{12}$. This gives the basic insight for our definitions.

Given a target ensemble $\ens \in \Ens$ and an arbitrary ensemble $\ens[a] \in \Ens$, we define the \textbf{fraction} of $\ens[a]$ in $\ens$ to be
\begin{equation}
	\fraction_{\ens}(\ens[a]) = \sup(\{ p \in [0,1] \, | \, \exists \, \ens_1 \in \Ens \text{ s.t. }  \ens = p \ens[a] + \bar{p} \ens_1 \}).
\end{equation}
The fraction is always well-defined because we can always write $\ens = 0 \ens[a] + 1 \ens$, therefore it must be zero or greater. This quantity tells us how much of the ensemble $\ens$ can be constructed from $\ens[a]$. 

We now extend this idea from a single ensemble $\ens[a] \in \Ens$ to a Borel set of ensembles $A \subset \Ens$. Given that the $\hull(A)$ is the set of all possible convex combinations (i.e. mixtures),\footnote{Since we work with convex sets within a topology, the hull is both the closure under the topology and convex combinations. That is, $\hull(A)$ is the smallest closed and convex set that contains $A$} we define the \textbf{fraction capacity} of $A$ for $\ens$ to be
\begin{equation}
	\frcap_{\ens}(A) = \sup(\fraction_{\ens}(\hull(A))\cup\{0\}).
\end{equation}
This returns the biggest fraction of $\ens$ that can be achieved with a mixture of elements of $A$.\footnote{The name fraction capacity is chosen to both signify the ability of the set $A$ to contain $\ens$ (i.e. fraction capacity of one means $\ens$ is within the convex combinations of $A$) and to indicate that it will be a non-additive monotonic measure, which are called ``capacities'' in some literature. TODO \href{ https://link.springer.com/book/10.1007/978-3-319-03155-2}{cite} }

One can then show that the fraction capacity $\frcap_{\ens} : \Sigma_{\Ens} \to [0,1]$ is a set function that satisfies the following:
\begin{enumerate}
	\item non-negative and unit bounded - $\frcap_{\ens}(A) \in [0,1]$
	\item monotone - $A \subseteq B \implies \frcap_{\ens}(A) \leq \frcap_{\ens}(B)$
	\item sub-additive - $\frcap_{\ens}(A \cup B) \leq \frcap_{\ens}(A) + \frcap_{\ens}(B)$
	\item continuous from below and above - $\frcap_{\ens}(\lim\limits_{i \to \infty} A_i) = \lim\limits_{i \to \infty} \frcap_{\ens}(A_i)$ for any increasing or decreasing sequence $\{A_i\}$.
\end{enumerate}

The first property readily comes from the range of the fraction. For the second, note that $\hull$ and $\sup$ are both monotone. For sub-additivity, note that the fraction of two components can at most sum during mixing. For the last, increasing and decreasing sequences strictly add or remove possible mixtures; therefore, they will lead to increasing or decreasing sequences of real numbers bounded between zero and one. The limit of these sequences must agree with the fraction capacity of the limit. If not for the lack of additivity over disjoint sets, the fraction capacity would be a probability measure.

Our setup is very similar to what happens in Choquet theory. The fraction capacity, in fact, is not just defined on the extreme points, but it is defined on the whole space of ensembles. In the classical discrete case, for example, if $A \in \Omega \ni \Ens$ is an event, then $\frcap_{\ens}(A)$ will return the probability of event $A$; if $P \subset \Ens$ represents all the Poisson distributions with integer coefficient, $\frcap_{\ens}(P)$ will return how much of $\ens$ can be expressed as a mixture of those distributions. Similarly, the probability measures in Choquet theory are, in general, defined on the whole ensemble space, and then one finds those particular ones that have support on the extreme points.

We can establish a direct connection between the fraction capacity and Choquet theory, because the ability to express an ensemble $\ens$ as the convex combination of $A$ is precisely the ability to find a measure $\mu$ that has support within $A$. That is, $\ens = \sum_{i=1}^{\infty} p_i \ens[a]_i$ if an only if the probability measure defined by $\mu(\{\ens[a]_i\}) = p_i$ represents $\ens$.\footnote{TODO: add proof somewhere} Consequently, if $M_{\ens}$ is the set of all probability measures that represent $\ens$ as defined in Choquet theory, then $\frcap_{\ens}(A) = \sup(\{\mu(A) \, | \, \mu \in M_{\ens}\})$.\footnote{TODO: prove somewhere}

As we saw, the classical case corresponds to a simplex in Choquet theory. This means that each element of the ensemble space is represented by a unique probability measure over the extreme points. In that case, the fraction capacity, extended to the extreme points, will also be additive. In the non-classical case, the closure of the ensemble space will not be a simplex, and there will be multiple representations, which will lead to a non-additive measure. Suppose, in fact, that $\mu$ and $\nu$ are two distinct measures that represent the same ensemble $\ens$. Then there will be a set $A$ such that $\mu(A) \neq \nu(A)$. Suppose, without loss of generality, that $\mu(A) > \nu(A)$ We will also have $\mu(A^{\complement}) = 1- \mu(A) < 1 - \nu(A) = \nu(A^{\complement})$. Since the fraction capacity for a set cannot be less than the measure on that set, we have $\frcap_{\ens}(A \cup A^{\complement})=\frcap_{\ens}(X) = 1 = \mu(A) + \mu(A^{\complement}) < \mu(A) + \nu(A^{\complement}) \leq \frcap_{\ens}(A) + \frcap_{\ens}(A^{\complement})$. That is, the fraction capacity is additive over disjoint sets of extreme points in and only in the classical case. In other words, the fraction capacity reduces to a classical probability distribution over pure states in the classical case.

\section{State capacity}

We now want to introduce a measure theoretic representation of the entropy that generalizes the notion of count of states. Since entropy characterizes the variability of the preparations of the ensemble, greater variability means the ensemble is spread over more cases. Therefore, the count of distinguishable states must be a monotonic function of the entropy. It has been shown[cite Hall] under very general conditions that the exponential of the entropy is the only function to have the appropriate features. One required feature, that we prove in general, is that the exponential of the entropy is subadditive (i.e. the count of states associated to a mixture cannot exceed the combined count of states associated to both mixtures) and it is additive for orthogonal ensembles (i.e. no overlap in possible instances) in a specific linear combination.

Given a Borel set of ensembles $A \subseteq \Ens$, we define the \textbf{state capacity} of $A$ to be
\begin{equation}
\capacity(A) = \sup(2^{S(\hull(A))}\cup\{0\}).
\end{equation}
This returns the exponential of the biggest entropy achievable with a mixture of elements of $A$.

One can then show that the fraction capacity $\capacity_{\ens} : \Sigma_{\Ens} \to [0,\infty]$ is a set function that satisfies the following:
\begin{enumerate}
	\item non-negative - $\capacity(A)\geq 0$
	\item monotone - $A \subseteq B \implies \capacity(A) \leq \capacity(B)$
	\item sub-additive - $\capacity(A \cup B) \leq \capacity(A) + \capacity(B)$
	\item additive over orthogonal sets - $\capacity(A \cup B) = \capacity(A) + \capacity(B)$ if $A \ortho B$
	\item continuous from below and above - $\capacity(\lim\limits_{i \to \infty} A_i) = \lim\limits_{i \to \infty} \capacity(A_i)$ for any increasing or decreasing sequence $\{A_i\}$.
\end{enumerate}

The first property readily comes from the range of the exponential. For the second, note that $\hull$, exponential, and $\sup$ are all monotone. For sub-additivity, the exponential of the entropy itself is sub-additive, and it is additive over orthogonal ensembles. For the last, since the entropy itself is continuous, this property carries over to the set function.

Again, we note that the non-additive measure we recover is very well-behaved and connects directly with important quantities. When Choquet theory applies, state capacity can be extended to the extreme points. In the classical case, disjoint sets of extreme points will be orthogonal, meaning that the state capacity there restricted is an additive measure. In the discrete case, every point is associated with zero entropy, and since $2^0=1$, the state capacity reduces to the counting measure. In the continuous case, given a subset of phase space $A$, the maximum entropy is reached by the uniform distribution $\rho_A$, which means we recover $\log_2 \capacity(A) = S(\rho_A)$, the fundamental postulate of statistical mechanics. Similarly, in quantum mechanics, given an orthogonal subspace $A$, the maximum entropy is recovered by the maximally mixed state, which means $\capacity(A) = 2^{\dim(A)}$.

The state capacity, then, recovers the number of distinguishable states. In classical mechanics, all states are perfectly distinguishable. In quantum mechanics, only orthogonal states are distinguishable, and therefore the state capacity recovers the number of orthogonal states.

\section{Connection to fuzzy logic}



\section{Possible generalization of calculus}

As we pointed out, the state capacity and the fraction capacities are always well-defined, even in the cases where, potentially, Choquet theory does not apply because the closure of the ensemble space is not compact.

General problems:
1) Can we always find extreme points?
2) Can we generalize to the non-compact case? Can \href{https://en.wikipedia.org/wiki/Krein%E2%80%93Milman_theorem#convex_compactness}{convex compactness} help?
3) In the classical case, both measures become additive on the extreme points. Standard calculus is used. Is there a non-additive version of calculus that generalizes?

\section{Conclusion}



\section*{Acknowledgments}
This paper is part of the ongoing \textit{Assumptions of Physics} project \cite{aop-book}, which aims to identify a handful of physical principles from which the basic laws can be rigorously derived. This article was made possible through the support of grant \#62847 from the John Templeton Foundation.


\bibliography{bibliography}

\newcommand{\pj}[1] {\underbar{$#1$}}


\end{document}
