\documentclass[10pt,twocolumn, nofootinbib]{revtex4-2}

\usepackage{assumptionsofphysics}
\usepackage{tikz}
\usepackage{breakurl}

\newcommand\hull{\mathrm{hull}}
\newcommand\stcap{\mathrm{scap}}
\newcommand\fraction{\mathrm{frac}}
\newcommand\frcap{\mathrm{fcap}}

\newcommand{\ens}[1][e] {\mathsf{#1}} % Ensemble
\newcommand{\Ens}[1][E] {\mathcal{#1}} % Ensemble space


\def\>{\rangle}
\def\<{\langle}
\DeclareMathOperator{\erf}{erf}

\begin{document}

\title{A non-additive generalization of probability theory \\for quantum mechanics and beyond}
\author{Gabriele Carcassi}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}
\author{Christine A. Aidala}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}

\date{\today}


\begin{abstract}
	We present a physically motivated generalization of probability theory that is suitable for classical mechanics, quantum mechanics and any future physical theory that allows a statistical description. The goal is to put the use of classical and quantum probability in a broader context, and show how the current mathematical structures are likely not suitable to solve the open problems in the foundations of physics. For the more mathematically inclined, we will point to areas where new math or generalization of established math are needed. For the more philosophically incline, we will point to areas where further conceptual work is needed.
	
	
	%Given a generic space of ensembles, we can define the fraction capacity as the maximum fraction of a particular ensemble that can be understood as a mixture of ensembles from a given set. This gives a non-additive (i.e. fuzzy) measure that reduces to a probability (i.e. additive) measure for classical spaces and for quantum measurements. We can also define the state capacity as the exponential of the maximum entropy reachable by a mixture of ensembles from a given set. This is also a non-additive measure and reduces to the Liouville measure in classical mechanics and the Hilbert space dimensions for subspaces of quantum mechanics. Conceptually, it gives us a notion of probability that is both theory and interpretation independent. Mathematically, it gives us a measure theoretic generalization of probability. Physically, it may allow to find new way to understand current theory and tool to investigate new ones. The purpose of this paper is to show the core ideas and present questions that may be developed on the mathematical, physical and philosophical side.
\end{abstract}

\maketitle


\section{Introduction}
Paper for physicists:
* argue that we need to develop mathematical structures from physical requirements
* create a theory of probability that works for classical, quantum mechanics and beyond
* inverted structure

\section{Ensemble spaces}

We are looking to find a minimal set of axioms required by a physical theory that allows statistical descriptions. Such a theory must, at the very least, provide us with a notion of ensemble, and the space of ensembles that are allowed in the theory. An ensemble can be understood as the infinite collection of the outputs of a particular preparation procedure. The ensemble space of classical mechanics, for example, is the space of continuous probability distributions of phase space. In quantum mechanics, instead, it is the space of density operators. While the ensemble space for each theory will be in principle different, some will have many common features. In all cases we must be able to talk about conservative or dissipative processes, measurable properties of the system, probability distributions for outcomes of said properties and so on. We are looking for necessary minimal basic requirements upon which these common tools can be built.

Note that there may good conceptual grounds to require that a physical theory allows for statistical descriptions. If a physical theory is to be repeatedly testable experimentally, then we fail to see how it could be so without providing statistical tools. Every state preparation and measurement, in fact, is typically statistical in nature. Moreover, physical laws are statements of the type ``whenever we prepare this, we can measure that.'' A philosophical exploration of this area would be welcome.

We have identified three basic requirements on ensembles: first, they have to be identifiable experimentally; second, they need to allow mixtures; lastly, they need a well-defined entropy. Let us go through these items one by one.

\subsection{Experimental verifiability}
The first requirement is that ensemble must be connected to experimental verification. That is, we must have enough \textbf{verifiable statements} at our disposal to define ensembles and tell them apart. By verifiable statement we mean a statement for which an experimental test is available that terminates in finite time if and only if the statement is true. For example, the statement ``the mass of the photon is less than $10^{-13} \, eV$'' is verifiable, while ``the mass of the photon is exactly $0 \, eV$'' is not verifiable due to its infinite precision. Mathematically, this requirement forces an ensemble space to be a $T_0$ second countable topological space, where open sets represent verifiable statements and Borel sets represent testable statements without guarantee of termination.

The topological structure, then, represent the most foundational structure for a physical theory. It tells us why functions are ``well-behaved,'' which simply means they have to be topologically continuous in the ``natural topology'' induced by this requirement. It tells why the Banach-Tarski paradox does not apply in physics since non-Borel sets are physically ill-defined as they are not connected to experimental verification. Similarly, it tells us that sets with cardinality greater than that of the continuum are physically not relevant, as they cannot be given a $T_0$ second countable topology.

\subsection{Statistical mixtures}
Another basic requirement of an ensemble space is ability to prepare a \textbf{mixture} of two ensembles. If $\ens[a], \ens[b] \in \Ens$ are two ensembles, and $p \in [0,1]$ a weight, we will find the $\ens = p \ens[a] + (1-p) \ens[b] \in \Ens$ that describes a process that selects the first ensemble over the second $p$ percent of the times. Mathematically, the ensemble space is endowed with a \textbf{convex structure}.

Ultimately, the convex structure is responsible for all linear structures we have in physics. Both in classical and quantum mechanics, the ensemble space is not just a convex set, but a convex subset of a vector space. In those cases, in fact, the convex structure is, in a sense, invertible. If we fix $\ens$, $\ens[a]$ and $p$ then, if it exists, there is only one $\ens[b]$ such that $\ens = p \ens[a] + (1-p) \ens[b]$. The ensemble space is \textbf{complement}, meaning that there is only one complement to a component of an ensemble.

However, we found no physical justification for this requirement, and we suspect that it may need to be relaxed in future physical theories.

\subsection{Entropy}
The last requirement is that each ensemble must have a well defined entropy. That is, there is a scalar function $S : \Ens \to \mathbb{R}$ that satisfies the following
\begin{enumerate}
	\item strictly convex: $S(p \ens[a] + (1-p) \ens[b]) - (p S(\ens[a]) + (1-p) S(\ens[b]) ) \geq 0$
	\item upper bounded: $S(p \ens[a] + (1-p) \ens[b]) - (p S(\ens[a]) + (1-p) S(\ens[b]) ) \leq - p \log p - (1-p) \log(1-p)$
\end{enumerate}
The first tells us that, during mixing, the entropy cannot decrease, and it stays the same if and only we are mixing an ensemble with itself. The second tells us that the most the entropy can increase is given by the choice between the two ensembles.

The entropy is ultimately responsible for all geometrical structure in physics. In classical mechanics, the geometry is essentially defined by volumes in phase space. The entropy of uniform distributions is given by the logarithm of the volume, meaning that given the volumes we are able to calculate the entropy and given the entropy we are able to reconstruct the volume. In quantum mechanics, the inner product (i.e. the Born rule) allows us to calculate the entropy of the mixture of two pure states, and given that entropy we can recover the Born rule.

An interesting insight is that the entropy, being strictly concave, has a negative defined Hessian. The negation of the Hessian, then, is a symmetric positive defined function of two infinitesimal variations, and can serve as a metric tensor. In both classical and quantum mechanics this recovers the Fisher-Rao metric.

Another important feature is that if two ensembles saturate the upper bound, then they are orthogonal in both classical and quantum mechanics. Therefore we define two ensembles to be \textbf{orthogonal} if their mixture lead to a maximal entropy increase. If two states are orthogonal, on physical grounds, the must have no common component. However, the reverse is not case: two pure states in quantum mechanics have no components to have in common, but are not necessarily orthogonal. However, classical probability spaces are exactly those space in which no common component implies orthogonality. This is the main source of the ``weirdness'' of quantum mechanics.

\subsection{Interplay between structures}

Each structure is important, but the balance between them is even more important. The key is to understand, on physical grounds, where each problem lies and therefore use the appropriate tool. For example, one may focus too much on the convex/linear structure and may be tempted to close the space under infinite convex/linear combinations. Mathematically, it would be very convenient. Physically, it is unsound. It is the link to experimental verification that tells us which limits are allowed, that particles cannot have an infinite average position. Therefore, it is the topology that has to tells us which limits are allowed, not the convex structure. Conceptual clarity has to come first. The math follows. A full examination of these issues is beyond the scope of this article, and we refer to our larger, and future, work. However, there is one key issue that we need to discuss as, we found, it was the main conceptual obstacle to progress.

On the convex structure we can define \textbf{pure states} as the ones that cannot be decomposed into other ensembles. Mathematically, these are the extreme points of the space and all other points can be understood as convex combinations, probability distributions, over them. So, for example, the pure states for a six-sided die are the six possible outcomes, and all ensembles are simply convex combinations $\sum_{i+1}^{6} p_i = 1$. This is exactly how classical discrete spaces work. As physicists, we are somehow trained to think that that's how everything works. In classical mechanics the pure states are perfect preparations of position and momentum. In a classical field theory, they are perfect preparation of all fields at each point in space. Things start breaking down in quantum mechanics, where we do have pure states, but then we also want pure states of position and momentum, which kind of work if you are careful, though we typically aren't. And then there is quantum field theory.

If we look closely, this does not make sense on physical grounds. The fact that the math becomes harder is simply a consequence of trying to fit a square peg in a round hole.

Perfect preparation of classical particles, continuous quantities in quantum mechanics and field configurations in any field theory are not things we can actually achieve. Physically, all ensembles we can prepare have finite precision and finite entropy, and those preparations have neither. They are not ensembles and they are not part of the a convex structure. In the classical case, we can understand them as infinitesimal decompositions: convex combinations that have more and more terms. This is true, but we cannot do that in the quantum case. If we have a pure state, this cannot be decomposed. Therefore an eigenstate of position is not a limit of decomposition, a limit in the convex structure, but a limit of a sequence of pure states, a limit in the topology, and the topology will not even include that limit. We may be able to see a pure state, in some sense, as a superposition of eigenstates of position, but that structure is only available in quantum mechanics, and it does not have a clear physical correspondence.

The insight is that probability distributions for a quantity are not defined by mixing the best ensembles we can prepare (i.e. a convex combination of extreme points), but by assigning a probability over each experimentally verifiable (i.e. open) interval of the possible values of said quantity (its spectra). The two coincide over a discrete set of orthogonal pure states, which is the least physically interesting case. If our are to be physically well defined and true generalizations, they must work in the more complicated case. We not only have to be able to talk about probability in terms of mixtures of ensembles, but we must be able to recover the interval of possible values for physical properties (i.e. the spectra), the object we typically assign probability two.

To recap, the topological structure captures experimental verifiability and is responsible for the limits and the connection to measureable quantities. The convex structure captures statistical mixing and is responsible for all linear structures of physical theories. The entropic structure captures the variability within an ensemble and is ultimately responsible for all geometric structure. We now concentrate on recovering the measure theoretic structures associated to probability and state counting.

\section{Fraction capacity}

Note that the mixing coefficient $p$ is connected to probability but it is not itself probability. It represents a parameter for the mixing states, mixing preparations, while probability is typically understood as the chance of something happening, the likelihood of a final event. Mathematically, the probability measure is a function from an event, which can be understood as a collection of fully distinct outcomes, to a number from zero to one. If we are mixing ensemble that represent fully distinct outcomes (i.e. they are orthogonal), then the mixing $p$ is a probability. If we are mixing ensemble that do not represent fully distinct outcomes (e.g. spin up and spin left), the probability of getting one of the outcomes is not the mixing coefficient $p$.

This is exactly what happens in quantum mechanics: probability is defined only on a set of orthogonal states. If we focus only on preparation, then, we only talk about mixing coefficients across non-orthogonal elements and define a more general tool that, when restricted on orthogonal sets, will recover probability.

We concentrate on the following question: given an ensemble $\ens \in \Ens$ and a (Borel) set of ensembles $A \subset \Ens$, what fraction of $\ens$ can be constructed by a mixture of $A$? How much of $\ens$ can be explained as coming from preparations corresponding to $A$? For example, if $\ens$ is a uniform distribution over a fair six-sided die, and $A$ contains four ensemble, each fully on ${1,2}$ and uniform distribution over ${5,6}$ two and four, than $5/6$ of $\ens$ can be constructed from $A$, which is the probability that we d

Given a target ensemble $\ens \in \Ens$ and an arbitrary ensemble $\ens[a] \in \Ens$ we will be able to write $\ens = p \ens[a] + (1-p) \ens[b]$ for some $p \in [0,1]$ and some $\ens[b] \in \Ens$. At the very least, we can use $p=0$ and $\ens[b] = \ens$. Given that $p$ is bounded, there will be a biggest $p$ possible which we call the \textbf{fraction} of $\ens[a]$ in $\ens$. Mathematically
\begin{equation}
	\fraction_{\ens}(\ens[a]) = \sup(\{ p \in [0,1] \, | \, \exists \, \ens_1 \in \Ens \text{ s.t. }  \ens = p \ens[a] + \bar{p} \ens_1 \}).
\end{equation}
This quantity tells us how much of ensemble $\ens$ can be constructed, can be characterized, by $\ens[a]$. 

We now extend this idea from a single ensemble $\ens[a] \in \Ens$ to a set of ensembles $A \subset \Ens$., meaning we want to characterize how much of a target ensemble $\ens$ can be constructed as a mixture of ensembles from $A$. Given a set of ensembles $A$, its \textbf{hull}, noted $\hull(A)$ is the set of all mixtures that can be constructed from $A$. That is, is the set of all convex combinations $\sum_i^n p_i \ens[a]_i$ such that $\{a_i\}_1^n \subseteq A$. The \textbf{fraction capacity} is the biggest fraction among all the elements of the hull. Mathematically
\begin{equation}
	\frcap_{\ens}(A) = \sup(\fraction_{\ens}(\hull(A))\cup\{0\}).
\end{equation}

If $\Ens$ is a topological space, we can imagine the fraction capacity to be defined on the sigma algebra $\Sigma_{\Ens}$. We can then show that the fraction capacity $\frcap_{\ens} : \Sigma_{\Ens} \to [0,1]$ is a set function that satisfies the following:
\begin{enumerate}
	\item non-negative and unit bounded - $0 \leq \frcap_{\ens}(A) \leq 1$
	\item monotone - $A \subseteq B \implies \frcap_{\ens}(A) \leq \frcap_{\ens}(B)$
	\item sub-additive - $\frcap_{\ens}(A \cup B) \leq \frcap_{\ens}(A) + \frcap_{\ens}(B)$.
	\item continuous from below and above - $\frcap_{\ens}(\lim\limits_{i \to \infty} A_i) = \lim\limits_{i \to \infty} \frcap_{\ens}(A_i)$ for any increasing or decreasing sequence $\{A_i\}$.
\end{enumerate}



\section{Statistical properties}

\section{Beyond real valued quantities}


\section{State capacity}

\section{Quantization}
* 3 pick 2

\section{Quantizing space-time}
* we need a non-additive measure on degrees of freedom
*

\section{Conclusion}



\section*{Acknowledgments}
This paper is part of the ongoing \textit{Assumptions of Physics} project \cite{aop-book}, which aims to identify a handful of physical principles from which the basic laws can be rigorously derived. This article was made possible through the support of grant \#62847 from the John Templeton Foundation.


\bibliography{bibliography}

\newcommand{\pj}[1] {\underbar{$#1$}}


\end{document}
