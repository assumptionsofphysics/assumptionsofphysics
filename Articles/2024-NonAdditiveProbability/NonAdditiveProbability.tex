\documentclass[10pt,twocolumn, nofootinbib]{revtex4-2}

\usepackage{assumptionsofphysics}
\usepackage{tikz}
\usepackage{breakurl}
\usepackage{tcolorbox}

\newcommand\hull{\mathrm{hull}}
\newcommand\stcap{\mathrm{scap}}
\newcommand\fraction{\mathrm{frac}}
\newcommand\frcap{\mathrm{fcap}}

\newcommand{\ens}[1][e] {\mathsf{#1}} % Ensemble
\newcommand{\Ens}[1][E] {\mathcal{#1}} % Ensemble space

\def\ortho{\perp}
\def\northo{\nperp}
\def\separate{\downmodels}
\def\nseparate{\ndownmodels}

\def\>{\rangle}
\def\<{\langle}

\begin{document}

\title{A non-additive generalization of probability theory \\for quantum mechanics and beyond}
\author{Gabriele Carcassi}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}
\author{Christine A. Aidala}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}

\date{\today}


\begin{abstract}
	We present a physically motivated generalization of probability theory that is suitable for classical mechanics, quantum mechanics and any future physical theory that allows a statistical description. The goal is to put the use of classical and quantum probability in a broader context, and show how the current mathematical structures are likely not suitable to solve the open problems in the foundations of physics. For the more mathematically inclined, we will point to areas where new math or generalization of established math are needed. For the more philosophically incline, we will point to areas where further conceptual work is needed.
	
	
	%Given a generic space of ensembles, we can define the fraction capacity as the maximum fraction of a particular ensemble that can be understood as a mixture of ensembles from a given set. This gives a non-additive (i.e. fuzzy) measure that reduces to a probability (i.e. additive) measure for classical spaces and for quantum measurements. We can also define the state capacity as the exponential of the maximum entropy reachable by a mixture of ensembles from a given set. This is also a non-additive measure and reduces to the Liouville measure in classical mechanics and the Hilbert space dimensions for subspaces of quantum mechanics. Conceptually, it gives us a notion of probability that is both theory and interpretation independent. Mathematically, it gives us a measure theoretic generalization of probability. Physically, it may allow to find new way to understand current theory and tool to investigate new ones. The purpose of this paper is to show the core ideas and present questions that may be developed on the mathematical, physical and philosophical side.
\end{abstract}

\maketitle

\section{TOC}

Overall goal: to create a generalization of probability theory and measure theory for general physical systems.


General question: how do we represent physical states? Different types of representations: talk about examples, ....

-- General problem

For this work: assume we have a locally convex topological vector space V. E is a bounded convex subset of V. Equipped with an entropy. Topology generated by entropic balls?

Separateness and orthogonality

Statistical quantities

Show that they reduce to standard things in classical mechanics and quantum mechanics.

-- Connection to Choquet theory 

Choquet theory - in the case that the closure of the space is compact -> representation through measures. May not be unique

Choquet simplex. Uniqueness of decomposition and separateness/orthogonality.

-- Connection to signed probability

TODO: literature search on use of signed measure in QM. ( e.g. \href{https://arxiv.org/pdf/2302.00118}{this paper})

Characterize signed measures as an affine representation of the ensemble space (i.e. bounded convex subset of a locally convex topological vector space).

Show that the probability is signed if and only if multiple decomposition, non-additive, no Choquet simplex

Signed probability is unique, but does not characterize the convex set (ensemble space). Note: the other measures are defined on the extreme points. Knowing the extreme points means you know the convex space (i.e. take the hull). The non-additive measure does not characterize the extreme points, so it doesn't really characterize the space.

-- Our non-additive measure approach

State capacity

Fraction capacity. Show uniqueness. Also show uniqueness on extreme points.

Fraction capacity as supremum of all possible measures

Additivity of fraction capacity (and state capacity).

Single decomposition and classical spaces and measurement contexts

Show that they reduce to standard things in classical mechanics and quantum mechanics.

-- Connection to fuzzy logic?

Literature search.

Possible connection to
* Choquet integral
* Sugeno integral

? Space of fuzzy measure like we have a space of additive measures?


\section{Introduction}

In the past few decades, there has been a growing interest in developing approaches that generalize the notion of states and physical theories. One basic insight is that statistical/probabilistic concepts often provide a suitable basis for this generalization, and it is in this setting where most of these approaches operate. A key problem, then, is what notion of probability should be used given that the standard measure theoretic Kolmogorovian approach does not generally work in quantum mechanics.

We reformulate the problem in the following way: given the space of all possible statistical ensembles allowed by a statistical theory, what measure theoretic tools can we use to represent each ensemble? If they are not always available, in what circumstances and with what limitations are they available? Is a suitable generalization possible, and what mathematical tools would need to be developed?

In this paper, we will define a space of ensembles as a convex bounded set of a Hausdorff locally convex topological vector space, upon which an entropy function is defined. These mathematical properties are physically grounded in the ability to experimentally define ensemble (leading to a topological space), to define statistical mixtures (leading to a convex space), to quantify the variability of the elements of the ensembles (leading to an entropy functions) and to having enough real valued quantities to identify each ensemble (leading to the continuous embedding into a Hausdorff locally convex topological vector space). The question is, then, what type of measure theoretic tools can we use to quantify the number of states and to represent each ensemble in the space. Both are needed to be able to define a physically meaningful notion of probability density.

We will first review some results from Choquet theory, which is, to our knowledge, the most general approach to represent elements of a convex set using additive probability measures. The key insight is that, at least in the compact case, such representation is always possible, but it is not unique. Uniqueness requires the space to be a Choquet simplex, which is a generalization of the standard finite-dimensional simplex. Quantum theory does not fall in this case: it is precisely the multiple decomposition of mixed states into pure states that prevents a quantum ensemble space to be simplex.

We will also review some results from the use of signed probability measures. While the most known example is the use of Wigner functions, there are actually several ones and there is also work on developing a general theory of signed probability. Given that the space of signed probability measure is, unlike a simplex, a vector space, we can find an affine map between signed probability measures and the embedding vector space. This gives us a unique representation of each ensemble in term of a singed probability measure. However, unlike in the Choquet case, the measure is not supported by the set of extreme points of the space. That is, there is no unique or privileged choice for this representation. Moreover, since the map is at the level of the full vector spaces, there is no general way to characterize which subset of signed probability measures correspond to ensembles.

At this point, we will show that we can define two types of measures: the state capacity, which generalizes the notion of count of states, and a fraction capacity for each ensemble, which generalizes the notion of probability. Both are non-negative sub-additive monotonic set functions, which the second being unit bounded. These are defined on the ensemble space itself, which means the domain is uniquely defined by the ensemble space. Moreover, in the compact case, these measure can be restricted on the extreme points and still retain uniqueness. The fraction capacity, in fact, reduce to the supremum of all Choquet measures over the extreme points. The classical case exactly corresponds to the simplex case, in which fraction capacity corresponds to the unique measure, and is therefore additive. The state capacity, instead, recovers the Liouville measure in the classical case and the dimensionality of the Hilbert space in the quantum case.

Having shown that a general approach based on non-additive measures is possible, we lay down a series of questions to explore in the future. The biggest one is whether it is possible to generalize notions such as integration, derivation and expectation to this non-additive case.  It should be noted that non-additive measures are used in the context of decision theory, so there may be a connection between the use of non-additive measures in quantum theory and interdependent complimentary choices.[TODO: find citation that sub-additive is when choices are complementary] Both the Choquet and the Sugeno integral, for example, are defined over a monotonic set-functions, so there is a chance that these tools may turn out to be useful. We see this work as starting this line of research, which will take more effort to investigate.

\section{Ensemble space}

Conceptually, a statistical ensemble is a collection of outputs, taken at once, of a reproducible preparation procedure for a physical system. In the classical case, this corresponds to a probability distribution over a discrete set of outcomes (i.e. a fair coin) or over phase space (i.e. the symplectic manifold represent position and momentum for a classical particle). In the quantum case, this corresponds to a mixed state (i.e. a positive semi-definite self-adjoint operator of trace one). An ensemble space is the collection of all possible ensembles defined in a physical theory, together with a basic mathematical structure that all physical theories must possess. The development of a full theory of ensembles goes beyond the scope of this work, and we refer to the appendix and our ongoing work (cite).

For the purpose of this work, we will assume an ensemble space $\Ens$ is a convex subset of a Hausdorff locally convex topological vector space $V$. The topology is the physically natural topology, where each open set represents a verifiable statement. The convex structure represents statistical mixing. For example, $p \ens[a] + \bar{p} \ens[b]$ represent the statistical mixture of two ensembles $\ens[a]$ and $\ens[b]$ with probability $p$ and $\bar{p} = 1 - p$. Only finite mixtures are guaranteed, while the topology will decide whether an infinite mixture $\sum_{i}^{\infty} p_i \ens_i$ converges in the space or not. We also assume that there is a countable family of affine continuous functions $F_i : \Ens \to \mathbb{R}$ that represent statistical quantities and fully identify each ensemble. That is, $F_i(p \ens[a] + \bar{p} \ens[b]) = p F_i(\ens[a]) + \bar{p} F_i(\ens[b])$ and $\ens[a] = \ens[b]$ if and only if $F_i(\ens[a]) = F_i(\ens[b])$ for all $i$. For example, we can imagine $F_i$ representing the expectation of all polynomials of position and momentum.\footnote{In this case, the possible probability distributions would be restricted to those that go to zero at infinity on all variables faster than any polynomial. We will leave these technical details out of the main section of the text as to not overwhelm the reader.} Mathematically, these statistical quantities can be used to define semi-norms on the space, giving us a locally convex topology.

Additional, each ensemble space $\Ens$ is equipped with a continuous entropy function $S : \Ens \to \mathbb{R}$. The entropy of a statistical mixture satisfies the following bounds
\begin{equation}
	\begin{aligned}
		p S(\ens[a]) + (1-p) &S(\ens[b]) \leq S(p \ens[a] + (1-p) \ens[b]) \\
		&\leq I(p,\bar{p}) + p S(\ens[a]) + (1-p) S(\ens[b])
	\end{aligned}
\end{equation}
where $I(p,\bar{p}) = - p \log p - \bar{p} \log(\bar{p})$. The bounds come from assuming that the entropy quantifies the variability of each ensemble. Intuitively, the variability cannot decrease during mixing, and will have an upper bound when mixing two ensemble that are non-overlapping, or orthogonal. In fact, we define two ensembles $\ens[a] \ortho \ens[b]$ to be orthogonal when they mixture gives a maximal entropy increase. This does recover the notion of orthogonality in the corresponding classical and quantum vector spaces of statistical ensembles. Lastly, mixtures preserve orthogonality in the sense that mixing non-overlapping ensembles still gives non-overlapping ensembles. More precisely, $\ens[a] \ortho \ens[b]$ and $\ens[a] \ortho \ens[c]$ if and only if $\ens[a] \ortho p \ens[b] + \bar{p} \ens[c]$ for any $p \in (0,1)$.

To summarize, the above mathematical structure represents the requirements that ensembles must be experimentally well-defined, allow statistical mixture, can be identified with statistical quantities and provide a well-defined entropy.\footnote{While the full physical justification of the above mathematical structure is beyond the scope of this work, it is important to us that such justification exists and this physically straight forward. This is the essence of what we call Physical Mathematics. We will summarize some of these justifications in the appendix, leaving the full and more updated discussion in our work in progress.} We now turn to the question of how we can represent both the entropy and each ensemble in terms of some measure theoretic structure.

\section{Old introduction}

Our current understanding of probability is given in terms of the measure theoretic formalization due to Kolmogorov, and its various philosophical interpretations. However, probability in quantum mechanics, as it is widely known, is not Kolmogorovian, in the sense that it provides results that are not reproducible in standard probability spaces. It follows that our mathematical axioms for probability and their interpretations are insufficient for physical theories.

An alternative measure theoretic strategy, that has been successful is quantum mechanics, is the use of quasi-probability distributions: measures that are still unitary over the whole space, but can have negative values in small regions. However, these haven't resulted in an alternative axiomatic approach with suitable interpretations for at least, in our mind, three reasons. First, it is not clear how negative probability should be interpreted. Second, it is not clear how to characterize the space of valid quasi-probability distributions (e.g. there is no simple rule to specify which $\rho(q,p)$ are valid Wigner functions). Third, there are multiple options for quasi-probability distributions (e.g. Wigner function, Husimi distribution, Sudarshanâ€“Glauber distribution). This lack of foundational clarity presents an additional problem: how do we know whether future physical theories will not require yet another extension?

The goal of this paper is to show that, in the same way that classical probability can be axiomatized through standard measure theory, quantum probability, and in fact any probability used a physical theory, can be axiomatized through non-additive (i.e. fuzzy) measure theory. We will define a generalized concept of probability that is ultimately based on physically motivated axioms, giving the physicist a clear operational motivation to our definitions, the mathematicians a formally well-defined mathematical structure and the philosopher a unified framework in which probability can be interpreted in the context of all physical theories. We are not going to be able to develop a full theory here, as there are still a number of conceptual and technical challenges to be solved that will require work in physics, mathematics and philosophy. However, we will show that there are enough results that point in the same direction, and therefore make it reasonable to invest time and effort to work on the open problems.

The material presented is part of a larger work that aims to coalesce ideas and results from different branch of mathematics and physics. For those interested, the current state of the research is openly available in an open document that will be routinely updated as experts from different fields review and help us improve the overall theory.

In a nutshell, we will start by arguing that physical theory must at least be able to talk about statistical ensembles, and that ensembles must allow the preparation of statistical mixture. Given a target ensemble $\ens$ and a set of ensemble $A$, we define the fraction capacity of $A$ for $\ens$ as the biggest fraction of $\ens$ that can be constructed from another set of ensembles $A$. That is, the highest $p$ such that we can express our ensemble as $\ens = p \ens[a] + (1-p) \ens[b]$, where $\ens[a]$ is a mixture of elements of $A$. For example, if $\ens$ is the uniform distributions for a roll of a six-faced die, and $A$ includes only two ensembles with $100\%$ probability for the outcomes $3$ and $4$ respectively, then the faction capacity of $A$ for $\ens$ will be $\frac{1}{3}$. Note that this coincides with the probability of the event ``$3$ or $4$''. The fraction capacity is a non-negative, unit bounded, sub-additive and continuous set function, which is therefore like a probability measure except for the additivity, which is recovered for classical mechanics and over quantum measurements.

Similarly, given a set $A$ of ensembles we define the \textbf{state capacity} as the exponential of the highest entropy obtainable with a mixture of $A$. Note that the highest entropy of a subspace in both classical and quantum statistical mechanics is given by the uniform distribution, therefore the state capacity recovers the count of states (i.e. the Liouville volume) in classical mechanics and the dimensionality of the Hilbert space in quantum mechanics. The state capacity is also a non-negative, sub-additive and continuous set function. This points to a possible generalization of measure theoretic calculus to a sub-additive case that would be guaranteed to work on all physical theories.

\section{Quick review of probability theory}

The standard way to characterize probability is through measure theory. Given a sample space $\Omega$, which represents the possible outcomes, and a $\sigma$-algebra $\Sigma_{\Omega}$, which represents the events, we define a probability measure $p : \Sigma_{\Omega} \to [0,1]$. Note that the probability is assigned to events, not the outcomes. This is what allows to treat discrete and continuous spaces within the same framework. In physics, all sample spaces are ultimately topological spaces, and the $\sigma$-algebra is typically the Borel algebra (i.e. the smallest $\sigma$-algebra that contains all open sets).

In classical mechanics, the sample space is phase space $M$ given by all possible values of position and momentum for all particles under considerations. Phase space is equipped with another measure, the Liouville measure $\mu : \Sigma_M \to [0, +\infty]$, that returns volume of each region. In statistical mechanics, this volume represents the count of states and is another key ingredient. The probability density over states is given by the Radon-Nikodym derivative $\rho = \frac{dp}{d\mu}$ and the entropy is given by $S(\rho) = - \int_M \rho \log \rho d\mu$. In case of a uniform distribution $\rho_U$ with support $U$, the entropy is given by $S(\rho_U) = \log \mu(U)$.\footnote{Note that entropy, the count of states and the symplectic form that characterizes phase space are deeply intertwined, yet they require different mathematical framework to express them. We suspect that they are "shadows" of a more fundamental mathematical structure, which our work on ensembles space is trying to identify.}

The above definitions do not apply to quantum mechanics. In the Hilbert space formulation, mixed states are represented by density operator (i.e. positive semi-definite self-adjoint operators with trace one). Note that if $\rho(x)$ is an integrable function such that $\int_X \rho(x) dx = 1$, the map $\psi(x) \mapsto \rho(x) \psi(x)$ is a density operator. Therefore the space of probability densities over position are mixed states in quantum mechanics. However, no joint distribution between position and momentum can be given, therefore quantum states cannot be described by the space of probability measures over a single sample space.

Alternative representations over phase space do exist, and they are widely used.(CITE) The use quasi-probability measure, meaning that the measure over the full space is still unitary (i.e. $\mu(M) =1$). However, the measure can be negative for smaller regions, which means it can also be grater than $1$, and there is no simple characterization of which quasi-probability measures are allowed and which aren't. Moreover, there is no simple link between then entropy of the distribution and the Liouville measure.

\section{Ensemble spaces}

Note that our goal is not to construct an abstract probability theory that applies to everything, but rather to construct a specific probability theory that is guaranteed to apply to all physical systems. Therefore we do not to define an axiomatic theory of probability and apply it to physical systems, but rather characterize basic properties of physical states and from those build a theory of probability that fits those definitions. Whether it is applicable to decision theory, economics, or psychology is out of scope. As is common in our larger project, we start from axioms that, in our opinion, represent requirements of scientific practice. We call these \textbf{constitutive assumptions} as they are essential to proceed with the scientific endeavor. The advantage is that the realm of applicability and physical interpretation will be clear from the start. 

We start from requiring a physical theory to allow statistical descriptions: at the very least, it will provide a notion of ensemble and describe which ensembles that are allowed by the theory. An ensemble can be understood as the infinite collection of the outputs of a particular preparation procedure. The ensemble space of classical mechanics, for example, is the space of probability distributions on phase space. In quantum mechanics, instead, it is the space of density operators. While the ensemble space for each theory will in principle be different, they will all share common features: they must allow measurable properties of the system, probability distributions for outcomes of said properties, characterize reversible and irreversible processes through an entropy function, and so on. We are looking for necessary minimal basic requirements upon which these common tools can be built.

At least two arguments can be made to show that any physical theory must allow statistical descriptions, i.e.~provide an ensemble space. First, note that physical laws are never about specific instances but rather regularities. They are statements of the type ``whenever we prepare this, we can measure that.'' This means that, ultimately, they are about ensembles. Second, if a physical theory is to be testable experimentally, and repeatedly so, it must be in terms of ensembles because this is what we prepare in practice and this is what, in the end, we characterize experimentally. Moreover, repeatability requires the ability to always test ``one more time,'' which implies that ensembles are infinite collections, and therefore idealization. The approach, then, can most likely be given firm conceptual grounds, which we leave as an open but likely solvable philosophical problem.

We have identified three basic requirements on ensembles: first, they have to be identifiable experimentally; second, they need to allow statistical mixtures; lastly, they need a well-defined entropy. Let us go through these items one by one. We leave the full details to the working document (TODO: decide how to call it).

\subsection{Experimental verifiability}
The first requirement is that ensembles must be connected to experimental verification. That is, we must have enough \textbf{verifiable statements} at our disposal to define ensembles and tell them apart. By verifiable statement we mean a statement for which an experimental test is available that terminates in finite time if and only if the statement is true. For example, the statement ``the mass of the photon is less than $10^{-13} \, eV$'' is verifiable, while ``the mass of the photon is exactly $0 \, eV$'' is not verifiable due to its infinite precision. Based on previous work, (CITE) this mathematically requires an ensemble space to be a $T_0$ second-countable topological space, where open sets represent verifiable statements and Borel sets represent statements associated with a test, but without guarantee of termination.

The topological structure, then, represents the most foundational structure for a physical theory. It tells us why functions are ``well-behaved,'' which simply means they have to be topologically continuous in the ``natural topology'' induced by this requirement. It tells why probability is assigned to Borel sets (i.e. all statements associated with tests) and why the Banach-Tarski paradox does not apply in physics (i.e. non-Borel sets are physically ill-defined as they are not connected to experimental verification). Similarly, it tells us that sets with cardinality greater than that of the continuum are not physically relevant, as they cannot be given a $T_0$ second-countable topology.

In the case of ensembles, typical verifiable statements will be ``the average energy of the particle is between 3 and 4 $eV$,'' ``the probability of getting heads is between 49 and 51 percent,'' ``the probability distribution for the position is a Gaussian of mean 0 $m$ and 1 $m$ standard deviation within 1\%.'' Note that these are not results of single-shot measurements, and therefore verifiable statements are distinctly different from observables in quantum mechanics. In the same manner, these are not measurements that extract one bit of information, as there is no requirement of termination in the negative case.

\begin{tcolorbox}[colback=white, colframe=black]
	Experimental verifiability $\Rightarrow$ An ensemble space $\Ens$ is a $T_0$ second-countable topological space.
\end{tcolorbox}


\subsection{Statistical mixtures}
Another basic requirement is the ability to prepare a \textbf{mixture} of two ensembles. If $\ens[a], \ens[b] \in \Ens$ are two ensembles, and $p \in [0,1]$ a weight, then $\ens = p \ens[a] + (1-p) \ens[b] \in \Ens$ is the ensemble that describes a process that selects the first ensemble over the second $p$ percent of the times. Mathematically, the ensemble space is endowed with a \textbf{convex structure}. Moreover, since the mixing operation must be consistent with experimental verifiability, it will be topologically continuous, and the ensemble space will be a topological convex space.

Ultimately, the convex structure is responsible for all linear structures we have in physics. In face, the entropy and its bounds, defined later, forces the ensemble space to be the subset of a convex subset of a vector space. That is, the convex structure is ``invertible.''\footnote{If we fix $\ens$, $\ens[a]$ and $p$ then, if it exists, there is only one $\ens[b]$ such that $\ens = p \ens[a] + (1-p) \ens[b]$. In the context of convex spaces, this property is called ``cancellative.''} What is still not clear is whether the ensemble space embeds continuously in a topological vector space. While topological vector spaces are well established in the literature, topological convex spaces are not. 

Regardless of the topology, we are already equipped to understand what negative probability is. Suppose we have a real vector space of dimensions $n$, then $n$ linearly independent vectors are enough to express every point as a linear combination. We can also take an additional vector and express every point as an affine combination, that is a linear combination where the coefficients sum to one, though some can still be negative. Since every ensemble space, including the space of mixed states in quantum mechanics, is a subset of a real vector space, we can choose a linearly dependent set of states that span the whole space, and every mixed state can be expressed as an affine combination, as a pseudo-probability. However, the boundaries of the ensemble space will determine which affine combinations are valid states (i.e. they end up within the ensemble space), and there is no requirement that these boundary are conveniently expressible in terms of affine combinations.

The convex structure, however, allows us to ask whether two ensembles have a \textbf{common component}. That is, then can be seen as different statistical mixture that have some part in common. Two ensemble are separate, noted $\ens[a] \separate \ens[b]$, if they do not have a common component. This allows to characterize the space in terms of statistical decomposition, and see how this differs from other decompositions.

\begin{tcolorbox}[colback=white, colframe=black]
	Statistical mixture of ensembles $\Rightarrow$ An ensemble space $\Ens$ is a topological convex space.
\end{tcolorbox}

\subsection{Entropy}
The last requirement is that each ensemble must have a well defined \textbf{entropy}. That is, there is a scalar function $S : \Ens \to \mathbb{R}$ that satisfies the following:
\begin{enumerate}
	\item strictly concave: $S(p \ens[a] + (1-p) \ens[b]) - (p S(\ens[a]) + (1-p) S(\ens[b]) ) \geq 0$
	\item bounded from above: $S(p \ens[a] + (1-p) \ens[b]) - (p S(\ens[a]) + (1-p) S(\ens[b]) ) \leq - p \log p - (1-p) \log(1-p)$
\end{enumerate}
The first tells us that, during mixing, the entropy cannot decrease, and it stays the same if and only we are mixing an ensemble with itself. The second tells us that the most the entropy can increase is given by the choice between the two ensembles.\footnote{The expression for the Shannon entropy is actually derived instead of imposed axiomatically.} We define two ensembles to be \textbf{orthogonal} when they maximally increase the entropy during mixing. This recovers the standard notion of orthogonality: two classical ensembles are orthogonal when they have disjoint support, and therefore $\int_M \rho_1 \rho_2 d\mu = $; two quantum ensembles are orthogonal when they are defined on orthogonal subspaces, and therefore $\tr(\rho_1 \rho_2) = 0$.

The entropy is ultimately responsible for all geometrical structure in physics. In classical mechanics, the geometry is essentially defined by phase space volumes and areas in each degree of freedom (DOF). The entropy of uniform distributions is given by the logarithm of the volume, meaning that given the volume we are able to calculate the entropy and given the entropy we are able to reconstruct the volume. Similarly, uniform marginals will recover DOF areas. In quantum mechanics, the inner product (i.e. the Born rule) allows us to calculate the entropy of the mixture of two pure states, and given the entropy we can recover the Born rule.

An interesting insight is that the entropy, being strictly concave, has a negative defined Hessian. The negation of the Hessian, then, is a symmetric positive defined function of two infinitesimal variations, and can serve as a metric tensor on the affine structure given by the mixing coefficients. In both classical and quantum mechanics this recovers the Fisher-Rao metric.

Another crucial feature is that two ensembles maximize the entropy increase if and only if they are orthogonal: two classical ensembles maximize the entropy increase when they have disjoint support, and therefore $\int_M \rho_1 \rho_2 d\mu = 0$; two quantum ensembles when they are defined on orthogonal subspaces, and therefore $\tr(\rho_1 \rho_2) = 0$. This corresponds to the case when the two ensemble are made of instances that are mutually exclusive. Therefore we define two ensembles to be \textbf{orthogonal}, noted $\ens[a] \ortho \ens[b]$, if their mixture leads to a maximal entropy increase. The following additional axiom is valid for orthogonal ensembles
\begin{enumerate}
	\setcounter{enumi}{2}
	\item mixtures preserve orthogonality: $\ens[a] \ortho \ens[b]$ and $\ens[a] \ortho \ens[c]$ if and only if $\ens[a] \ortho p \ens[b] + \bar{p} \ens[c]$ for any $p \in (0,1)$
\end{enumerate}
as mixing elements distinguishable from those of ensemble $\ens[a]$, still gives elements distinguishable from ensemble $\ens[a]$.

It can be shows that orthogonal ensembles are separate, but the converse is not true. In quantum mechanics, for example, all pure states are separate (they cannot be decomposed and therefore cannot have components in common) but not all are orthogonal. In classical spaces, however, all separate ensembles are also orthogonal. Understanding the relationships between these two properties, then, is key in understanding the difference between classical and quantum mechanics, and what can happen in other physical theories.

\begin{tcolorbox}[colback=white, colframe=black]
	Existence of entropy $\Rightarrow$ Strictly concave function whose upper bound defines orthogonality. Statistical mixtures preserve orthogonality.
\end{tcolorbox}

\subsection{Interplay between structures}

Before introducing the actual measures, we want to note that there is a delicate balance between these structures which is important at both a conceptual level and at a more mathematical level. Part of the difficulty is to recognize which standard modes of understanding the problem should be kept, as the they generalize nicely, and which should be discarded. Let us go through some key examples.

Those that focus on the convex/linear structure may be tempted to close the space under infinite convex/linear combinations. It would be very convenient mathematically, and, in fact, this is what happens in Hilbert spaces. However, this closure leads yo physically ill-defined systems, such as a particle with an infinite average position or undefined energy.(CITE Hilbert) It is the connection to experimental verification, the topology, that will tell us which limits are allowed and which are not. Therefore, the lack of infinite convex is a crucial feature of the space, and convex sets in ensemble spaces are to be understood as closed convex sets, so that closure on the topology includes those, and only those, infinite mixtures that are physically meaningful.

If we focus on probability, we are tempted to extend the intuition from classical mechanics and it is useful to understand exactly why and where it fails. On a convex structure we can define the extreme points, those that cannot be further decomposed. Therefore it is tempting to define \textbf{pure states} as those ensembles that cannot be further decomposed into other ensembles. For example, the pure states for a six-sided die are the ensembles perfectly prepared respectively on the six possible outcomes. All ensembles are convex combinations $\sum_{i=1}^{6} p_i \ens_i = 1$ of those pure states $\ens_i$. This is exactly how classical discrete spaces work and much of our intuition for probability comes from there.

This approach, however, already becomes problematic in classical mechanics. Since we can take uniform distributions over any region of phase space, we can imagine to take a sequence of more and more refined distributions around a value of position and momentum. Therefore we may be tempted to say that the points of phase space represent perfectly prepared systems, and every other probability is a distribution on these space. This is physically unsound. First of all, we cannot consistently prepare position and momentum with infinite precision, therefore those ensembles do not exist even exist as an idealization. Secondly, the entropy of those ensembles, if they existed, would be minus infinity, which is a problem for any thermodynamic treatment. To make this intuition consistent with the math would require each point in phase space to be topologically isolated, which would extend the $\sigma$-algebra to the power set of $\mathbb{R}^n$, which means there would be no consistent way to define a volume. The fact that the math breaks down is simply a symptom that our physical problem is ill-specified.

In quantum mechanics, things are even worse. Pure states are indeed extreme points of the convex set. However, we also have discrete spectra. Here the temptation is, like we ``did'' in classical mechanics, to consider wave-functions all concentrated at a single value of position. While one may think this is the analogue of classical mechanics, it is not. We can, in fact, take a wave-function that is uniformly distributed on every finite range of position, and we can imagine to shrink that range. However, this is a sequence of pure states. All pure states have zero entropy, and the reduced spread on position will mean an increased spread on momentum. That is, we are not providing a sequence of ensembles at ever greater prevision, like in the classical case. In other words, the eigenstates of position in quantum mechanics, even if we wanted to say they were actual states, they would not be the limit of infinitesimal decomposition (i.e. a limit in the convex structure) but rather the limit of a path over pure states (i.e. a topological limit).

The key insight is that \textbf{mixing coefficients and probabilities are different in general} and they coincides only when a particular ensemble can be understood as a statistical mixtures of perfect preparations of a given quantity (i.e. of orthogonal ensembles). This always happens in the classical case and over measurement contexts in the quantum case, but is not the general case. For example, an equal mixture of spin up and spin left, for example, does not correspond to a 50\% probability of measuring spin up. The key to develop a general theory of probability, then, is to characterize the mixing coefficients first and understand when they recover probabilities of outcomes.

To recap, the topological structure captures experimental verifiability and is responsible for the limits and the connection to measureable quantities. The convex structure captures statistical mixing and is responsible for all linear structures of physical theories. The entropic structure captures the variability within an ensemble and is ultimately responsible for all geometric structures. Since the physical problem is indivisible, there is an interplay across these structure which goes beyond this article and we refer to our larger work.

\section{Fraction capacity}

Given that mixing coefficients have a more general applicability than probability of outcomes, let us concentrate on the following problem: given an ensemble $\ens \in \Ens$ and a (Borel) set of ensembles $A \subseteq \Ens$, what fraction of $\ens$ can be constructed by a mixture of $A$? How much of $\ens$ can be explained as coming from preparations corresponding to $A$? For example, let $\Ens$ be the space of all probability distributions for a six-sided die. Let $\ens_{123456}$ be the uniform distribution over all outcomes. Let $A_1 = \{\ens_1\}$ where $\ens_1$ represents outcome one with 100\% probability. Since we can write $\ens_{123456} = \frac{1}{6} \ens_{1} + \frac{5}{6} \ens_{23456}$, where $\ens_{23456}$ represents a uniform distributions over the five outcomes, $1/6$ of $\ens$ can be constructed from $A$, but no more. Similarly, if $A_{12} = \{\ens_{1},\ens_{2}\}$, we can write $\ens = \frac{1}{3} \left(\frac{1}{2} \ens_1 + \frac{1}{2} \ens_2 \right)  + \frac{2}{3} \ens_{3456}$, so $1/3$ of $\ens$ can be constructed from $A$, but no more. Note that, given the uniform distribution, $1/6$ is exactly the probably for the event $A_1$ and $1/3$ the probability for event $A_{12}$. This gives the basic insight for our definition.

Given a target ensemble $\ens \in \Ens$ and an arbitrary ensemble $\ens[a] \in \Ens$, we define the \textbf{fraction} of $\ens[a]$ in $\ens$ to be
\begin{equation}
	\fraction_{\ens}(\ens[a]) = \sup(\{ p \in [0,1] \, | \, \exists \, \ens_1 \in \Ens \text{ s.t. }  \ens = p \ens[a] + \bar{p} \ens_1 \}).
\end{equation}
The fraction is always well-defined because we can always write $\ens = 0 \ens[a] + 1 \ens$, therefore it must be zero or greater. This quantity tells us how much of ensemble $\ens$ can be constructed from $\ens[a]$. 

We now extend this idea from a single ensemble $\ens[a] \in \Ens$ to a Borel set of ensembles $A \subset \Ens$. Recalling that the $\hull(A)$ is the set of all possible convex combinations (i.e. mixtures), we define the\textbf{fraction capacity} of $A$ for $\ens$ to be
\begin{equation}
	\frcap_{\ens}(A) = \sup(\fraction_{\ens}(\hull(A))\cup\{0\}).
\end{equation}
This returns the biggest fraction of $\ens$ that can be achieved with a mixture of elements of $A$.\footnote{The name fraction capacity is chosen to both signify the ability of the set $A$ to contain $\ens$ (i.e. fraction capacity of one means $\ens$ is within the convex combinations of $A$) and to indicate that it will be a non-additive monotonic measure, which are called ``capacities'' in some literature. TODO \href{ https://link.springer.com/book/10.1007/978-3-319-03155-2}{cite} }

One can then show that the fraction capacity $\frcap_{\ens} : \Sigma_{\Ens} \to [0,1]$ is a set function that satisfies the following:
\begin{enumerate}
	\item non-negative and unit bounded - $\frcap_{\ens}(A) \in [0,1]$
	\item monotone - $A \subseteq B \implies \frcap_{\ens}(A) \leq \frcap_{\ens}(B)$
	\item sub-additive - $\frcap_{\ens}(A \cup B) \leq \frcap_{\ens}(A) + \frcap_{\ens}(B)$
	\item continuous from below and above - $\frcap_{\ens}(\lim\limits_{i \to \infty} A_i) = \lim\limits_{i \to \infty} \frcap_{\ens}(A_i)$ for any increasing or decreasing sequence $\{A_i\}$.
\end{enumerate}

The first property readily comes from the domain of the fraction. For the second, note that $\hull$ and $\sup$ are both monotone. For sub-additivity, note that the fraction of two components can at most sum during mixing. For the last, increasing and decreasing sequences strictly add or remove possible mixtures, therefore they will lead to increasing or decreasing sequences of real numbers bounded between zero and one. The limit of these sequences must agree with the fraction capacity of the limit.

If not for the lack of additivity over disjoint sets, the fraction capacity would be a probability measure. However, the difference between fraction capacity and probability goes beyond that. While the probability measure is defined over sets of outcomes, the fraction capacity is defined over ensembles, over probability distributions. Conceptually the two are different object, but they, as we will see later, linked. Suppose $\Ens$ is the classical ensemble space given by the set of probability distributions over a sample space $\Omega$. Let $U \subseteq \Omega$ be an event, let $A_U \subset \Ens$ be the set of all probability distributions with support within $U$ and $\ens \in \Ens$ a probability distribution. Then we can show that $\frcap_{\ens}() $

In the classical case, the fraction capacity reduces to a probability measure. Note that a probability satisfies all the properties of a fraction capacity, but it also satisfy on additional property: additivity over disjoint sets. That is, if $A \cup B = \emptyset$ then $\mu(A \cup B) = \mu(A) + \mu(B)$. Adding finite additivity over disjoint sets is enough as continuity will extended the property to additivity over countably many disjoint sets. Conversely, having countable additivity over disjoint sets implies sub-additivity and continuity from below and above. 



While additivity over disjoint set holds in classical probability, it does not hold in quantum mechanics

 This is not true in general, as we can see in a simple example from quantum mechanics. Let $\Ens$ be the ensemble space of a qubit, that is the space of density matrices of a two dimensional Hilbert space. Geometrically, this is the convex set represented by a three dimensional ball. Let $\ens_{\psi} = |\psi\>\<\psi|$ be a pure state and let $\ens_{\phi} = |\phi\>\<\phi|$ be its orthogonal. Geometrically, these are two opposite points on the surface of the ball. The maximally mixed state can be expressed as $\ens = \frac{1}{2} \ens_0 + \frac{1}{2} \ens_1$. This is the center point of the ball and is in fact the midpoint between two opposite points. Since this is valid for any point, we have $\frcap_{\ens}(\{\ens_{\psi}\})=\frac{1}{2}$ for any $\ens_{\psi} \in \Ens$ and $\frcap_{\ens}(\{\ens_{\psi}, \ens_{\phi}\})=1$ for any pair of orthogonal pure states. If $A$ is a set of two orthogonal pure states, and $B$ is state different from the two, we have $\frcap_{\ens}(A \cup B) = 1 < \frcap_{\ens}(A) + \frcap_{\ens}(B) = \frac{3}{2}$.

If we restrict ourselves to lattice of subsets of $\Ens$ we can recover additivity. For example, if we take a family $A_i$ of orthogonal sets (i.e. all the elements of one set are orthogonal to all the elements of any other set) and take an element that is a mixture of elements from that family, then the fraction capacity will be additive over the lattice formed by closing the family under union and intersection. In quantum mechanics, this would correspond to all probability distributions that are outcomes of the projective measurement that distinguishes between $A_i$. Part of the task, then, will be finding a set of necessary and sufficient conditions under which the fraction capacity for a particular ensemble is additive over a particular lattice of subsets

What is important here is that 


\section{Statistical properties}

\section{Beyond real valued quantities}


\section{State capacity}

\section{Quantization}
* 3 pick 2

\section{Quantizing space-time}
* we need a non-additive measure on degrees of freedom
*

\section{Conclusion}



\section*{Acknowledgments}
This paper is part of the ongoing \textit{Assumptions of Physics} project \cite{aop-book}, which aims to identify a handful of physical principles from which the basic laws can be rigorously derived. This article was made possible through the support of grant \#62847 from the John Templeton Foundation.


\bibliography{bibliography}

\newcommand{\pj}[1] {\underbar{$#1$}}


\end{document}
