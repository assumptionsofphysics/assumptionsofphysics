\documentclass[10pt,twocolumn, nofootinbib]{revtex4-2}

\usepackage{assumptionsofphysics}
\usepackage{tikz}
\usepackage{breakurl}
\usepackage{tcolorbox}

\newcommand\hull{\mathrm{hull}}
\newcommand\stcap{\mathrm{scap}}
\newcommand\fraction{\mathrm{frac}}
\newcommand\frcap{\mathrm{fcap}}

\newcommand{\ens}[1][e] {\mathsf{#1}} % Ensemble
\newcommand{\Ens}[1][E] {\mathcal{#1}} % Ensemble space


\def\>{\rangle}
\def\<{\langle}
\DeclareMathOperator{\erf}{erf}

\begin{document}

\title{A non-additive generalization of probability theory \\for quantum mechanics and beyond}
\author{Gabriele Carcassi}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}
\author{Christine A. Aidala}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}

\date{\today}


\begin{abstract}
	We present a physically motivated generalization of probability theory that is suitable for classical mechanics, quantum mechanics and any future physical theory that allows a statistical description. The goal is to put the use of classical and quantum probability in a broader context, and show how the current mathematical structures are likely not suitable to solve the open problems in the foundations of physics. For the more mathematically inclined, we will point to areas where new math or generalization of established math are needed. For the more philosophically incline, we will point to areas where further conceptual work is needed.
	
	
	%Given a generic space of ensembles, we can define the fraction capacity as the maximum fraction of a particular ensemble that can be understood as a mixture of ensembles from a given set. This gives a non-additive (i.e. fuzzy) measure that reduces to a probability (i.e. additive) measure for classical spaces and for quantum measurements. We can also define the state capacity as the exponential of the maximum entropy reachable by a mixture of ensembles from a given set. This is also a non-additive measure and reduces to the Liouville measure in classical mechanics and the Hilbert space dimensions for subspaces of quantum mechanics. Conceptually, it gives us a notion of probability that is both theory and interpretation independent. Mathematically, it gives us a measure theoretic generalization of probability. Physically, it may allow to find new way to understand current theory and tool to investigate new ones. The purpose of this paper is to show the core ideas and present questions that may be developed on the mathematical, physical and philosophical side.
\end{abstract}

\maketitle


\section{Introduction}

Our current understanding of probability is given in terms of the measure theoretic formalization due to Kolmogorov, and its various philosophical interpretations. However, probability in quantum mechanics, as it is widely known, is not Kolmogorovian, in the sense that it provides results that are not reproducible in standard probability spaces. It follows that our mathematical axioms for probability and their interpretations are insufficient for physical theories.

An alternative measure theoretic strategy, that has been successful is quantum mechanics, is the use of quasi-probability distributions: measures that are still unitary over the whole space, but can have negative values in small regions. However, these haven't resulted in an alternative axiomatic approach with suitable interpretations for at least, in our mind, three reasons. First, it is not clear how negative probability should be interpreted. Second, it is not clear how to characterize the space of valid quasi-probability distributions (e.g. there is no simple rule to specify which $\rho(q,p)$ are valid Wigner functions). Third, there are multiple options for quasi-probability distributions (e.g. Wigner function, Husimi distribution, Sudarshanâ€“Glauber distribution). This lack of foundational clarity presents an additional problem: how do we know whether future physical theories will not require yet another extension?

The goal of this paper is to show that, in the same way that classical probability can be axiomatized through standard measure theory, quantum probability, and in fact any probability used a physical theory, can be axiomatized through non-additive (i.e. fuzzy) measure theory. We will define a generalized concept of probability that is ultimately based on physically motivated axioms, giving the physicist a clear operational motivation to our definitions, the mathematicians a formally well-defined mathematical structure and the philosopher a unified framework in which probability can be interpreted in the context of physical theories. We are not going to be able to develop a full theory here, as there are still a number of conceptual and technical challenges to be solved that will require work in physics, mathematics and philosophy. However, we will show that there are enough results that point in a good direction, and therefore make it reasonable to invest time and effort to work on the open problems. For those interested, the current state of the research will always be available in an open document that will be routinely updated as the theory matures.

In a nutshell, we will start by arguing that physical theory must at least be able to talk about statistical ensembles, and that ensembles must allow the preparation of statistical mixture. Given a target ensemble $\ens$ and a set of ensemble $A$, we define the fraction capacity of $A$ for $\ens$ as the biggest fraction of $\ens$ that can be constructed from another set of ensembles $A$. That is, the highest $p$ such that we can express our ensemble as $\ens = p \ens[a] + (1-p) \ens[b]$, where $\ens[a]$ is a mixture of elements of $A$. For example, if $\ens$ is the uniform distributions for a roll of a six-faced die, and $A$ includes only two ensembles with $100\%$ probability for the outcomes $3$ and $4$ respectively, then the faction capacity of $A$ for $\ens$ will be $\frac{1}{3}$. Note that this coincides with the probability of the event ``$3$ or $4$''. The fraction capacity is a non-negative, unit bounded, sub-additive and continuous set function, which is therefore like a probability measure except for the additivity, which is recovered for classical mechanics and over quantum measurements.

\section{Quick review of probability theory}


\section{Ensemble spaces}

As mentioned in the introduction, our goal is to construct a probability theory of general applicability to physical systems. Our approach is not to define an axiomatic theory of probability and apply it to physical systems, but rather to characterize basic properties of physical states and from those build a theory of probability that fits those definitions. As is common in our larger project, we start from axioms that, in our opinion, represent requirements of scientific practice. The advantage is that the realm of applicability and physical interpretation will be clear from the start. 

We start from requiring a physical theory to allow statistical descriptions: at the very least, it will provide us with a notion of ensemble, and the space of ensembles that are allowed by the theory. An ensemble can be understood as the infinite collection of the outputs of a particular preparation procedure. The ensemble space of classical mechanics, for example, is the space of continuous probability distributions on phase space. In quantum mechanics, instead, it is the space of density operators. While the ensemble space for each theory will in principle be different, they will all share common features: they must allow measurable properties of the system, probability distributions for outcomes of said properties, characterize reversible and irreversible processes through an entropy function, and so on. We are looking for necessary minimal basic requirements upon which these common tools can be built.

At least two arguments can be made to show that any physical theory must allow statistical descriptions, i.e.~provide an ensemble space. First, note that physical laws are never about specific instances but rather regularities. They are statements of the type ``whenever we prepare this, we can measure that.'' This means that, ultimately, they are about ensembles. Second, if a physical theory is to be testable experimentally, and repeatedly so, it must be in terms of ensembles because this is what we prepare in practice and this is what, in the end, we characterize experimentally. The approach, then, can most likely be given firm conceptual grounds, which we leave as an open but solvable philosophical problem.

We have identified three basic requirements on ensembles: first, they have to be identifiable experimentally; second, they need to allow statistical mixtures; lastly, they need a well-defined entropy. Let us go through these items one by one. We leave the full details to the working document (TODO: decide how to call it).

\subsection{Experimental verifiability}
The first requirement is that ensembles must be connected to experimental verification. That is, we must have enough \textbf{verifiable statements} at our disposal to define ensembles and tell them apart. By verifiable statement we mean a statement for which an experimental test is available that terminates in finite time if and only if the statement is true. For example, the statement ``the mass of the photon is less than $10^{-13} \, eV$'' is verifiable, while ``the mass of the photon is exactly $0 \, eV$'' is not verifiable due to its infinite precision. Based on previous work, this mathematically requires an ensemble space to be a $T_0$ second-countable topological space, where open sets represent verifiable statements and Borel sets represent statements associated with a test, but without guarantee of termination.

The topological structure, then, represents the most foundational structure for a physical theory. It tells us why functions are ``well-behaved,'' which simply means they have to be topologically continuous in the ``natural topology'' induced by this requirement. It tells why probability is assigned to Borel sets (i.e. all statements associated with tests) and why the Banach-Tarski paradox does not apply in physics (i.e. non-Borel sets are physically ill-defined as they are not connected to experimental verification). Similarly, it tells us that sets with cardinality greater than that of the continuum are not physically relevant, as they cannot be given a $T_0$ second-countable topology.

In the case of ensembles, typical verifiable statements will be ``the average energy of the particle is between 3 and 4 $eV$,'' ``the probability of getting heads is between 49 and 51 percent,'' ``the probability distribution for the position is a Gaussian of mean 0 $m$ and 1 $m$ standard deviation within 1\%.'' Note that these are not results of single-shot measurements, and therefore verifiable statements are distinctly different from observables in quantum mechanics. In the same manner, these are not measurements that extract one bit of information, as there is no requirement of termination in the negative case.

\begin{tcolorbox}[colback=white, colframe=black]
	Experimental verifiability $\Rightarrow$ An ensemble space $\Ens$ is a $T_0$ second-countable topological space.
\end{tcolorbox}


\subsection{Statistical mixtures}
Another basic requirement is the ability to prepare a \textbf{mixture} of two ensembles. If $\ens[a], \ens[b] \in \Ens$ are two ensembles, and $p \in [0,1]$ a weight, then $\ens = p \ens[a] + (1-p) \ens[b] \in \Ens$ is the ensemble that describes a process that selects the first ensemble over the second $p$ percent of the times. Mathematically, the ensemble space is endowed with a \textbf{convex structure}. Moreover, since the mixing operation must be consistent with experimental verifiability, it will be topologically continuous, and the ensemble space will be a topological convex space.

Ultimately, the convex structure is responsible for all linear structures we have in physics. Both in classical and quantum mechanics, the ensemble space is not just a convex set, but a convex subset of a vector space. In those cases, in fact, the convex structure is, in a sense, invertible. If we fix $\ens$, $\ens[a]$ and $p$ then, if it exists, there is only one $\ens[b]$ such that $\ens = p \ens[a] + (1-p) \ens[b]$. The ensemble space is \textbf{complemented}, meaning that there is only one complement to a component of an ensemble.\footnote{In the context of convex spaces, this property is called ``cancellative.''}

This perspective allows us to understand what negative probability is. Suppose we have a real vector space of dimensions $n$, then $n$ linearly independent vectors are enough to express every point as a linear combination. If we add another point, we can now express every point as an affine combination, that is a linear combination where the coefficients sum to one, though some can still be negative. Since the space of mixed states in quantum mechanics is a real vector space, we can choose a linearly dependent set of states that span the whole space, and every mixed state can be expressed as an affine combination, as a pseudo-probability.

Pseudo-probability distributions, then, require the additional vector space structure. However, we found no physical justification to require the additional structure, and we suspect that it may need to be relaxed in future physical theories.  Therefore it is important that our fundamental definitions require only the convex structure, and not the vector space structure.

\begin{tcolorbox}[colback=white, colframe=black]
	Statistical mixture of ensembles $\Rightarrow$ An ensemble space $\Ens$ is a topological convex space.
\end{tcolorbox}

\subsection{Entropy}
The last requirement is that each ensemble must have a well defined \textbf{entropy}. That is, there is a scalar function $S : \Ens \to \mathbb{R}$ that satisfies the following:
\begin{enumerate}
	\item strictly concave: $S(p \ens[a] + (1-p) \ens[b]) - (p S(\ens[a]) + (1-p) S(\ens[b]) ) \geq 0$
	\item bounded from above: $S(p \ens[a] + (1-p) \ens[b]) - (p S(\ens[a]) + (1-p) S(\ens[b]) ) \leq - p \log p - (1-p) \log(1-p)$
\end{enumerate}
The first tells us that, during mixing, the entropy cannot decrease, and it stays the same if and only we are mixing an ensemble with itself. The second tells us that the most the entropy can increase is given by the choice between the two ensembles.\footnote{It can be shown that if an upper bound that only depends on the mixing coefficients exists, then it must be the Shannon entropy.}

The entropy is ultimately responsible for all geometrical structure in physics. In classical mechanics, the geometry is essentially defined by phase space volumes and areas in each degree of freedom (DOF). The entropy of uniform distributions is given by the logarithm of the volume, meaning that given the volume we are able to calculate the entropy and given the entropy we are able to reconstruct the volume. Similarly, uniform marginals will recover DOF areas. In quantum mechanics, the inner product (i.e. the Born rule) allows us to calculate the entropy of the mixture of two pure states, and given the entropy we can recover the Born rule.

An interesting insight is that the entropy, being strictly concave, has a negative defined Hessian. The negation of the Hessian, then, is a symmetric positive defined function of two infinitesimal variations, and can serve as a metric tensor on the affine structure given by the mixing coefficients. In both classical and quantum mechanics this recovers the Fisher-Rao metric.

Another important feature is that if two ensembles saturate the upper bound, then they are orthogonal in both classical and quantum mechanics. Therefore we define two ensembles to be \textbf{orthogonal} if their mixture leads to a maximal entropy increase. An additional axiom is valid for orthogonal ensembles. We say an ensemble is a component of another ensemble if the second can be expressed as a mixture of the first with some other ensemble. On physical grounds, two orthogonal ensembles $\ens_1, \ens_2 \in \Ens$ are perfectly distinguishable and must have no common component. That is, there is no $\ens_3 \in \Ens$ that is a component of both $\ens_1$ and $\ens_2$. Note that the converse is not generally true: two pure states in quantum mechanics have no components in common, but are not necessarily orthogonal. We find that an ensemble space where no common component implies orthogonality can be described by a space of classical probability measures. Non-classicality, then, is exactly the failure of this property.

\begin{tcolorbox}[colback=white, colframe=black]
	Existence of entropy $\Rightarrow$ Strictly concave function whose upper bound defines orthogonality.
\end{tcolorbox}

\subsection{Interplay between structures}

Each structure is important, but the balance between them is even more important. The key is to understand, on physical grounds, where each problem lies and therefore use the appropriate tool. For example, one may focus on the convex/linear structure and may be tempted to close the space under infinite convex/linear combinations. Mathematically, it would be very convenient. However, some infinite mixtures lead to physically ill-defined systems, such as a particle with an infinite average position. It is the connection to experimental verification, the topology, that will tell us which limits are allowed and which are not. Conceptual clarity on these details is essential to find the correct generalization. A full examination of these issues is beyond the scope of this article, and we refer to our larger, and future, work. However, there is one key issue that we need to discuss as, we found, it was the main conceptual obstacle to progress.

On the convex structure we can define \textbf{pure states} as the ones that cannot be decomposed into other ensembles. For example, the pure states for a six-sided die are the ensembles perfectly prepared respectively on the six possible outcomes. All ensembles are convex combinations $\sum_{i=1}^{6} p_i \ens_i = 1$ of those pure states $\ens_i$. Mathematically, the pure states are the extreme points of the space and all other points can be understood as convex combinations over them. This is exactly how classical discrete spaces work and there is a temptation to think this is how everything works. For example, in classical mechanics the pure states would be perfect preparations of position and momentum, and ensembles are probability distributions. Not only is this problematic mathematically, it is conceptually not sustainable.

On physical grounds, we cannot consistently prepare classical particles perfectly, therefore there is no ensemble corresponding to a perfect preparation of a point of phase space. The mathematical facts that delta Dirac are not proper functions, their support has an empty interior, have minus infinite entropy, and so on, are simply a consequence of our conceptual problem. One may concede that the ensembles do not exist physically, but may still want them to consider them as the limit of infinitesimal decomposition: classical ensembles can be decomposed into more and more precise ones, and the points are the limit of this decomposition. Probability distributions are defined on these limits. This kind of works in classical mechanics, but in general.

In quantum mechanics we do have pure states, and these are ensembles that cannot be further decomposed into finer ensembles. Therefore an eigenstate of position is not a limit of decomposition, a limit in the convex structure. It is the limit where we are preparing a pure state on a smaller and smaller region of space. It is the limit of a sequence of pure states, a limit in the topology. Physically, more than mathematically, it is a completely different thing. It is only when we realized this difference that a clean route to generalization presented itself.\footnote{The fact that pure states can be seen, in some sense, as superposition of eigenstates of position, makes the matter even more confusing.}

The insight is that probability distributions for a quantity are not defined by statistical mixtures of perfect preparations (i.e. a convex combination of extreme points), but by assigning a probability over each experimentally verifiable (i.e. open) interval of the possible values of said quantity (its spectra). The two coincide in classical discrete spaces, which are the least physically interesting case as they do not even allow classical particle mechanics. Equivalently, \textbf{mixtures are about preparations, not measurements, and probability distributions are about measurements, not preparations}. Since they can only coincide in classical spaces, a generalized notion of probability for physical theories must resolve and characterize the distinction.

To recap, the topological structure captures experimental verifiability and is responsible for the limits and the connection to measureable quantities. The convex structure captures statistical mixing and is responsible for all linear structures of physical theories. The entropic structure captures the variability within an ensemble and is ultimately responsible for all geometric structure. We now concentrate on recovering the measure theoretic structures associated to probability and state counting.

\section{Fraction capacity}

Note that the mixing coefficient $p$ is connected to probability but it is not itself probability in the standard sense. It represents a parameter for the mixing states, mixing preparations, while probability is typically understood as the chance of something happening, the likelihood of a final event. Mathematically, the probability measure is a function from an event, which can be understood as a collection of fully distinct outcomes, to a number from zero to one. If we are mixing ensembles that represent fully distinct outcomes (i.e. they are orthogonal), then the mixing $p$ is a probability in the standard sense. If we are mixing ensembles that do not represent fully distinct outcomes, that are not orthogonal like spin up and spin left, the probability of getting one of the outcomes is not the mixing coefficient $p$.

We essentially have two ways in which two events can be compared. We have \textbf{disjoint events}, those that represent disjoint sets of different states, like spin up and spin down; we also have \textbf{orthogonal events}, those that represent represent orthogonal sets of states, like spin up and spin down. The standard notion of probability applies only when considering orthogonal events. Classical probability rests on the assumption that all disjoint events are orthogonal. That is, that all states can be perfectly distinguished. Quantum mechanics breaks this assumption, and therefore we need a probability theory that can deal with disjoint but non-orthogonal events.

Let us concentrate on the following question: given an ensemble $\ens \in \Ens$ and a (Borel) set of ensembles $A \subseteq \Ens$, what fraction of $\ens$ can be constructed by a mixture of $A$? How much of $\ens$ can be explained as coming from preparations corresponding to $A$? For example, let $\Ens$ be the space of all probability distributions for a six-sided die. Let $\ens_{123456}$ be the uniform distribution over all outcomes. Let $A_1 = \{\ens_1\}$ where $\ens_1$ represents outcome one with 100\% probability. Since we can write $\ens_{123456} = \frac{1}{6} \ens_{1} + \frac{5}{6} \ens_{23456}$, where $\ens_{23456}$ represents a uniform distributions over the five outcomes, $1/6$ of $\ens$ can be constructed from $A$, but no more. Similarly, if $A_{12} = \{\ens_{1},\ens_{2}\}$, we can write $\ens = \frac{1}{3} \left(\frac{1}{2} \ens_1 + \frac{1}{2} \ens_2 \right)  + \frac{2}{3} \ens_{3456}$, so $1/3$ of $\ens$ can be constructed from $A$, but no more. Note that, given the uniform distribution, $1/6$ is exactly the probably for the event $A_1$ and $1/3$ the probability for event $A_{12}$. This gives the basic insight for our definition.

Given a target ensemble $\ens \in \Ens$ and an arbitrary ensemble $\ens[a] \in \Ens$, we define the \textbf{fraction} of $\ens[a]$ in $\ens$ to be
\begin{equation}
	\fraction_{\ens}(\ens[a]) = \sup(\{ p \in [0,1] \, | \, \exists \, \ens_1 \in \Ens \text{ s.t. }  \ens = p \ens[a] + \bar{p} \ens_1 \}).
\end{equation}
The fraction is always well-defined because we can always write $\ens = 0 \ens[a] + 1 \ens$, therefore it must be zero or greater. This quantity tells us how much of ensemble $\ens$ can be constructed from $\ens[a]$. 

We now extend this idea from a single ensemble $\ens[a] \in \Ens$ to a Borel set of ensembles $A \subset \Ens$. Recalling that the $\hull(A)$ is the set of all possible convex combinations (i.e. mixtures), we define the\textbf{fraction capacity} of $A$ for $\ens$ to be
\begin{equation}
	\frcap_{\ens}(A) = \sup(\fraction_{\ens}(\hull(A))\cup\{0\}).
\end{equation}
This returns the biggest fraction of $\ens$ that can be achieved with a mixture of elements of $A$.\footnote{The name fraction capacity is chosen to both signify the ability of the set $A$ to contain $\ens$ (i.e. fraction capacity of one means $\ens$ is within the convex combinations of $A$) and to indicate that it will be a non-additive monotonic measure, which are called ``capacities'' in some literature. TODO \href{ https://link.springer.com/book/10.1007/978-3-319-03155-2}{cite} }

One can then that the fraction capacity $\frcap_{\ens} : \Sigma_{\Ens} \to [0,1]$ is a set function that satisfies the following:
\begin{enumerate}
	\item non-negative and unit bounded - $0 \leq \frcap_{\ens}(A) \leq 1$
	\item monotone - $A \subseteq B \implies \frcap_{\ens}(A) \leq \frcap_{\ens}(B)$
	\item sub-additive - $\frcap_{\ens}(A \cup B) \leq \frcap_{\ens}(A) + \frcap_{\ens}(B)$
	\item continuous from below and above - $\frcap_{\ens}(\lim\limits_{i \to \infty} A_i) = \lim\limits_{i \to \infty} \frcap_{\ens}(A_i)$ for any increasing or decreasing sequence $\{A_i\}$.
\end{enumerate}

The first property readily comes from the domain of the fraction. For the second, note that $\hull$ and $\sup$ are both monotone. For sub-additivity, note that the fraction of two components can at most sum during mixing. For the last, increasing and decreasing sequences strictly add or remove possible mixtures, therefore they will lead to increasing or decreasing sequences of real numbers bounded between zero and one. The limit of these sequences must agree with the fraction capacity of the limit.

Note that a probability satisfies all the properties of a fraction capacity, but it also satisfy on additional property: additivity over disjoint sets. That is, $\mu(A \cup B) = \mu(A) + \mu(B)$ if $A \cup B = \emptyset$. This is not true in general, as we can see in a simple example from quantum mechanics. Let $\Ens$ be the ensemble space of a qubit, that is the space of density matrices of a two dimensional Hilbert space. Geometrically, this is the convex set represented by a three dimensional ball. Let $\ens_{\psi} = |\psi\>\<\psi|$ be a pure state and let $\ens_{\phi} = |\phi\>\<\phi|$ be its orthogonal. Geometrically, these are two opposite points on the surface of the ball. The maximally mixed state can be expressed as $\ens = \frac{1}{2} \ens_0 + \frac{1}{2} \ens_1$. This is the center point of the ball and is in fact the midpoint between two opposite points. Since this is valid for any point, we have $\frcap_{\ens}(\{\ens_{\psi}\})=\frac{1}{2}$ for any $\ens_{\psi} \in \Ens$ and $\frcap_{\ens}(\{\ens_{\psi}, \ens_{\phi}\})=1$ for any pair of orthogonal pure states. If $A$ is a set of two orthogonal pure states, and $B$ is state different from the two, we have $\frcap_{\ens}(A \cup B) = 1 < \frcap_{\ens}(A) + \frcap_{\ens}(B) = \frac{3}{2}$.

If we restrict ourselves to lattice of subsets of $\Ens$ we can recover additivity. For example, if we take a family $A_i$ of orthogonal sets (i.e. all the elements of one set are orthogonal to all the elements of any other set) and take an element that is a mixture of elements from that family, then the fraction capacity will be additive over the lattice formed by closing the family under union and intersection. In quantum mechanics, this would correspond to all probability distributions that are outcomes of the projective measurement that distinguishes between $A_i$. Part of the task, then, will be finding a set of necessary and sufficient conditions under which the fraction capacity for a particular ensemble is additive over a particular lattice of subsets

What is important here is that 


\section{Statistical properties}

\section{Beyond real valued quantities}


\section{State capacity}

\section{Quantization}
* 3 pick 2

\section{Quantizing space-time}
* we need a non-additive measure on degrees of freedom
*

\section{Conclusion}



\section*{Acknowledgments}
This paper is part of the ongoing \textit{Assumptions of Physics} project \cite{aop-book}, which aims to identify a handful of physical principles from which the basic laws can be rigorously derived. This article was made possible through the support of grant \#62847 from the John Templeton Foundation.


\bibliography{bibliography}

\newcommand{\pj}[1] {\underbar{$#1$}}


\end{document}
