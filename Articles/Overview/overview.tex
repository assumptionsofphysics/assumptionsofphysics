\documentclass[twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}

\title{Overview and Status of the Assumptions of Physics}
\author{Gabriele Carcassi, Christine A. Aidala}
\date{November 2019}


\begin{document}

\maketitle

\begin{abstract}
	This article gives an overview of the project ``Assumptions of Physics'' which aims to rederive the known laws from a few physically meaningful starting points. It presents the motivations behind the project, a summary of the main findings and current status of the research.	
\end{abstract}

\section{Introduction}

The aim of this article is to give a broad overview of the project ``Assumptions of Physics'' so that the reader will understand its motivations, its status and a list of the main findings. This is intended to be a good start for anyone interested in simply learning more about the project or finding an area for collaboration.

As we plan to update this document, please make sure you have a recent version. Each subsection will be fairly self-contained to allow the reader to skim through the document and focus on the parts of interest. It will contain a summary section at the beginning and a titled paragraph for each of the main idea or finding.

\subsection{Index}
To this paper, not to the work.

- Goals and method
Goals: Understanding, have a better sense of the math, give a background for new theories...
Methods: reverse engineering vs forward engineering, holistic

- General theory
relationship between math and physics
logic of verifiable statements
properties and quantities
accuracy
differentiability

- Fundamental physics
states and processes
determinism/reversibility: isomorphism in the category
entropy

- Physical theories
Classical mechanics
Quantum mechanics
Thermodynamics
Statistical mechanics

\section{Goals and method}


A formal framework that forces us to clarify our assumptions
From those assumptions the mathematical objects are derived
Each mathematical object has a clear physical meaning and no object is unphysical
Gives us concepts and tools that span across different disciplines
Allows to explore what happens when the assumptions fail, possibly leading to new physics ideas


\textbf{A better understanding of physics.} The overall goal of the project is to better understand physics: to understand why the laws of quantum mechanics, or classical mechanics, are what they are; to understand the limit of the validity of the different theories; to understand what the Poisson brackets, or any other mathematical operation, represent physically; to understand which mathematical objects correspond to actual physical entities and which are just artifacts; to understand what new theories are possible.

To us, understanding means to see the necessity of something and how it relates to the rest. For example, once we know that the earth's revolution axis is not aligned with earth's rotation axis, we understand why there must be seasons; and we understand why days are shorter in the winter; and why the sun is brighter and warmer in the summer. The more connections, the fewer starting points, the more understanding.

\subsection{Objectives}

The overall goal of a better understanding of physics can be broken down into the following five objectives.

\textbf{Clarify the assumptions.} Physics is currently a patchwork of different theories (e.g. classical mechanics, quantum mechanics, thermodynamics, ...) that are used for different systems or for the same system in different contexts. When should we use one instead of another? What are the limits of validity of each theory? We want to understand what are the assumptions one has to make on a system such that it can be studied with a particular theory. The only way to make sure our assumptions are necessary and sufficient is to show the theory can be rederived from them. This, in turn, gives us a better idea of what each theory describes, how it fails, and what new directions one could take.

\textbf{Put physics back at the center of the discussion.} Physics used to try and identifies basic ``laws'' or ``principles'' for itself. Current physical theories, instead, just postulate a mathematical structure relegating physics to a mere after-the-fact interpretation. We want to go back to the old approach: we want to start with the physics and derive the math suitable to capture those physical concepts. Ideally, no mathematically construct should be introduced if it is not physically motivated. Why is phase-space formed with conjugate pairs? What does the commutator represent? If these are really representing physical objects, we should have a better understanding than a vague ``you can think of it as...'' or a cryptic mathematical ``it is the left action of a fiber bundle on a...''. Most of our understanding should come from physical intuition and not from the mathematics it is used to represent it.

\textbf{Give science sturdier mathematical grounds.} Physicists are generally trained to think that mathematical details are physically not interesting and are the domain of mathematicians. Even mathematical structures that originally came from physics are now formalized by mathematicians to solve their own needs, and not the physicist's. This, in turns, reinforces the idea that those details are not interesting to the physicists. The reality is that a well-posed physical problem must also be a well-posed mathematical problem, but not the other way around, so leaving the details to the mathematicians means having fundamental structures that are less physical. For example, quantum states are modeled with Hilbert spaces even though we know they contain mathematical objects that are unphysical. If we give a theory, as we said before, by simply stating a mathematical structure, then there is no guarantee that what we give is a fully physical theory. Unphysical elements simply means the physical problem was not specified correctly. If we truly are able to give a precise physical meaning to every mathematical object, then our formal structures will map one-to-one to our physical understanding. This means no unphysical mathematical artifacts and a more precise mathematical treatment of physics.

\textbf{Foster connections between different fields of knowledge.} Knowledge is increasingly specialized and fewer and fewer people are well versed in more than a couple of subjects. Yet, connections between different fields of science, mathematics and engineering are routinely found to be useful for one simple reason: nature is one and does not care about such divisions. A more holistic vision of scientific knowledge, then, is a natural byproduct of our effort. The notions of state, environment, process and equilibrium, for example, are intertwined and are fundamental to most fields of science and engineering. Proper characterization of such basic concepts will give a common language and mathematical tools that span across different disciplines.

\textbf{Provide a solid basis for new theories.} Most of the attention in fundamental physics is focused on the development of new theory or the search for new effects. A better understanding of the current theories, recast in a single broader and more precise framework, facilitates that search. It gives more insight and intuition leading to new ideas. It makes explicit what ingredients went into the theories, so we know what assumption can fail and what principles cannot be changed. For example, we may abandon the assumption that space is measurable at an indefinite precision, leaving the realm of manifold and real numbers; yet we cannot abandon the notion that space is measurable, staying in the realm of second countable $T_0$ topological spaces. As all the physical theories are part of a more general structure, they also provide templates for new theories and prove that the more general structure is sound.

The objectives are cover many areas and are very broad. As such, we are always looking for people with diverse background and interest. We ultimately believe that all these objectives are connected and cannot be pursued independently.

\subsection{Methodology}

As each topic is investigated by the project, it generally goes through three phases. We mention them here as they will give context to the status of the project and to people who may want to contribute.

\textbf{Reverse engineering.} The goal of this phase is to deconstruct an already existing theory or mathematical structure to understand what each piece is supposed to represent physically. One way to conduct such investigation is to ask what would happen if a particular characteristic of the framework would fail. For example, a line of reasoning could be as follows: Hamilton's equations are differential; if they weren't the mapping of densities and areas would not be well defined; maybe Hamiltonian mechanics is not about mapping points but about mapping densities or areas. This usually leads to a set of necessary assumptions about the physical system one is describing, the ones one must take for the particular description to apply.

\textbf{Forward engineering.} The goal of this phase is to construct an argument from the necessary assumptions outlined in the previous phase to rederive the framework at hand. This checks whether the necessary assumptions are also sufficient. For example, if we started simply by assuming we have density distributions mapped in time, this would give us all possible differentiable evolutions, not necessarily Hamiltonian ones. This leads to a set of necessary and sufficient conditions to rederive the theory.

\textbf{Formalization.} The goal of this phase is to take the previous set of arguments and formalize them in pure mathematical language. This forces us to clarify all the starting points and what parts of the arguments are truly formal and what are physical. This can be done only starting from the foundations and therefore takes a considerable longer time. For example, before formalizing Hamiltonian mechanics we have to formalize differentiable manifolds, real number and topological spaces.

\subsection{Organization}

The overall project is organized into the following macro-categories.

\textbf{The general theory.} This formalizes the basic mathematical framework that is the basis for all science. Like in mathematics every algebraic structure is a specialization of the generic structures axiomatized by set theory, all scientific theory are ultimately specialization of the generic structure provided by the general theory. This defines the basic requirements imposed on a theory by logical consistency and experimental verifiability. It defines properties and quantities, accuracy and other concepts of general applicability.

\textbf{Fundamental physics concepts.} This defines the notions that are fundamental to physics specifically, such as states, processes, environment, equilibrium, determinism and so on. The idea is to define these concepts in the most abstract way possible such that common requirements can be identified. Every physical theory is a further refinement of this structure in that it will choose to study a particular set of states over a particular process.

\textbf{Physical theories.} This studies the different assumptions that are needed to recover the known physical theories. The assumptions typically describe what level of description is available for the system and its parts, how does it change in time and whether trajectories are enough to recover it.

We will present the work in the reverse order, from physical theories to general theory, as we find most physicist 

\section{The general mathematical theory of experimental science}

This part of the project focuses on the development of a rigorous mathematical framework that can provide the building blocks that are common to all scientific theories. As logic and set theory provides the foundation for all other mathematical structure, which are essentially sets with operations defined on them, the general theory needs to provide the same fundamental layer.

\subsection{The Principle of Scientific Objectivity}

Our guiding is that ``Science is universal, non-contradictory and evidence based.'' Any scientific theory must be logically consistent, its content be equally true or false for everybody and it must deal with what can be established experimentally. Different branches of science will specialize on different systems and topics, but they all the theories and models they develop must satisfy those basic constraints. The general theory deals with the fundamental mathematical structure that realize that principle. 

\emph{Not every subject can be studied by science.} The principle indirectly states that anything that is subjective, contradictory or not evidence based cannot be the subject of scientific investigation. The properties of prime numbers, moral and existential questions or the aesthetics of music fail in at least on of those respects and are excluded. Therefore science cannot be a substitute of mathematics, arts, philosophy or religion. Only the parts of nature that are accessible through consistent experimental verification can be studied scientifically. New experimental techniques extend its reach.

\subsection{The Logic of Verifiable Statements}

The basic building block in our framework is the idea of a verifiable statement: an assertion that is either true or false for everybody and for which we have an experimental test that will terminate successfully in finite time if and only if the statement is true. The basic structures of our general theory need to provide a logic framework to keep track of verifiable statements and their relationships.

\subsubsection{Logical contexts}

A logical context consists of a set of statements with well defined logical relationships. It is the most fundamental structure and it is the only one that is axiomatically introduced.  A scientific theory will consist of a set of statements taken within a logical context with particular logical relationships. For example, Newtonian mechanics will use statements like ``the mass of the object is 1 $Kg$'' and ``the acceleration of the object is 1 $m/s^2$''. If both of these are true, then the statement ``the force on the object is  1 $N$'' is also true. The context defines basic logical relationships (e.g. equivalence, narrowness, compatibility and independence) which form the basis for higher level constructs (e.g. ordering, linear and statistical independence). Every other structure imposed on the statements will need to ``play'' nice with these fundamental relationships.

\emph{Note on terminology.} The terminology may depart slightly from what logicians may be accustomed to, simply because our aim is different. Namely, our goal is not to study the rule of inference or study what can be formally proven, but simply to keep track of the logical relationships between statements that are assumed to be already given.

\emph{Statements as primary object.} Every formal theory needs prime objects, elements that will not further be described by the theory. We will take statements, assertion that are either true or false for everybody, as these objects. The formal system will specifically:
\begin{description}
	\item not be propositional; the language of the statements, its syntax, its grammar, will be left unspecified; statements will represent the content regardless of how it is expressed
	\item be algebraic; operations on statements will not form new statements, but describe relationships within a given set of statements; note that most mathematical structures used in science are algebraic (computer science is a notable exception)
	\item purely formal; it will not not try to capture the semantic of the statements, though the semantic will impose logical relationships that will be formally captured
\end{description}

\emph{Statements cannot exist in isolation.} Statements should, in general, not be regarded as independent of the context they are part of. Take the statement ``The mass of the electron is 510 $\pm$ 0.5 KeV''. If we are measuring the mass of the electron, that is a statement that may or may not be true depending on experiment. If we are performing particle identification in a detector, that statement is assumed to be true. The truth values allowed by the same statement depends on context.

\emph{Logical context as the fundamental structure.} A logical context is a set of statements with well defined logical relationships. It defines which truth assignments are possible (i.e. are consistent with the semantic of the statements). It identifies one of the possible assignments as the true one. It allows to find statements whose truth value depends on others through a particular Boolean function. The existence of this structure is introduced by axioms as they are crossing the lines between physical objects and mathematical ones.

\emph{Tautologies, contradictions and contingent statements.} We define tautologies as those statements that are true in every assignment, contradictions as those statements that are false in every assignment and contingent all the remaining.

\emph{Statement equivalence and Boolean algebra.} Within our formal system, we can distinguish between:
\begin{description}
	\item statement equality: two statements that re-express the same fact in different languages or in different words
	\item statement equivalence: two statements have the same truth value in all possible assignments
	\item statement material equivalence: two statements have the same truth value
\end{description}
A logical context is a complete Boolean algebra under statement equivalence.

\emph{Narrowness and ordering.} A statement is narrower then another if whenever it is true then the other is also true. Narrowness can often be thought as implication, except in a few corner case (e.g. a statement that is always false is narrower than any other statement). Narrowness imposes a partial order to the structure. This is the same order associated to the fact that every Boolean algebra is a complemented distributed lattice.

\emph{Compatibility and independence.} Two statements are said to be compatible if they can both be true in the same assignment. Two statements are independent if fixing the truth value of one does not restrict the truth value of the other. Independence, like linear independence or statistical independence of which it is the foundation, is a property of a set of statements and is not transitive.

\subsubsection{Verifiable statements}

Logical contexts need to keep track which statements are experimentally verifiable. A statement is experimentally verifiable if we are provided with a test that, if the statement is true, will always terminate successfully in a finite time. The fact that the test has to terminate in a finite time in the positive case and that it may not terminate in the negative case has profound implications: verifiable statements are not closed under the standard logical connectors. The negation of a verifiable statement is not in, general, a verifiable statement. The finite conjuntion (i.e. logical AND) of verifiable statements is a verifiable statement, but not an infinite one. The countable disjunction (i.e. logical OR) of verifiable statements is a verifiable statement, but not a more then countable infinite one.

\emph{TODO.} The negation of a verifiable statement is not in general a verifiable statement. We may be able to verify that ``there exists extra-terrestrial life'' if at some point we find a clear signal. Absence of signal, however, will never confirm that ``there is no extra-terrestrial life''. 


\subsubsection{Experimental domains}

We will define an experimental domain as a set of verifiable statements that can be expressed as the combination of a countable subset (i.e. a countable base). Because of finite time verifiability, this is the biggest space we can experimentally probe given an indefinite amount of time. An experimental domain, then, represents all the information that can be gathered experimentally about a particular subject.

From each experimental domain we construct a theoretical domain by allowing negation as well. This will include all statements for which an experimental test is in principle possible, though there is no guarantee of termination. A theoretical domain, then, represents all statement to which we can attach a prediction.

Within a theoretical domain we define the set of possibilities as those statements that, once known to be true, will set the truth value of all other statements. Each possibility represents a possible case that is distinguishable experimentally, which corresponds to a unique possible assignment for the experimental and theoretical domain. Because of the countable base, the set of possibilities can never be greater than the continuum.

To sum up, an experimental domain consists of a set of statements that can be tested experimentally (e.g. ``the animal has whiskers'', ``the mass of the photon is less than $10^{-13} eV$''). The theoretical domains extends to statements that may not be tested experimentally (e.g. ``the mass of the photon is exactly 0 eV''). The possibilities consists of all the possible cases (e.g. ``the animal is a cat'', ``the mass of the photon is exactly $0.102 10^-{34} eV$ ).

\subsubsection{Topologies and $\sigma$-algebras}

An experimental domains provide a natural topology over the possibilities. Each verifiable statements can be identified with the set of possibilities compatible with it (e.g. the statement ``the mass of the photon is less than $10^{-13} eV$'' is equal to the disjuntion of all statement of the form ``the mass of the photon is exactly x $eV$'' with $x <  10^{-13}$). Since verifiable statements are closed under finite conjunction and countable disjunction, the sets corresponding to verifiable statements form a $T_0$ second countable topology.

On the other hand, theoretical domains provide a natural $\sigma$-algebra over the possibilities. Each theorical statement can also be identified with the set of possibilities compatible with it. Since theoretical statements are closed under negation and countable disjunction, the sets corresponding to theoretical statements form a $\sigma$-algebra, which is the Borel algebra of the natural topology.

Topologies and $\sigma$-algebras provide the foundations for differential geometry, Lie algebras, measure theory, probability theory and many other mathematical tools used in physics and the sciences. As we have now have a precise understanding of what they represent, all concepts and proofs in those subject can be understood in terms of experimental verifiability.

\subsubsection{Status and open issues}

This part of the work is very well developed. More work could be done in finding meaning for all mathematical concepts (e.g. compact sets, all separability axioms, ...). It would be interesting to work with people from different fields (i.e. logic, foundations of mathematics, philosophy of science, ...) to see whether any aspect of this section would appeal to different communities.

\subsection{Common constructions}

With the basic mathematical structure defined, we develop some definitions and constructions that are of general use.

\subsubsection{Relationships and equivalence between domains}

We define two relationships between domain: inference relationships and causal relationships. An inference relationship establishes that testing a verifiable statement in one domain is the same as testing a verifiable statement in the other. Mathematically it is a map that takes a verifiable statement from the one and returns a verifiable statement from the other that is equivalent to the first. A causal relationship establishes that determining which possibility is true in one domain also determines which possibility is true in the second. Mathematically it is a map that given a possibility of the first returns a possibility of the second that is broader than (i.e. implied from) the first.

Among the main results we find that there exists an inference implication between two domains if and only if there exists a causal relationship between them. The direction of the inference is the opposite of the causal direction. Therefore we say that a domain depends on another if there exists an inference relationship between the first and the second or if there exists a causal relationship between the second and the first. Causal relationships must be continuous function in terms of the natural topologies, which justifies why functions in science are always assumed to be ``well-behaved'' (i.e. analytically continuous with at most countable discontinuities).

\subsubsection{Composite domains}

\subsubsection{Relationship domains}

\subsubsection{Status and open issues}

The overall mechanics of composite domains and relationships domains are fairly well developed and understood. There are details, though, that can be understood better. The connection to category theory can be also better developed. Other common concepts and constructions, like similarities or sequences of domains, could be formulated.

\subsection{Properties and Quantities}

\subsubsection{Properties and quantities}

\subsubsection{Ordering theorems for discrete and continuous quantities}


If a scientific theory is a model, then it is a collection of statements with well-defined logical relationships between them. For example, a model of the solar systems will be able to say characterized by statements such as ``the earth rotates around

* Formal theories are specializations of more general ones
* Algebraic structures vs Propositional structures
* Physical theories as models; the theory of models
* Difference from ontological theories and subjective theories; accessible information
* Verifiable statements
* Meaning of statements can't be captured, requires context

\section{Physics theories}

\subsection{Classical mechanics}

Classical mechanics can be derived from three main assumptions: infinitesimal reducibility, determinism and reversible evolution and kinematic equivalence. From the first assumption we can show that the state space of the infinitesimal parts of a system (i.e. particles) has the structure of phase space. From the second we can show that the time evolution follows Hamiltonian mechanics. From the third we can show that the system is Lagranian and is restricted to the case of massive particles under potential forces.

\subsubsection{Classical state spaces}

\textbf{Infinitesimal reducibility.} A system is said reducible if giving the state of the whole system is equivalent to giving the state of its parts and vice-versa. For example, given a ball, we can throw it and study the motion of the ball. Alternatively, one can take a red marker, make a red dot on the ball and study the motion of the dot. The system is reducible if studying the system is equivalent to studying the motion of all possible red dots. The system is infinitesimally reducible if we can keep reducing the system to smaller and smaller parts. We call particle an infinitesimal part of the system, the limit of recursive subdivision. The system is then described by a distribution of the state its parts over phase space, which is the only structure that allows coordinate invariant distributions over continuous variables.

\emph{Composite states are distributions over particle states.} Under infinitesimal reducibility, if $\mathcal{C}$ is the state space of the while system and $\mathcal{S}$ is the state space of the particles, then each state $c \in \mathcal{C}$ for the full system is identified by a distribution $\rho : \mathcal{S} \to \mathbb{R}$ over the states of infinitesimal parts. The function is real to signify that we can associate arbitrarily small amounts to each particle states. The distribution tells us how many particles can be found in each particle state.

\emph{Continuous time implies particle state space is a manifold.} If time is assumed to be continuous, then the state space $\mathcal{S}$ is a manifold. That is, each particle state $s \in U \subseteq \mathcal{S}$ can be identified by a set of continuous quantities $\xi^a : U \to \mathbb{R}$ which we call state variables. It is a consequence of the general theory that, if we wanted to write a trajectory $\xi^a(t)$, then this must be a topologically continuous function. Therefore, conceptually, once we assume time to be a real number, the state space of the particles must be charted by a set of real quantities: the topology that is given to time reasserts itself on all the quantities that will depend on time.

\emph{Densities imply differentiability.} As we can write the state $s(\xi^a)$ as a function of the state variables $\xi^a$, we will also want to write the density $\rho(s(\xi^a)) = \rho(\xi^a)$ as a function of the state variables. This requires the state variables to be differentiable with respect to the distribution and with each other. Recall that if the Jacobian is not well defined during a change of variable $\rho(\hat{\xi}^b) = \left|\frac{\partial \hat{\xi}^b}{\partial \xi^a}\right| \rho(\xi^a)$ then the density wouldn't be expressible in the new coordinates. Mathematically, this means that the manifold is equipped with a differentiable structure. Note that any time evolution that would map a density to another density will not only have to be continuous, but also differentiable. In short: assuming density distributions as our primary objects, instead of points, justifies why differentiable structures and differentiable time evolution is present in physics.

\emph{Coordinate invariant densities imply phase-space.} As particles states must be defined independently of coordinate systems, they must be invariant under coordinate transformations. That is, $s(\xi^a) = s(\hat{\xi}^b(\xi^a))$. As the density distribution depends only on the state, it should be invariant as well, that is $\rho(s(\xi^a)) = \rho(s(\hat{\xi}^b(\xi^a)))$. We have two seemingly contradicting requirement: $\rho$ should both change like a density and be invariant. The only way to satisfy both requirements is the following: particle states are fully identified by a set of coordinates $q^i$ and by conjugate set of variables $k_i$ whose units are the inverse of the ones defined by $q^i$. This way the product $dq^i dk_i$ is a pure number that is invariant under coordinate transformations. A coordinate transformation $\hat{q}^j = \hat{q}^j (q^i)$ induces a transformation $\hat{k}_j = \frac{\partial q^i }{\partial \hat{q}^j} k_i$ such that the area element $d\hat{q}^j d\hat{k}_j = dq^i \frac{\partial \hat{q}^j }{\partial q^i} \frac{\partial q^i }{\partial \hat{q}^j} dk_i=dq^i dk_i$ is invariant. An independent degree of freedom, then, is a pair $(q,k)$ for which the units can be changed independently from all other state variables. Mathematically, symplectic manifolds are the only spaces that allow us to define coordinate invariant densities. We can introduce a constant $\hbar$ such that $p_i = \hbar k_i$ which at this point is simply an arbitrary choice of unit.

These results gives us a precise understanding of what each layer of the mathematical structure corresponds to physically:
\begin{description}[noitemsep]
	\item \emph{Set of points:} possible states for the particles.
	\item \emph{Topological space:} ability to experimentally distinguish the states.
	\item \emph{Manifold:} states can be distinguished with a set of real valued quantities or state variables.
	\item \emph{Differentiable manifold:} we can define a distribution over the states and the state variables.
	\item \emph{Symplectic manifold:} we can define a distribution over the states that are coordinate independent.
\end{description}

\subsubsection{Hamiltonian mechanics}

\textbf{Determinism and reversibility.} A system undergoes deterministic and reversible evolution if given the initial state one can predict the final state and given the final state one can reconstruct the initial state. In other words, the system is predictable and retrodictable. Densities at each past state must be equal to the density of the corresponding future state, which leads to Hamiltonian dynamics.

\emph{Reversibility as reconstructing the past.} This definition of reversibility should not be confused with time symmetry or the ability to undo the change. The first one means that if one substitute $t \to -t$ the dynamics is invariant (i.e. the same) or form invariant (i.e. different but described by the same equations) depending on the author. The second one means that we can find another process that brings the final state back to the original state. These concepts do not seem to work well in the general case. For example, when discussing reversibility in the sense of undoing the changes, one typically discusses changing the direction of the velocities to bring about the initial state with inverted velocities. In the presence of forces or an arbitrary dynamics, this operation because more complicated or ill-defined. The core problem with this definition is that it is not a property of the process itself, but on the existence of other processes (i.e. the process that undoes the changes or the process that flips the velocities, therefore changing the state). The requirement of time-invariance is too restrictive as it excludes all time dependent processes. Form invariance on an arbitrary set of equations, instead, can be broken by a suitably complicated change of variables, so it is only well-defined for a limited number of systems (e.g. particles under potential forces). The definition of reversibility we use is preferable because it works more generally and, when appropriate conditions apply, reduces to the other two which are better regarded as special cases. It is mathematically more elegant as it is the dual of determinism. It connects directly with information theory, as it states that the present has enough information to reconstruct the past, which gives a logical bridge to connect reversibility with information entropy and thermodynamics/statistical mechanics entropy.

\emph{Determinism and reversibility imply Hamiltonian mechanics.} In the context of infinitesimal reducibility, determinism and reversibility means that all the particles and only the particle within a given initial state $\rho(s(t)) = \rho(s(t+\Delta t))$ will be mapped to a particular final state. That is, the densities will be preserved over time. Time evolution, then, is a canonical transformation in phase space. As time is continuous, we can find a function $H : \mathcal{S} \to \mathbb{R}$ which acts as a time generator, which we recognize as being the Hamiltonian. This is essentially Louisville's theorem applied in reverse.

\emph{Invariant densities under time transformations imply extended phase-space.} So far, time has been treated as a parameter. If we introduce coordinate transformation that mix time, however, this is no longer possible. If $d\hat{q} = dq + vdt$ then the densities need to be defined on $dt$ as well. To do that, we have to introduce a conjugate variable $\omega$ of unit inverse time so that $dt d\omega$ is dimensionless and invariant. The invariant area element becomes $\sum_i dq^i dk_i - dt d\omega$ or $\sum_i dq^i dp_i - dt dE$ in units of $\hbar$. For surfaces at equal time $dt=0$ it reduces to the previous case. For surfaces that are not at equal time, since the temporal degree of freedom does cannot introduce new states, we need to correct the area element to account for the larger areas measured by the state variables that does not correspond to new states. If we set $q^\alpha = [t, q^i]$ and $p_\alpha = [-E, p_i]$ the area element becomes $dq^\alpha dp_\alpha$. These are elements of special relativity that come just by requiring invariance under time transformation. They do not come from invariance of the speed of light, they do not require a metric tensore. In other words, there is no way to have time transformations in Hamiltonian mechanics without requiring a four-momentum co-vector. Special relativity is not an independent choice: it is already partially baked into the two assumptions we took.

\emph{Extended phase-space imply classical anti-particles.} The trajectories in the extended phase space are in terms of an affine parameter $s$ and not time: $q^\alpha(s)$. As the motion is still deterministic and reversible, though, time $t(s)$ must be monotonic with respect to the affine parameter. For each state, the direction of time can be either aligned or anti-aligned with the affine parameter. Because the relationship is monotonic, aligned states can only connect to other aligned states and anti-aligned states can only connect to other anti-aligned states. We call aligned states the particle states and the anti-aligned states the anti-particle states.

\emph{Time evolution is better expressed by the Hamiltonian constraint.} The temporal degree of freedom introduces two variables but, since one is energy, there is an additional constraint since the value of the energy is linked to the other variables through the Hamiltonian. This link is better expressed by the Hamiltonian constraint $\mathcal{H} : \mathcal{\bar{S}} \to \mathbb{R}$ which is a function over the extended phase space, just like the Hamiltonian was a function over the standard phase space. The Hamiltonian constraint has two roles: it gives us the equation of motions in the same way of the standard Hamiltonian, and it constraints the motion over the surface $\mathcal{H} = 0$. To give context, the Hamiltonian constraint for a free particle is $\mathcal{H} = \frac{|p^\alpha|^2}{2m} - \frac{1}{2}mc^2$, which constraints the norm of the four-momentum to $mc$. The Hamiltonian constraint, unlike the standard Hamiltonian, is invariant under time transformation. It is the generator of the evolution in terms of an affine parameter.

\emph{Hamiltonian constraint as operator on distributions.} If $\rho$ is the distribution for the whole system, then $\mathcal{H} \rho = 0$. In fact, the distribution can only be non-zero on the constrain, for which the Hamiltonian constraint is zero. So, for each point in the extended phase space, either the distribution is zero or the Hamiltonian constrain is zero. Note that this is the same form one has in quantum mechanics for the Klein-Gordon and Dirac equations $\mathcal{H} |\psi\rangle = 0$, which in fact can be understood as Hamiltonian constraints that fix the norm of the momentum four-vector and four-velocity.

\emph{Determinism and reversibility is equivalent to conservation of information entropy.} Given a distribution, its information entropy is given by $I = - \int \rho \log \rho \, d\xi^n$. In general, information entropy is not invariant under change of variable: $- \int \rho(\hat{\xi}^b) \log \rho(\hat{\xi}^b) \, d\hat{\xi}^n = - \int \rho(\xi^a) \log \rho(\xi^a) \, d\xi^n - \int \rho(\xi^i) \log \left|\frac{\partial \hat{\xi}^b}{\partial \xi^a}\right| \, d\xi^n$. Under a canonical transformation over phase space, though, the Jacobian determinant is unitary everyone therefore information entropy is invariant. That is, invariance of the density and invariance of its information entropy are one and the same. Requesting that densities are conserved in time is therefore equivalent to requesting that information entropy is conserved in time. This should be intuitive: the amount of information needed to identify an element within the initial or the final distribution is the same if the evolution is deterministic and reversible. Deterministic and reversible evolution, Hamiltonian evolution and conservation of information entropy, then, are one and the same.

\emph{Conservation of information entropy leads to classical uncertainty principle.} As information entropy is conserved, a Hamiltonian process can only map a distribution to one that has the same information entropy. If we consider a single degree of freedom $(q, p)$ and fix the information entropy $I_0$, then the distribution that minimizes the product of the standard deviation is the Gaussian. The information entropy for the Gaussian is given by $I_G = \ln (2\pi e \sigma_q \sigma_p)$. During the evolution, we have:
$$ \sigma_q \sigma_p \geq \frac{e^{I_0}}{2\pi e} $$.

\subsubsection{Lagrangian mechanics and massive particles}

\textbf{Kinematic equivalence.} A system is said to satisfy kinematic equivalence if giving the state of the system is equivalent to giving the trajectory in space and vice-versa. That is, for each possible state there is one and only one trajectory associated with it. Note that this is not true in general: for example, given the trajectory of a photon we cannot reconstruct its momentum since all photons travel at the same speed. Under this assumption the relationship between position/momentum and position/velocity is invertible and the system admits a Lagrangian. Moreover, given the transformation rules of momentum and velocity, we find that there must be a linear relationship between the two, which constrains the dynamics to the one of massive particle under potential forces.

Note that we use $(q^\alpha, p_\alpha)$ for position and momentum and $(x^\alpha, u^\alpha)$ for position and velocity even though $q^\alpha = x^\alpha$. We do this because $\frac{\partial}{\partial q^\alpha} \neq \frac{\partial}{\partial x^\alpha}$ since one is taken at constant $p_\alpha$ and the other at constant $u^\alpha$. This is a tremendous source of confusion which is avoided by the use of the two symbols.

\emph{Kinematic equivalence means having an invertible relationship between velocity and momentum.} If the state of the system $(q^\alpha, p_\alpha)$ is enough to determine the trajectory $x^\alpha(s)$, then it will be enough to determine the position and velocity $(x^\alpha, u^\alpha)$. These will be enough to reconstruct the state as they span a space of equal dimension. As $q^\alpha = x^\alpha$, then $u^\alpha = u^\alpha(q^\beta, p_\gamma)$ is invertible.

\emph{Kinematic equivalence leads to Lagrangian mechanics.} Since $u^\alpha(q^\beta, p_\gamma)$, we can write $\mathcal{L} = \dot{q}^\alpha p_\alpha - \mathcal{H}$ as a function of $(x^\alpha, u^\alpha)$. That is the Lagrangian of the system. Note that this cannot be done if we do not have kinematic equivalence. For example, for a photon treated as a particle, we have $H = c |p|$. Using Hamilton's equations, we have $u^i = \frac{dx^i}{dt} = c \frac{p}{|p|}$ which is not invertible. While we can write the Lagrangian, this cannot be expressed in terms of position and velocity.

\emph{Kinematic equivalence leads to linear relationship between velocity and momentum.} Being able to go from $(q^\alpha, p_\alpha)$ to $(x^\alpha, u^\alpha)$ and vice-versa is not enough. We need to express the density $\rho$ in terms of position and velocity as well. This means expressing area elements $dq^\alpha dp_\alpha$ in terms of $dx^\alpha$ and $du^\alpha$. Since $dx^\alpha = dq^\alpha$, we must have $dp_\alpha = m g_{\alpha \beta} du^\beta$ where $g_{\alpha \beta}$ is an arbitrary linear function and $m$ is an arbitrary constant of proportionality. 

\emph{Linear relationship between velocity and momentum leads to massive particles under potential forces.} We can integrate the relationship $dp_\alpha = m g_{\alpha \beta} du^\beta$ and find $p_\alpha = m g_{\alpha \beta} u^\beta + A_\alpha(x^\gamma)$ where $A_\alpha$ are arbitrary functions. Since $u^\alpha = \frac{dq^\alpha}{ds} = \frac{\partial \mathcal{H}}{\partial p_\alpha}$, we can take the previous relationship, integrate it again and find $\mathcal{H} = \frac{1}{2m} (p_{\alpha} - A_\alpha) g^{\alpha \beta} (p_{\beta} - A_\beta) + V$. We recognize this to be the Hamiltonian for a massive particle under potential forces.

\emph{Inertial mass measures the number of states per unit of velocity.} Inertial mass is usually introduced using the Newtonian relationship $F=ma$: the lower the mass, the easier is accelerate the body. If the mass where zero, then, we would expect the body to be infinitely easy to accelerate. If the mass is zero, however, the velocity is fixed and the body cannot be accelerated. The derivation offers a different insight: inertial mass is the constant of proportionality between ranges of velocity and ranges of states, as measured by momentum. The higher the mass, the more possible states are there per unit of velocity. If there are more states per unit of velocity, then the body is more difficult to accelerate as we have to go through more states, which recover the Newtonian intuition. However, if the mass is zero, set of states defined in a unit of velocity has measure zero: we cannot change state, we cannot accelerate.

\emph{The speed of light is a ration between the possible states in a unit of space over the possible states in a unit of time.} Regarding the constant $c$ as a speed is problematic. It is unclear why a speed, which is a property of motion, would be part of the metric of the space-time, which is defined regardless of motion and of what is there in the space itself. The derivation offers a different insight. As we move a state in space, it will go through different possible states: this can be quantified in phase space. As we move a state in time, it will also go through different possible states: this can be quantified on the extended phase space. The ratio between the two is the constant $c$. If we imagine a particle moving in both space and time, the position may remain the same but the time will always need to change. As the motion is deterministic and reversible, at most we can have one position in space for every position in time. Therefore, over a finite time, we must have $\frac{\Delta x}{\Delta t} \leq c$. Thinking about the constant $c$ as relating the count of states in space over time is more fundamental.

\emph{The Plank constant, the speed of light and the inertial mass serve to transfer the measure of states from one quantity to another.} As we saw initially, $(q, k)$ are the quantities on which we can measure the number of states. The constant $\hbar$ serves to transfer the count from $dk$ to $dp$. The inertial mass $m$ serves to transfer the count from $du$ to $dp$. The speed of light served to transfer the count from $dt$ to $dx$. They are book-keeping tools we use as we express states over different quantities which allow us to express in different ways the fundamental relationship coming from determinism and reversibility: the count of states must remain the same.

\subsubsection{Status and open issues}

We consider classical mechanics to be essentially forward engineered. There are some details, though, that could be better understood. The formalization will need a lot more time as it has many mathematical prerequisites.

\emph{Physical meaning of the Poisson brackets.} We still miss an understanding of the Poisson brackets as a generalization of Jacobian determinants. In principle, they should tells us how densities over a degree of freedom formed by two variables transform. We haven't found the right approach.

\emph{Densities as measure theoretic derivatives.} As we want finite objects to be our starting points, densities should be the ration of amount of ``stuff'' divided by the size of the region. This, in  measure theory, is known as the Radon-Nikodym derivative. We want this idea to be combined with statistics and differential geometry, so that we can talk about marginal distributions. It seems that gemoetric measure theory at least partially addresses that, which would be the first step for a full formalization of this section of the project.

\emph{Relationship between metric tensor and symplectic form.} The way the derivation works, the metric tensor is a secondary object that should be understood as being applied to a differential of position and a differential of velocity (e.g. $dx^\alpha g_{\alpha \beta} du^\beta$). This gives us the number of states on an area defined by the differentials. It is unclear whether the curvature of $g_{\alpha \beta}$ is linked to the geometry set by the symplectic form.


\subsection{Quantum mechanics}

\subsection{Thermodynamics}

\subsection{Statistical mechanics}




\end{document}
