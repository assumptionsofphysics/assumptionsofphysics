%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Trim Size: 9.75in x 6.5in
%% Text Area: 8in (include Runningheads) x 5in
%% ws-ijqi.tex   :   18-6-08
%% Tex file to use with ws-ijqi.cls written in Latex2E.
%% The content, structure, format and layout of this style file is the
%% property of World Scientific Publishing Co. Pte. Ltd.
%% Copyright 1995, 2002 by World Scientific Publishing Co.
%% All rights are reserved.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\newcounter{myctr}
\def\myitem{\refstepcounter{myctr}\bibfont\noindent\ifnum\themyctr>9\else\phantom{0}\fi\hangindent17pt\themyctr.\enskip}
\def\myhead#1{\vskip10pt\noindent{\bibfont #1:}\vskip4pt}

\documentclass{ws-ijqi}
\usepackage{hyperref}
\usepackage[super,sort,compress]{cite}
\usepackage{url}
\begin{document}

%%%%%%%%%%%%%%%%%%%%% Publisher's Area please ignore %%%%%%%%%%%%%%
\catchline{}{}{}{}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{THE FUNDAMENTAL CONNECTIONS BETWEEN CLASSICAL HAMILTONIAN MECHANICS, QUANTUM MECHANICS \\ AND INFORMATION ENTROPY}

\author{GABRIELE CARCASSI}

\address{Physics Department, University of Michigan, 450 Church Street\\
Ann Arbor, MI 48109-1040,
United States\\
carcassi@umich.edu}

\author{CHRISTINE A. AIDALA}

\address{Physics Department, University of Michigan, 450 Church Street\\
Ann Arbor, MI 48109-1040,
United States\\
caidala@umich.edu}

\maketitle

\begin{history}
%\received{Day Month Year}
%\revised{Day Month Year}
%\accepted{Day Month Year}
%\comby{(xxxxxxxxxx)}
\end{history}

\begin{abstract}
In a previous work we have shown how the principal difference between classical Hamiltonian mechanics and quantum mechanics is not the physical size of the object being studied, but the degree of accessibility its internal dynamics. Namely, in classical mechanics it is assumed to be fully accessible while in classical mechanics is assumed to be totally inaccessible. As information entropy can be used to characterize how much the state of the whole system characterizes the state of its parts, it provides fundamental insights that allow us to properly understand the analogies and differences between the two theories. Conservation of information, for example, leads to Hamiltonian and Schroedinger evolutions, which describe the deterministic and reversible dynamics. We review
\end{abstract}

\keywords{Information entropy; Hamiltonian mechanics; Quantum mechanics; determinism and reversibility.}

%\tableofcontents  % optional

\markboth{Carcassi, Aidala}
{The Fundamental Connections between Mechanics and Information Entropy}

\section{Introduction}
In a previous work\cite{Carc1} we identified a small set of physical assumptions from which classical and quantum particle mechanics can be rederived. By assumptions here we mean simplifying conditions that our system is supposed to satisfy under the processes under study. Our work differs from others\cite{PhysRevA.84.012311} in that the starting points are physical assumptions about the system being studied (e.g whether the parts can be studied or whether the evolution is deterministic) and not some abstract mathematical or information theoretic considerations. It is also different in that both classical and quantum cases are derived on equal footing. That is, we have identified a single point where one assumption leads to the classical case while another lead to the quantum system.

The difference is the answer to a single question: is the internal dynamics of the system accessible or not? If it is fully accessible we recover classical states and if it is totally inaccessible we recover quantum states. This difference can be understood in terms of information entropy and in this paper we develop that insight. We will present here a much shorter version of the classical derivation that carries all the important points. For quantum mechanics, unfortunately we do not currently have one, so we will cover only some of the general ideas. For details on both, we refer to the previous longer paper. We then provide a classical analogue of the quantum uncertainty relationship and see how both can be understood in terms of information entropy conservation under the different initial assumptions.

We believe these insights better clarify the difference between classical and quantum systems, their limit of validity and therefore where one can expect them to fail.

\section{Reducible Systems and Classical Phase Space}
We define a classical system as one that is assumed to be infinitesimally reducible. That is, the system can be divided into parts indefinitely and giving the state of the system is equivalent to giving the state of its parts. For example, we can throw a ball and study its motion; or we can take a red marker, make a dot on the ball and study the motion of the red dot. Studying the motion of the whole ball is equivalent to studying the motion of all the possible red dots we could draw. To a first approximation, a ball can be thought as infinitesimally reducible, made of a continuous material.

If we assume our system is reducible, then the state of the whole system is simply a distribution over the states of the infinitesimal parts, which we call particles. That is, our state is a normalized integrable function
\begin{equation}
\begin{aligned}
	\rho : \mathcal{S} \to \mathbb{R} \\
	\int_{\mathcal{S}} \rho ds = 1
\end{aligned}
\end{equation}
over the particle state space $\mathcal{S}$. If we have a set of state variable $\xi^a : \mathcal{S} \to \mathbb{R}$ that are sufficient to characterize our particles, we can write both the density and the information entropy
\begin{equation}
\begin{aligned}
\rho(\xi^a) &= \rho(s(\xi^a)) \\
I[\rho] &= - \int_{\mathcal{S}} \rho \log (\rho) d\xi^1 d\xi^2 ...
\end{aligned}
\end{equation}
in terms of the state variables.

The issue is that changing coordinates system will change the numeric value of the density. Yet, the density is fully specified by the particle states and, since states are invariant under coordinates transformations, it should be invariant. That is, we have
\begin{equation}
\begin{aligned}
\rho(\hat{\xi}^b) &= \left| \frac{\partial \hat{\xi}^b}{\partial \xi^a} \right| \rho(\xi^a) \\
\rho(s(\hat{\xi}^b)) &= \rho(s(\xi^a)) \\
\end{aligned}
\end{equation}
which means our distribution $\rho$ has to both transform as a scalar (i.e. it fully depends on the state) and as a density (i.e. the integral needs to be invariant).

This apparent contradiction is actually a formal requirement that severely limits what types of manifolds can be used to describe particle states. It turns out that these are precisely the symplectic manifolds: the structure of phase space is exactly the one required to write coordinate invariant distributions. That is, given a set of coordinates $q^i$ (state variables that define the reference frame) we will need another set of state variables $k_i$ with inverse units such that their product will become an invariant pure number. Under a general coordinate transformation $\hat{q}^j = \hat{q}^j (q^i)$ we will simply have 
\begin{equation}
\begin{aligned}
dq^i dk_i = d\hat{q}^j d\hat{k}_j
\end{aligned}
\end{equation}
which means the Jacobian of the transformation are unitary. The invariance of areas is equivalent to the invariance of densities and of information entropy. In other words, the structure of phase space is precisely the one that allows us to define information entropy as coordinate independent. Only on these spaces information entropy can be a physically well-defined quantity over continuous variables.

If we require deterministic and reversible evolution, we require that the density over one state is transported exactly to another state. Equivalently, we require that the information given at one time to identify a part is exactly the same to identify the same part at a different time: information entropy is the same. In this case we are requiring that the time evolution is an area preserving (i.e. canonical) transformation. This gives us Hamilton's equations. This tells us that the following four concepts
\begin{itemize}
	\item Deterministic and reversible evolution - state densities are mapped one-to-one
	\item System isolation - the evolution of the system depends only on its state and on nothing else
	\item Conservation of information entropy - the information required to describe the system does not change in time
	\item Conservation of energy - Hamiltonian evolution
\end{itemize}
are the same concept from different angle.

\section{Irreducible systems and quantum mechanics}

Conversely, we define a quantum system as one that is assumed to be irreducible. That is, the state of the system tells us nothing about the state of the parts. For example, we can isolate an electron and study its motion; yet we cannot put a red mark on part of the electron, we cannot scatter a photon off of only a part of an electron. As the interaction is always with the whole electron, no information can be gained about its internal dynamics. An electron can be thought as irreducible.

We stress that both these assumptions are operational and depend not only on the system, but also on the processes we are studying or available to us. For example, a proton can be modeled as a single quantum system and will behave as such provided the internal dynamics is not relevant. It will exhibit diffraction and interference patterns, have half-integer spin and so on. If we interact in a way that disturbs the internal dynamic, however, we will see that there is internal dynamics in terms of quarks and gluon. In those regimes the proton cannot be treated a single quantum system, but as a composite system.

We weren't able to prepare a shortened version of the quantum derivation, but the gist is that, since the state of the system does not capture the state of the parts, we can imagine these continuously permutating without affecting the dynamic of the whole. The internal motion is the characterized by random variables, a pair $(A,B)$ for each degree of freedom, which maximize the entropy of the distribution of the parts. This means that the variable along each degree of freedom are independent and only the second moments are significant (i.e. statistically equivalent to a multivariate Gaussian). A complex number $\sigma_A + \imath \sigma_B$, then, can be used to keep track of a variable within a degree of freedom and ultimately leads to the state space of quantum mechanics. The norm of the quantum number represents the strength of the internal random process, which is proportional to the size of the system, and the phase is the correlation angle. Superposition, then, means superimposing the random motions which will interfere constructively where correlated and interfere destructively where anti-correlated.

We want to stress, again, that we consider the classical state to be a matter distribution $\rho(q^i, k_i)$. Therefore, when comparing classical and quantum mechanics, we should be comparing the properties of our wave function $\psi(q^i)$ to the matter distribution $\rho(q^i, k_i)$, not to points of phase space. A point-particle is the limit where the extent of the distribution can be considered small, and therefore the center of mass is enough for our description. This limit can be performed within both classical and quantum mechanics if the scale of the problem allows, and, because of Ehrenfest theorem, the behavior is the same. In light of the assumptions, it should be obvious this should be the case: the difference between classical and quantum is in the description of the parts; if the length scale are much greater than the system, then it doesn't matter what we assume about the parts.

\section{Uncertainty principles and information entropy}

Since classical Hamiltonian evolution conserves information entropy, the space of distribution explored by time evolution is limited by that constrain. We then ask the following question: what is the distribution that, given a certain amount of information entropy, minimizes the spread over phase space? That is, what is $\rho(q, k)$ such that $\sigma_q\sigma_k$ is minimized? Given the constrain, no Hamiltonian evolution can lead to a state less spread than that.

It turns out this distribution is the product of two Gaussian distributions
\begin{equation}
\rho(q,k) = \frac{1}{2\pi\sigma_q\sigma_k} e^{- \frac{(q - \mu_q)^2}{2 \sigma_q^2}} e^{- \frac{(p - \mu_p)^2}{2 \sigma_p^2}}
\end{equation}
with information entropy
\begin{equation}
I_G = 2 \pi e \sigma_q \sigma_k.
\end{equation}
Since this is the distribution that minimizes the spread, during the evolution we have
\begin{equation}
\sigma_q \sigma_k \leq \frac{I_0}{2 \pi e}
\end{equation}
where $I_0$ is the initial information entropy. This classical uncertainty relationship is reminiscent of the quantum uncertainty relationship for both the form and the fact that the equality holds for the Gaussian packages. The analogy can be made stronger with a few extra conditions, though we will not expand that here. What we want to show here is the connection between this uncertainty relationship, information entropy, deterministic and reversible evolution and the difference in reducibility between a classical and a quantum system.

First of all we note that this classical uncertainty relationship is given by conservation of entropy and therefore deterministic and reversible evolution. We are free to set the initial amount of information entropy for the classical distribution. Once it is set, this cannot be changed under Hamiltonian evolution. Quantum mechanics has a stronger condition: it is not possible to create a state that violates the uncertainty. That is, we do not have the initial freedom we have in classical mechanics. For quantum states, all pure states have the same information entropy which is set to zero. This difference is a direct consequence of the difference in reducibility: in classical mechanics we can always access the internal dynamics, and therefore we can create narrower and narrower distributions (e.g. we can always divide a continuous material) and study their individual evolution; in quantum mechanics we cannot (e.g. we cannot prepare half an electron) so all states necessarily are associated with the same information entropy. The entropy for a pure state, then, is set to zero to signify that the dynamics within a quantum system is not accessible, there is no information to be had.

The deterministic and reversible case in quantum mechanics is given by unitary evolution, which in fact conserves Von Neumann entropy because each pure state is mapped to a pure state. Conversely, the Lindblad equation allows the entropy to increase and corresponds to irreversible evolution. However, in that case it is the entropy of the mixed case that is being increased. In classical mechanics this would correspond to mapping one initial distribution to multiple final distribution with the same entropy according to different probability. That is, we have a probability distribution over matter distributions. As each matter distribution has the same entropy, it is the spread in probability that increases the entropy. But in classical mechanics we can also do something more: we can have a density distribution spread more or less over phase space with a dynamic that is not Hamiltonian and change the information entropy of the matter distribution itself. For example, a damped harmonic oscillator would concentrate the distribution around the equilibrium and decrease the entropy. This type of dynamic cannnot be described in the context of quantum mechanics. Another aspect of this same issue is that given two quantum states we can always find a unitary evolution that connects the two (at least mathematically) while this is not possible in classical Hamiltonian mechanics (e.g. two distributions with different entropy cannot be linked by Hamiltonian evolution).

We believe these insights clarify two issues. The first is that the difference between a classical and quantum system is not primarily about is size. The previous discussion does not depend on the physical extension of the system or about the number of its degrees of freedom. It is about the accessibility of the internal dynamics. If we are to look at scenarios where classical mechanics fails, we simply have to look at cases where we can no longer assume that the internal dynamics is indefinitely accessible. In a Bose-Einstein condensate, in a pair of entangled photons or in a diffracting neutron the parts cannot be studied independently without significantly altering the system: infinitesimal reducibility does not apply. Conversely, if we are to look at scenarios where quantum mechanics fails as applied for a specific system, we simply have to look at cases where the internal dynamic of that system is indeed accessible.

The second issue is that it tells us what is being quantized to begin with. Quantities like energy, spin or momentum are indeed quantized in some cases, but they are not always quantized in the same way and there is a continuum of different cases. For example, spin along one direction is quantized, but we can continuously change the direction. Energy is quantized only for bound systems, and even in those cases a relative boost can continuously change the value. The entropy associated with all pure states, instead, is always the same for all states in all frames. Again, this is not true in classical mechanics: a matter distribution over phase space can have an arbitrary information entropy. The entropy associated to a pure quantum state, which is itself a distribution (i.e. a wave function), is always the same. That is what is quantized. It if weren't, the Von Neuman expression would not be the correct expression for entropy as each pure state would have to be weighted different. The lack of that term means all pure states carry the same contribution: the information entropy of pure states is what is quantized.


\section{Submitted abstract}
In a recent paper [1] we have shown that classical and quantum Hamiltonian particle mechanics can  be  rederived  from  a  handful  of  physical  assumptions.  We  will  present  and  expand  a consequence   of   that   work,   namely   that   classical   Hamiltonian   mechanics   coincides   with conservation of information entropy, and how this insight allows us to better understand quantum mechanics as well.We  will  start  by  showing  that  classical  Hamiltonian  mechanics  can  be  recovered  from  two fundamental assumptions. The first is that the system we study is infinitesimally reducible: it is made  of  infinitesimal  parts  and  the  state  of  those  parts  is  equivalent  to  the  state  of  the  whole system. The state of the overall system is then a distribution over the states of the infinitesimal parts. As states must be defined independently of coordinate system (i.e. of observer), the density over the states of the parts must be invariant under coordinate transformation. This allows us to recover  the  geometrical  structure  of  phase  space  (i.e.  state  variables  organized  in  conjugate pairs). The second assumption is that of deterministic and reversible evolution: knowing the state at one time is enough to predict the state at future times and reconstruct the state at past times. This means that the density for a given state will be mapped exactly to a future state, which will give us Hamiltonian evolution. We then show that requiring densities to be coordinate invariant and  not  change  in  time  is  equivalent  to  requiring  that  the  information  entropy  (i.e.  the  number bits of identify an element of the distribution up to unit precision) is conserved.Throughout the derivation, we will see how classical analogues of quantum mechanics (i.e. wave number, uncertainty principle, particle/anti-particle states) naturally emerge. We will discuss how quantum mechanics differs from classical mechanics by assuming irreducibility instead: that the system is still made of infinitesimal parts but that the state of the overall system tells us nothing about  the  state  of  the  parts.  We  will  see  how  this  qualitatively  leads  to  the  known  quantum effects.This work is part of a larger project [2] that aims to rederive the known laws of physics from a handful of physical assumptions.

\section{Conclusion}

We have shown that the principal difference between classical and quantum systems is how much the state of the whole can say about it parts. This can be understood in terms of information entropy: classical states as distribution over phase space allow for ever decreasing information entropy while quantum states has wave functions have a fixed entropy which is set to zero. Deterministic and reversible evolution conserve information, therefore we have an uncertainty relationship under Hamiltonian evolution.

We conclude by noting that both the classical assumptions (i.e. the internal dynamic is infinitesimally accessible) and the quantum assumption (i.e. the internal dynamic is totally inaccessible) seem untenable. If we were to suppose that the internal dynamics, though not influenceable, had consequences (e.g. they would decide when a particle decays or how an entangled system is broken), then we would also have processes that increased entropy over time (i.e. more information is needed to identify future state). Such a change of assumption, though, is not trivial as it would necessarily require rethinking the state space as well.

\section*{Acknowledgments}

This section should come before the Appendices and References. Funding
information may also be included here.


\renewcommand\bibname{References}

\bibliographystyle{ws-ijqi}
\bibliography{bibliography}

\end{document}