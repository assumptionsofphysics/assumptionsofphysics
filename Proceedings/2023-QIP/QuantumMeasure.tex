\documentclass[10pt,twocolumn, nofootinbib]{revtex4-2}
%\documentclass[aps,pra,10pt,twocolumn,floatfix,nofootinbib]{revtex4-1}
%\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{assumptionsofphysics}
\usepackage{tikz}
\usepackage{breakurl}

%\usepackage{amsmath}
%\usepackage{mathrsfs}
%\usepackage{amsfonts}
%
%\usepackage{graphicx}
%\usepackage{hyperref}
%\hypersetup{
%	colorlinks=true,
%	citecolor=blue,
%	urlcolor=blue,
%	linkcolor=blue
%}
%\urlstyle{same}
%\frenchspacing
%
\def\>{\rangle}
\def\<{\langle}
\DeclareMathOperator{\erf}{erf}

\begin{document}

\title{How quantum mechanics requires non-additive measures}
\author{Gabriele Carcassi}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}
\author{Christine A. Aidala}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}

\date{\today}


\begin{abstract}
\end{abstract}

\maketitle

\section{Introduction}

The fact that quantum theory is a non-classical probability theory is well established. Quantum probability cannot be recovered assuming the standard axioms of Kolmogorov style probability, and their standard implementation in terms of measure theory. Various alternatives have been developed with different goals: some intend to give a more general account of probability, where classical and quantum cases are subcases; some want to offer better tools for calculation; some are developed to answer more interpretative or philosophical questions. Arguably the most well-known and used alternative approach is the use the quasi-probability distribution over phase space, either in the Wigner or the Glauberâ€“Sudarshan P representation.

While the unsuitability of standard measure theory to represent quantum probability is a crucial aspect, this focus seems to have obscured the fact that measure theory plays another crucial in physical theories: it is used to ``count', or better to ``quantify'', states. As part of our overall project Assumptions of Physics, we have developed an approach, called reverse physics, which aims to start from the laws and find a minimal set of physical assumptions needed to rederive the laws. What we found that the ability to quantify states is a crucial component of classical theory, so much so that it is enough to recover the structure of phase space and the laws of evolution. The former (i.e. the symplectic structure) is the only structure that allows to quantify states, and configurations over independent degrees of freedom, in a way that is observer independent. If one imposes deterministic and reversible evolution, that structure has to be preserved in time, leading to Hamiltonian evolution (i.e. a symplectomorphism). All elements of classical mechanics, including canonical variables, the Lagrangian and the action principle, can be rederived and understood in terms of quantifying the flow of states. Given the formal similarity between classical and quantum mechanics, we believe a similar approach is possible for the latter. Which raises the question: what is the quantum analogue of the Louisville measure, the measure that quantifies states in the classical theory?

Statistical mechanics and information theory mandate a link between entropy and state quantification, as the Shannon/Gibbs entropy over uniform distributions corresponds to the logarithm of the count of states. In the quantum case, we can start with the von Neumann entropy and define a measure that has the same connection. What we find is that this measure is non-additive and that its features can help us better understand what quantization is, why it requires contextuality and the failure of classical probability, and why the latter can be recovered separately within each context.

The goal of this article is to propose and articulate the goals of this new line of research. It provides the vision for future works on both the mathematical details and the conceptual structure. In section II we will show how measure theory is used to quantify elements in a set and how this is needed to assign probability and probability densities to each element. In section II we will see how statistical mechanics and information theory mandate a link between entropy and state quantification, which we will use to define the quantum analogue of the Louisville measure. We will find that this measure is not additive.\footnote{Technically, measures are defined to be additive, therefore what we find is not a measure, but a set function. However, this term is less widespread, so we use the oxymoron ``non-additive measure'' as it is customary in the mathematical literature (TODO: cite a couple of books on non-additive measure theory).} In section IV we will see that this non-additivity is linked to non-contextuality. In section V we will see that quantization means putting a lower unitary bound on the quantification of states, while keeping a finite value over finite regions. We will see that this very directly requires non-additivity and therefore non-contextuality. In section VI we will argue that quantization of space-time means using a quantized measure on the quantification of degrees of freedom. Lastly, in section VII we will outline a series of goals for a research program that aims to fully characterized the quantized measure.

\section{Measure theory and quantifying states}

Much of the focus in quantum foundations lies in the fact that quantum mechanics does not follow the rules of classical Kolmogorov probability. Given that measure theory is typically used to formalize classical probability, numerous efforts start to find an appropriate generalization of measure theory that...The investigation of the use measure theory has been predominantly tied to the Given that this are typically expressed as 

However, measure theory is not used only to characterize probability. More in general, measures are used to determine the size of sets. Even in probability theory, a probability measure assigns probability to sets, and to associate probability to points one needs two measures: the probability measure $p : \Sigma_X \to \mathbb{R}$ and another measure quantifying the extent of the set, which we call \textbf{quantifying measure} $\mu : \Sigma_X \to \mathbb{R}$. Given a set $U$, the ratio $\frac{p(U)}{\mu(U)}$ given us the average probability per unit size. As we shrink $U$ to a single point $x$, we will find the probability associated to that point. Since we are interested in physical theories, the measure $\mu(U)$ quantifying the number of states within the region $U$.

In the discrete case, the quantifying measure is simply the counting measure: the cardinality of the set. Therefore for a single point we simply have $\frac{p(\{x\})}{\mu(\{x\})} = \frac{p(\{x\})}{1} = p(x)$, which corresponds to the probability associated to the state $x$. The counting measure is fully determined by two properties: countable additivity, a defining property of measures, and that each states counts as one.

In the continuous case, things are a bit different. If $X$ is a manifold, a set of local coordinates induce a Lebesgue measure, which can be used to quantify the number of possible value combinations. Note that a single state has measure zero, unlike the discrete case. The Lebesgue measure is defined by two properties: countable additivity and that the size of parallelepipeds corresponds to the volume expressed by the product of the difference of coordinates. Naturally, this is a problem in physics given that the quantification of the number of states should be the same for all observers.

What most miss, is that the structure of classical phase space (i.e. symplectic structure) is exactly the structure that allows to define a quantifying measure for states that is invariant under change of observers.(TODO cite book, two phil papers) Given a change of space-time variables $q^\alpha$, conjugate momentum change covariantly, which means volumes of phase space are invariant. The role of quantifying measure in classical mechanics, then, is assigned to the Louisville measure, which we can understand as a Lebesgue measure on position and conjugate momentum (i.e. canonical coordinates). The amount of probability for each state is given by the Radon-Nikodym derivative $\frac{dp}{d\mu}$, which gives us the probability density.

Note that, in both cases, the quantifying measure is potentially unbounded, as the state space can have potentially infinitely many states, as it is the case for one spatial degree of freedom. Therefore the quantifying measures are implicitly different from probability measures, and they require specific attention.

While quantum mechanics does not obey classical probability, we do use of classical probability measure and quantifying measures in some cases. Measurement outcomes are described in terms of classical probability. If we have a discrete observable, like the energy level of a harmonic oscillator, we would use the counting measure; if we have a continuous observable, like position, we would use the Lebesgue measure associated to the variable. Preparations are also in terms of classical probability, and we also use both types of quantifying measures. For example, the maximally mixed state for a qubit can be written as $\frac{1}{2} | 0 \> \< 0 | + \frac{1}{2} | 1 \> \< 1 | $ or as a uniform mixture over all possible states $\int_\mathcal{H} \frac{1}{4\pi} | \psi \> \< \psi | d\Omega$.

Note how, in both cases, the quantifying measure is selected by the type of preparation or measurement, even though the state space is the same. But this means that these quantifying measure cannot be possibly quantifying the number of states as they would give contradicting results. This makes the problem rather crisp: the space of preparations and measurement outcomes are both classical, yet they cannot be connected by classical means. That is, we cannot simply put a classical probability of transition between the two because we are possibly connecting rather different quantifying measures.

To sum up, the typical foundational question is: how do we generalize probability measures for quantum mechanics? We are asking something different: how do we generalize measures that quantify the number of states in quantum mechanics?

\section{Measure theory and quantifying states}

While the standard structure of quantum mechanics does not have an answer to our previous question, statistical mechanics and quantum information theory have an implicit one. Statistical mechanics tells us that the thermodynamic entropy of a system, which is a physically meaningful quantity that has numerous empirical consequences, corresponds to the Shannon entropy for a classical discrete system, the Gibbs entropy\footnote{By Gibbs entropy we mean the differential Shannon entropy $-\int_X \rho \log \rho d\mu$ calculated over phase space using position and momentum (i.e. canonical coordinates).} for a classical continuous system, and the von Neumann entropy for a quantum system.

There is another expression for entropy, often referred to as the fundamental postulate of statistical mechanics, which is the logarithm of the count of microstates: $\log \mu(U)$.\footnote{Note that the Boltzmann constant is left out, as we will be always measured in bits or nats. Physically, the Boltzmann constant is needed to define temperature in Kelvin. tatistical mechanics can be equivalently formulated in terms of beta $\beta = \frac{1}{k_B T}$.TODO: cite the unit definition paper} In classical mechanics, the measure is indeed the quantifying measure discussed above: the counting measure for the discrete case and the Louisville volume in the continuous case. The two expressions for entropy are linked as the Shannon/Gibbs entropy over a uniform distribution matches the logarithm of the measure of the support. That is, if $\rho_U$ is a uniform distribution over $U$, then $S(\rho_U) = \log \mu(U)$.

In quantum mechanics, as we said before, we do not have a measure to quantify states. There is, however, a somewhat related expression: the maximum entropy attainable by a quantum system described by a Hilbert space $\mathcal{H}$ is the dimensionality $\dim_{\mathbb{C}}(\mathcal{H})$ of the space.

A similar connections can be established in information theory. The Shannon/Gibbs/von Neumann entropy quantifies the minimum number of bits required on average to send a message from a given source.\footnote{For a more general characterization of entropy that works in statistical mechanics, information theory and other contexts, and a more precise discussion of the continuous case, see TODO: cite our paper.} This, again, is a quantity that leads to empirical consequences. If we have $n$ bits at our disposal, we have $2^n$ possible combination, possible messages, that can be encoded. Therefore the logarithm of the number of possible messages corresponds to the entropy. Each different message can be encoded into a different state, therefore $\log \mu(U)$ tells us the number of bits that can be encoded using the states in that region. This will exactly correspond to the number of bits required by a source characterized by a uniform probability distribution over that number of symbols.

While the above discussion does not cover all technical details, the only point we want to make is that the link $S(\rho_U) = \log \mu(U)$ is fully justified by both statistical mechanics and information theory. It is not a capricious request, but rather a necessary requirement stemming not from just one, but from two disciplines that have empirical consequences. Therefore if quantum mechanics does not directly provide us with a measure to quantify states, we can use what it does provide, the von Neumann entropy, to define and calculate said measure.

The simplest case is when $U = \{ \psi \}$ is a single state. In this case, we have $S(|\psi\>\<\psi|) = 0$ and therefore $\mu({\psi}) = 2^{0} = 1$. Note that this is the same as the classical discrete case.

The next case is when $U = \{ \psi, \phi \}$ is the set of two states. A uniform distribution over two states will correspond to the mixed state
\begin{equation}
	\rho = \frac{1}{2} |\psi\>\<\psi| + \frac{1}{2} |\phi\>\<\phi|.
\end{equation}
The entropy of said state will depend on the probability $p=|\<\psi | \phi \>|^2$ in the following way
\begin{equation}
	S(\rho) = - \frac{1+\sqrt{p}}{2} \log \frac{1+\sqrt{p}}{2} 
	- \frac{1-\sqrt{p}}{2} \log \frac{1-\sqrt{p}}{2}.
\end{equation}
If the states are orthogonal, then $p=0$, and $S(\rho) = 1$ therefore $\mu(U) = 2^1 = 2$. In this case we do have that $\mu(\{ \psi, \phi \}) = \mu(\{\psi\}) + \mu(\{\phi\})$, and therefore $\mu$ is additive. We can also see that if $U$ is an orthonormal basis, the von Neumann entropy $S(U) = \log |U|$ is the logarithm of the cardinality of $U$. Therefore the quantifying measure $\mu(U) = |U|$ becomes the counting measure. But this is a special case that corresponds to the highest entropy reachable with a mixture. In all other cases the entropy will be lower, and therefore, in general, $\mu(\{ \psi, \phi \}) \leq \mu(\{\psi\}) + \mu(\{\phi\})$. In other words, the measure $\mu$ is not additive.

To sum up, the physically motivated way to quantify the number of states in quantum mechanics is not additive. When counting states in quantum mechanics, $1+1 \leq 2$.

\section{Non-additivity and contextuality}

While unusual claims are common in quantum mechanics, we want to have a clear, physically tenable, reason as to why the quantifying measure for quantum states must be non-additive. In the context of a spin 1/2 system, for example, why is it that $|z^+\>$ and $|z^-\>$ count as two states while $|z^+\>$ and $|x^+\>$ count as less than two?

Since we saw that the quantifying measure is indeed additive for a set of orthogonal states, this should make us think about contextuality: the impossibility to assign outcomes to all potential measurement of a quantum system. We have, in fact, derived that it is impossible to define an additive quantifying measure on all potential states, but we can do so on the outcomes of one measurement. The ability/inability to define additive measures is exactly the difference between classical/non-classical probability. We are definitely onto something.

While contextuality is often presented as a somewhat abstract, almost metaphysical, property of quantum mechanics, we have a rather more down-to-earth view. The selection of the measurement means a difference in physical process of either the preparation or measurement device. Choosing which direction of spin to prepare or measure means, at some point, orienting some polarizer or magnetic field. Something is changed in the physical world, not just on the system, but around the system, on its environment. If Amanda claims that a particle is in the $|z^+\>$ state, then Boris is able to infer something about the preparation/measuring device.\footnote{This is not at all particular to quantum mechanics. We can study the motion of a cannonball here on earth, but not on the surface of the sun, as the cannonball would be immediately vaporized. The mere claim that a system exists implies that the boundary conditions necessary for that system to exist are satisfies. In this sense, all physics is contextual.} Therefore we can understand the context simply as those conditions on the environment that are necessary on the system to be found in that particular state. A set of orthogonal states, then, has the property that they can be found in the same external conditions.

Given that we have, in general, a correlation between system and environment, it is natural to understand why the contextuality leads to a non-additive measure. Two states, in fact, should be counted as two only if they lead to a change in the system. That is, \textbf{states should be quantified all-else-being-equal}. While two states from different contexts are indeed two distinct states, stating we transitioned from one to the other does not give us information only about the system: it tells us something about the environment as well. The degree of incompatibility of observables, then, ultimately measure the degree of change when realizing the context physically.

So we have a clear, physically motivated reason as to why contextuality is linked to a non-additive measure. States should be quantified at-all-else-being-equal, meaning that possible correlations with the environment have to be removed.

\section{Non-additivity and quantization}

While we have clarified what it means for the quantifying measure to be non-additive, and how this is tied to non-contextuality, we haven't shown why it should be linked to quantization. In fact, what exactly is quantization?

Before trying to answer that question, let's make a direct comparison between the quantifying measures for the three cases: the counting measure $\mu_d$ for the classical discrete case, the Louisville measure $\mu_c$ for the classical continuous case and the newly defined quantum measure $\mu_d$ for the quantum case. This requires a space where all three can be defined, which fortunately exists: the surface of a sphere. First, we want to compare the behavior on a single point. The counting measure returns one, and so does the quantum measure. The Louisville measure, however, returns zero. Second, we want to compare the behavior on a finite region, an open set. The Louisville measure returns the solid angle, which is always going to be finite as it must be less than or equal to $4\pi$. The von Neuman entropy of a mixed state over a two state system is at most one, and therefore the measure must be less than or equal to $2$. However, given that an open set on a sphere has infinitely many point, the counting measure over a finite region will be infinite. Lastly, we want to compare the additivity. Both classical measures are additive, while the quantum measure is not.

Let us list the three properties we have mentioned:
\begin{enumerate}
	\item single states count as one
	\item finite continuous regions correspond to finite information (and therefore finite state quantification)
	\item state quantification is additive on disjoint regions.
\end{enumerate}
These seem very reasonable properties to assume for a quantifying measure of states. \textbf{Unfortunately, these three conditions are incompatible}. If each state counts as one, and the measure is additive, then a finite continuous region must be infinite as it contains infinite points. \textbf{Of the three conditions, we can only pick two}. This is, in fact, what each measure does. The counting measure rejects the second property. The Louisville (and Lebesgue) measure discards the first. The quantum measure sacrifices the third.

So, in the light of this, what is quantization? If we have a system characterized by continuous quantities, such as position, the state space will necessarily be a continuous space and finite regions will have to correspond to finite information as those correspond to the finite precision measurements we can make.\footnote{For a treatment of how finite time experimental verification is captured by topological concepts, and what necessary and sufficient operational assumptions are required for the use of real valued quantities, see TODO libro} If the regions are large enough, the Louisville (or Lebesgue) measure will work. But as we shrink a region we will encounter, given the additivity, regions that contain less than one state, which does not make sense. Low value of the quantifying measure correspond to low values of entropy and measure one corresponds do zero entropy. Regions with measure less than one correspond to negative thermodynamic entropy, which also does not make sense. In fact, classical statistical mechanics does fail in those regimes: the classical equations of state for an ideal gas to predict negative entropy, which is fixed by the Fermi and Bose statistics.

Quantization, then, is fixing the counting measure so that it is bounded from below by one (and the entropy by zero): each state counts as one. However, this comes at a price: non-additivity. This non-addivity is ultimately why classical probability has to fail as well, given that additivity is one of the axioms. It is also the same reason as why quantization implies contextuality: the non-addivity means that some states cannot be prepared at-all-else-being equal.
%(https://en.wikipedia.org/wiki/Sackur%E2%80%93Tetrode_equation)

In our view, this provides good conceptual reasons to understand the failure of classical mechanics, the need for the departure from classical ideas and the need for quantum theory. We believe this is already sufficient reason to start a research program to explore these ideas fully. However, since most physicists are not interested in this type of ``clean-up work'', let's see this is, in our mind, critical for the development of future physical theories.

\section{Implication for space-time quantization}

As we showed before, this line of reasoning gives us a good conceptual understanding of what quantization is: the quantifying measure for states must have a unitary lower bound. If we were to quantize space-time, what is it that we are quantizing?

The most fundamental theories that are available to us are field theories. In a field theory, the state is described by the value of the fields at each point. At each point, the value of the field is independent from the others: the degrees of freedom form a continuum.\footnote{If we restrict ourselves to continuous fields, then we only need to specify the value of the fields on a countable dense subset (i.e. rational values of the coordinates). Though these kind of details do not change the discussion: a Lebesgue measure on the rationals have the same characteristics of the one extended to the reals.} We can therefore pose the question: how do we quantify the degrees of freedom?

We have again the same problem as before: measures on the continuum are not invariant under generic coordinate transformations. We do have a sense, however, that if we double the spatial volume, we double the number of degrees of freedom, and spatial volume is an invariant. Therefore we can use a measure of volume constructed from the metric tensor. This would be the equivalent of the Louisville measure we got from phase space, and we end up with an equivalent problem.

Since the measure is finite for finite regions and is additive, as we shrink the region, we will find regions with less than one degree of freedom. This does not make sense. If we want to quantize space, then, we need to put a unitary lower bound to the measure. But now our measure cannot be additive: what does it mean that two degrees of freedom count less than two?

Before answering that question, let's add time. We said that if we double the spatial volume, we double the number of degrees of freedom. Suppose we have a space-time region and we double the time, are we doubling the number of degrees of freedom? Well, we certainly doubling the support of the function we have to specify for the field configuration, but we wouldn't consider that doubling the degrees of freedom. Intuitively, we just think of those as being the same degrees of freedom that are getting different values. This is because we expect the future and past values to be linked by some dynamical equations, so the future values would be linked, in some way, to the past values. That is, the past and future value of the same field at some point are not independent.

So, what does it mean that two degrees of freedom count less than two? It could simply mean that they are not independent. That is, the values of the field at two distant points can be chosen arbitrarily, but as the points get sufficiently close, this would not be the case. This would make sense. Note that the lack of independence of the field values in close-by regions may mean that high frequencies of the fields are damped: the values have to become closer. The quantized quantifying measure of the degrees of freedom would effectively provide a cutoff. This may provide a natural high energy cutoff for quantum field theories.

In our view, the arguments laid here highlight an undeniable problem and seem to offer a natural step to take in line with the change from classical to quantum theory. While the development of a quantum theory of gravity is not our primary interest, the mathematical tools needed to address the above problem are of the same nature as the one we propose to develop a non-additive measure theory to quantify quantum states.

\section{Open questions and future work}

Having outlined the scope and motivation for this new approach, we now outline the main questions we think need to be addressed.

Both the counting measure and the Lebesgue/Louisville measures are unique in some sense. For the counting measure, once we define the value for a single point, typically one, the whole measure is determined. Similarly, once we have defined a real variable, the measure is determined. The Louisville requires a bit more structure, but once the canonical coordinates are chosen, the measure is determined as well. Therefore the first question is whether these quantized measure are also unique in some sense.

While we have found some literature on non-additive measures and set functions (CITE), together with modified theories of integration, these seem to concentrate on monotonic measure, that is measure that increase as the set increase. Unfortunately, the quantized measure does not have this feature. Consider the set $\{ z^+, z^- \}$. An equal mixture will return the maximally mixed state which, as we said, would correspond to a measure equal to 2. Now consider the set $\{ z^+, z^-, x^+ \}$. This will not correspond to the maximally mixed state, and therefore the measure will be less than two. The set is bigger, but the measure is smaller. Therefore we may need to develop an appropriate theory.

To achieve the previous point, we will need to understand the necessary conditions for the quantifying measure. Some may be derived from the strict concavity of entropy, given the connection between the two. Furthermore, note that the relationship between the measure over two points, the entropy of the equal mixture of two states and the probability of transition between two states are invertible. Therefore the relationship between two states defines the geometry of the whole state space. Therefore there must be further consistency requirements that can be identified.

Another thing that need to be understand is how the quantized measure behaves with composite systems. In the classical case, the measure is defined on the product of the $\sigma$-algebra. Ideally, we would like to show that this simple procedure cannot support a quantized measure, and the $\sigma$-algebra of tensor product must be taken.\footnote{Technically it would be the $\sigma$-algebra projective space coming from the tensor product of the Hilbert spaces associated with the individual systems.}

The first spaces we should study are the symplectic ones as these are the ones that allow for observer independent densities. If we want to extend to a quantization of space-time, however, Riemannian spaces need to be studies as well. However, we have found a connection between the two: if the symplectic form is expressed in position and velocity, instead of canonical coordinate, the metric tensor appears as the position-velocity component. It then can be understood as defining volumes in position-velocity instead in position-momentum. This may allow us to transfer the quantized measure on the tangent bundle and then, hopefully, to the space itself.

There is therefore much work to be done, but one thing we want to stress: a clear and precise physical idea is driving the development of possibly new math. We are not looking for mathematical extensions and trying them to see whether something useful can be achieved. We have a goal, quantification of states .

 are looking to quantify states  We are not introducing something like negative probability, we have a clear understanding of what the physical significance of the generalized measure is. The goal is the quantification of states in the contextual case. 

\section{Conclusion}

We have seen that, in physics, measure theory is not only used to characterize classical probability but also to quantify states in state spaces. Ideally, such measure would have the following three conditions:
\begin{enumerate}
	\item single states count as one
	\item finite continuous regions correspond to finite information (and therefore finite state quantification)
	\item state quantification is additive on disjoint regions.
\end{enumerate}
However, these conditions are not compatible, and only two of the three can be chosen. The classical discrete cases discards the second; the classical continuous case discards the first; the quantum case discards the third. Quantization, then, can be understood as putting a unitary lower bound to the measure that quantifies states.

The lack of additivity on the whole space makes it impossible to use classical probability on the state space as a whole. Physically, the issue is that states must be counted at-all-else-being-equal, feature broken by contextuality. But if the states do belong to the same context (e.g. they are the output of the same measurement), additivity is recovered. This tells us very directly why quantization requires contextuality, and why classical probability works within the same context. The fact that single states and and finite continuous regions have both finite measure also tells us why quantum mechanics mixes features of the classical and discrete cases.

We believe that further study and characterization of the non-additive quantized measure will not only bring more clarity in the interplay between the above concept, but it may be necessary for the development of future physical theory. Given that, in a field theory, the number of independent degrees of freedom is taken to be proportional to the spatial volume, quantization of space time can be understood as the quantization of the measure that quantifies the indipendent degrees of freedom. The development of physically motivated mathematical tools may help in that regard.

\bibliography{bibliography}

\newcommand{\pj}[1] {\underbar{$#1$}}


\end{document}
