\documentclass[10pt,twocolumn, nofootinbib]{revtex4-2}
%\documentclass[aps,pra,10pt,twocolumn,floatfix,nofootinbib]{revtex4-1}
%\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{assumptionsofphysics}
\usepackage{tikz}
\usepackage{breakurl}

%\usepackage{amsmath}
%\usepackage{mathrsfs}
%\usepackage{amsfonts}
%
%\usepackage{graphicx}
%\usepackage{hyperref}
%\hypersetup{
%	colorlinks=true,
%	citecolor=blue,
%	urlcolor=blue,
%	linkcolor=blue
%}
%\urlstyle{same}
%\frenchspacing
%
\def\>{\rangle}
\def\<{\langle}
\DeclareMathOperator{\erf}{erf}

\begin{document}

\title{How quantum mechanics requires non-additive measures}
\author{Gabriele Carcassi}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}
\author{Christine A. Aidala}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}

\date{\today}


\begin{abstract}
	Measure theory is used in physics, not just to capture classical probability, but also to quantify the number of states. In previous works, we found that state quantification plays a foundational role in classical mechanics, and therefore, we set ourselves to construct the quantum equivalent of the Liouville measure. Unlike the classical counterpart, this quantized measure is non-additive and has a unitary lower bound (i.e. no set of states can have less than one state). Conversely, requiring that state quantification is finite for finite continuous regions and that each state counts as one already implies non-additivity, which in turn implies the failure of classical theory. In this article we show these preliminary results and outline a new line of inquiry that may provide a different insight into the foundations of quantum theory. Additionally, this new approach may prove to be useful to those interested in a quantized theory of space-time, as we believe this requires a quantized measure for the quantification of the independent degrees of freedom.
\end{abstract}

\maketitle

\section{Introduction}

It is well-established that standard measure theoretic tools cannot be used to characterize quantum probability as quantum theory is a non-classical (i.e. non-Kolmogorov) probability theory. To obviate this problem, various alternatives and extensions to measure theory and probability theory have been developed with different goals.\cite{groenewold1946principles, moyal1949quantum, gleason1957measures, sorkin1994quantum, hamhalter2003quantum, gudder2009quantum, svozil2022extending, monchietti2023measure} Our interest in measure theory, however, is not in its use to capture classical probability. Measures play another crucial use in physical theories: ``counting'', or better ``quantifying'', states. It is this use that will be the focus of this work.

As part of our overall project Assumptions of Physics,\cite{aop-book} we have developed an approach, called reverse physics,\cite{aop-phys-ReversePhysics} which, starting from the laws, aims to find a minimal set of physical assumptions needed to rederive them. We found that the ability to quantify states is a crucial component of classical theory, so much so that it is enough to recover the structure of phase space and the laws of evolution.\cite{aop-phil-Hamiltonianinformation,aop-phil-HamiltonianPrivilege} The former (i.e. the symplectic structure) is the only structure that allows one to quantify states and configurations over independent degrees of freedom in a way that is frame independent. If one imposes deterministic and reversible evolution, state quantification must be preserved, leading to Hamiltonian evolution (i.e. a symplectomorphism). All elements of classical mechanics, including canonical variables, the Lagrangian and the action principle, \cite{aop-phys-ActionPrincipleInterpretation} can be rederived and understood in terms of quantifying the flow of states. Given the formal similarity between classical and quantum mechanics, we believe a similar approach is possible for the latter. This raises the question: what is the quantum analogue of the Liouville measure, the measure that quantifies states in the classical theory?

Statistical mechanics and information theory mandate a link between entropy and state quantification, as the Shannon/Gibbs entropy over uniform distributions corresponds to the logarithm of the count of states. In the quantum case, we can start with the von Neumann entropy and define a measure that has the same connection. What we find is that this measure is non-additive and that its features can help us better understand what quantization is, why it requires both contextuality and the failure of classical probability, and why the latter can be recovered separately within each context. Therefore we believe studying this object is useful not only for the above stated goals of reverse physics, but more in general to get a better understanding of the foundations of quantum mechanics.

The aim of this article is to propose and articulate the goals of this new line of research. It provides the overall picture in which future works will add further mathematical detail and physical insights. In section II we will show how measure theory is used to quantify elements in a set and how this is needed to assign probability and probability densities to each element. In section III we will see how statistical mechanics and information theory mandate a link between entropy and state quantification, which we will use to define the quantum analogue of the Liouville measure. We will find that this measure is not additive.\footnote{Technically, measures are defined to be additive, therefore what we find is not a measure, but a set function. However, this term is not widespread, so we use the oxymoron ``non-additive measure'' as it is customary in the mathematical literature. \cite{denneberg1994non,nonadditive2014}} In section IV we will see that this non-additivity is linked to non-contextuality. In section V we will see that quantization means putting a unitary lower bound on the quantification of states, while keeping a finite value over finite regions. We will see that this very directly requires non-additivity and therefore non-contextuality. In section VI we will argue that quantization of space-time means using a quantized measure on the quantification of degrees of freedom. Lastly, in section VII we will outline a series of goals for a research program that aims to fully characterize the quantized measure.

\section{Measure theory and quantifying states}

As we said in the introduction, measure theory is not only used to characterize Kolmogorovian probability. More generally, measures are used to define the size of sets. Even in probability theory, since probability measures assign probability to sets of points, in order to associate probability to individual points one needs two measures: the \textbf{probability measure} $p : \Sigma_X \to \mathbb{R}$ and another measure quantifying the extent of the set, which we call \textbf{quantifying measure} $\mu : \Sigma_X \to \mathbb{R}$.\footnote{Measures assign values to elements of a $\sigma$-algebra, which can be thought of as a set of experimental procedures. \cite{aop-book}} Since we are interested in physical theories, the measure $\mu$ quantifies the number of states. Given a set $U$, the ratio $\frac{p(U)}{\mu(U)}$ gives us the average probability per unit of states. As we shrink $U$ to a single point $x$, we will find the probability associated to that point, and how this is done depends on the nature of $\mu$. 

In the discrete case, the quantifying measure is simply the counting measure: the cardinality of the set. Therefore for a single point we simply have $\frac{p(\{x\})}{\mu(\{x\})} = \frac{p(\{x\})}{1} = p(x)$, which corresponds to the probability associated to the state $x$. The counting measure is fully determined by two properties: countable additivity, a defining property of measures, and that each state counts as one.

In the continuous case, things are a bit different.\footnote{Note that ``discrete' and ``continuous'' have different meanings depending on context. At a set theoretic level, ``continuum'' is associated with the cardinality of a set. In topology, discrete and continuous spaces are characterized by  properties of open sets, not cardinality: a manifold can be given a discrete topology. In the context of quantifying measures in physics, discrete means that we can count elements with natural numbers, while continuous means we use real valued quantities with units, such as meters for distances, to define the size of regions.} If $X$ is a manifold, a set of local coordinates induce a Lebesgue measure, which can be used to quantify the number of possible value combinations. Note that a single state has measure zero, unlike the discrete case. The Lebesgue measure is defined by two properties: countable additivity and that the size of parallelepipeds corresponds to the volume expressed by the product of the differences of coordinates (i.e. $\mu(U) = \Delta x^1 \Delta x^2 \cdots \Delta x^n$). Naturally, this is a problem in physics given that the quantification of the number of states should be the same in all frames.

However, the structure of classical phase space (i.e. symplectic structure) is exactly the structure that allows one to define a quantifying measure for states that is invariant under change of frame.\cite{aop-book,aop-phil-Hamiltonianinformation,aop-phil-HamiltonianPrivilege} The role of the quantifying measure in classical mechanics, then, is assigned to the Liouville measure, which we can understand as a Lebesgue measure on position and conjugate momentum (i.e. canonical coordinates).\footnote{The volume of an infinitesimal parallelepiped is given by $dq^1 dq^2 \cdots dp_1 dp_2 \cdots$. Since $dq^i$ is contravariant and $dp_i$ is covariant under change of frame, the volume remains the same.} As we shrink the size of the set, $\frac{p(U)}{\mu(U)}$ becomes the Radon-Nikodym derivative $\frac{dp}{d\mu}$, which gives us the probability density, not the probability itself.

%Note that, in both cases, the quantifying measure is potentially unbounded, as the state space can have potentially infinitely many states, as it is the case for one spatial degree of freedom. Therefore the quantifying measures are implicitly different from probability measures, and they require specific attention.

Quantum mechanics, instead, does not provide a clear state quantifying measure. When we characterize preparations and measurement outcomes, we may use the discrete or continuous quantifying measures in different cases for the same system. For example, consider a particle. If we are interested in a discrete observable, like the energy level of a harmonic oscillator, we would use the counting measure; if we are interested in a continuous observable, like position, we would use the Lebesgue measure associated to the variable. As for preparations, the same maximally mixed state for a qubit can be written as a discrete mixture $\frac{1}{2} | 0 \> \< 0 | + \frac{1}{2} | 1 \> \< 1 | $ or as a continuous uniform mixture over all possible states $\int \frac{1}{4\pi} | \psi \> \< \psi | d\Omega$.

To be precise, the quantifying measures we considered are really on preparations and measurements, not the states themselves. It was the discrete or continuous character of the space of preparations that selected the appropriate space with the appropriate measure. But even if we have different preparation and measurement spaces, the state space of the system should always be the same, with a single way to count states. This, in other words, highlights the problem of quantum probabilities: while the spaces of preparations and measurement outcomes are both classical, they cannot be connected by classical means. That is, we cannot simply put a classical probability of transition between the two because we would be connecting qualitatively different quantifying measures.

To sum up, the typical foundational question is: how do we generalize probability measures for quantum mechanics? We are asking something different: given that state-quantifying measures play an equally fundamental role, how do we quantify the number of states in quantum mechanics?

\section{Measure theory and quantifying states}

While the standard structure of quantum mechanics does not give us an answer to our question, statistical mechanics and quantum information theory have an implicit answer. Statistical mechanics tells us that the thermodynamic entropy of a system, which is a physically meaningful quantity that has numerous empirical consequences, corresponds to the Shannon entropy for a classical discrete system, the Gibbs entropy\footnote{By Gibbs entropy we mean the differential Shannon entropy $-\int \rho \log \rho d\mu$ calculated over phase space using position and momentum (i.e. canonical coordinates).} for a classical continuous system, and the von Neumann entropy for a quantum system.

There is another expression for entropy, often referred to as the fundamental postulate of statistical mechanics, \cite{Peliti2011} which is the logarithm of the count of microstates: $\log \mu(U)$.\footnote{Note that the Boltzmann constant is left out, as entropy will always be expressed in bits or nats. Physically, the Boltzmann constant is needed to define temperature in Kelvin. Statistical mechanics can be equivalently formulated in terms of $\beta = \frac{1}{k_B T}$.\cite{chyla2011evolution}} In classical statistical mechanics, the measure is indeed the quantifying measure discussed above: the counting measure for the discrete case and the Liouville volume in the continuous case. The two expressions for entropy are linked as the Shannon/Gibbs entropy over a uniform distribution matches the logarithm of the measure of the support. That is, if $\rho_U$ is a uniform distribution over $U$, then $S(\rho_U) = \log \mu(U)$. In quantum mechanics, as we said before, we do not have a measure to quantify states. There is, however, a somewhat related expression: the maximum entropy attainable by a quantum system corresponding to a Hilbert space $\mathcal{H}$ is the logarithm of the dimensionality $\dim_{\mathbb{C}}(\mathcal{H})$ of the space. Therefore, $ S(\rho_U) \leq \log (\dim_{\mathbb{C}}(\text{span}(U)))$.

A similar connection can be established in information theory. The Shannon/Gibbs/von Neumann entropy quantifies the minimum number of bits required on average to send a message from a given source.\footnote{For a more general characterization of entropy that works in statistical mechanics, information theory and other contexts, and a more precise discussion of the continuous case, see \cite{aop-phys-variability}.} This, again, is a quantity that leads to empirical consequences. If we have $n$ bits at our disposal, we have $2^n$ possible combinations, possible messages, that can be encoded. Therefore the logarithm of the number of potential messages corresponds to the number of bits that can encode those messages. Physically, we can associate a different state to each message to be encoded. Therefore $\log \mu(U)$ corresponds to the number of bits. This will exactly correspond to the number of bits required by a source characterized by a uniform probability distribution over that number of symbols.

While the above discussion does not cover all technical details, the only point we want to make is that the link $S(\rho_U) = \log \mu(U)$ is fully justified by both statistical mechanics and information theory. It is not a capricious request, but rather a necessary requirement stemming not from just one, but from two disciplines that have empirical consequences. Therefore if quantum mechanics does not directly provide us with a measure to quantify states, we can use what it does provide, the von Neumann entropy, to define and calculate said measure.

We can start by calculating the simplest case, which is when $U = \{ \psi \}$ is a single state. In this case, we have $S(|\psi\>\<\psi|) = 0$ and therefore $\mu(\{\psi\}) = 2^{0} = 1$. The measure returns one for each state as in the classical discrete case.

The next case is when $U = \{ \psi, \phi \}$ is the set of two states. A uniform distribution over two states will correspond to the mixed state
\begin{equation}
	\rho = \frac{1}{2} |\psi\>\<\psi| + \frac{1}{2} |\phi\>\<\phi|.
\end{equation}
The entropy of said state will depend on the probability $p=|\<\psi | \phi \>|^2$ in the following way
\begin{equation}
	S(\rho) = - \frac{1+\sqrt{p}}{2} \log \frac{1+\sqrt{p}}{2} 
	- \frac{1-\sqrt{p}}{2} \log \frac{1-\sqrt{p}}{2}.
\end{equation}
If the states are orthogonal, then $p=0$, and $S(\rho) = 1$ therefore $\mu(U) = 2^1 = 2$. In this case we have that $2 = \mu(\{ \psi, \phi \}) = \mu(\{\psi\}) + \mu(\{\phi\}) = 1 + 1$, and therefore $\mu$ is additive. Moreover, if $U$ is an orthonormal basis, the von Neumann entropy $S(U) = \log(\dim_{\mathbb{C}}(\text{span}(U))) = \log |U|$ is the logarithm of the cardinality of $U$. Therefore the quantifying measure $\mu(U) = |U|$ is equal to the counting measure. But this is a special case that corresponds to the highest entropy reachable with a mixture. In all other cases the entropy will be lower, and therefore, in general, $\mu(\{ \psi, \phi \}) \leq \mu(\{\psi\}) + \mu(\{\phi\})$. In other words, the measure $\mu$ is not additive.

To sum up, the physically motivated way to quantify the number of states in quantum mechanics is, in general, not additive. When counting states in quantum mechanics, $1+1 \leq 2$.

\section{Non-additivity and contextuality}

While unusual claims are common in quantum mechanics, we want to have a clear, physically tenable, reason as to why the quantifying measure for quantum states must be non-additive. In the context of a spin 1/2 system, for example, why is it that $\{ |z^+\>, |z^-\>\}$ count as two states while $\{ |z^+\>, |x^+\> \}$ count as less than two?

Since we saw that the quantifying measure is indeed additive for a set of orthogonal states, this should make us think about contextuality: the impossibility to assign outcomes to all potential measurements of a quantum system. We have, in fact, derived that it is impossible to define an additive quantifying measure on all potential states, but we can do so on the outcomes of one measurement. The ability/inability to define additive measures is exactly the difference between classical/non-classical probability.

While contextuality is often presented as a somewhat abstract, almost metaphysical, property of quantum mechanics, we have a rather more down-to-earth view. The choice of the measurement means a difference in physical process of either the preparation or measurement device. Choosing which direction of spin to prepare or measure means, at some point, orienting some polarizer, magnetic field or making an equivalent change to a physical device. Something is changed in the physical world, not just in the system, but around the system, in its environment. If Amanda claims that a particle is in the $|z^+\>$ state, then Boris is able to infer something about Amanda's preparation/measuring device.\footnote{This is not at all particular to quantum mechanics. We can study the motion of a cannonball here on earth, but not on the surface of the sun, as the cannonball would be immediately vaporized. The mere claim that a system exists implies that the boundary conditions necessary for that system to exist are satisfied. In this sense, all physics is contextual.} Therefore we can understand the context simply as those conditions on the environment that are necessary for the system to be found in that particular state. A set of orthogonal states, then, has the property that they can be found in the same external conditions.

Given that we have, in general, a correlation between system and environment, it is natural to understand how contextuality leads to a non-additive measure. Two states, in fact, should be counted as two only if they lead to a change in the system alone. That is, \textbf{states should be quantified all-else-being-equal}. While two states from different contexts are indeed two distinct states, stating we transitioned from one to the other does not give us information only about the system: it tells us something about the environment as well. The degree of incompatibility of observables, then, ultimately measures the degree of change when realizing the context physically.

So we have a clear, physically motivated reason as to why contextuality is linked to a non-additive measure. States should be quantified at-all-else-being-equal, meaning that possible correlations with the environment have to be removed.

\section{Non-additivity and quantization}

While we have clarified what it means for the quantifying measure to be non-additive, and how this is tied to non-contextuality, we haven't shown why it should be linked to quantization. In fact, what exactly is quantization?

Before trying to answer that question, let's make a direct comparison between the quantifying measures for the three cases: the counting measure $\mu_d$ for the classical discrete case, the Liouville measure $\mu_c$ for the classical continuous case and the newly defined quantized measure $\mu_q$ for the quantum case. This requires a space where all three can be defined, which fortunately exists: the surface of a sphere. First, we want to compare the behavior on a single point. The counting measure returns one, and so does the quantum measure. The Liouville measure, however, returns zero. Second, we want to compare the behavior on a finite region, an open set. The Liouville measure returns the solid angle, which is always going to be finite as it must be less than or equal to $4\pi$. The von Neumann entropy of a mixed state over a two-state system is between zero and one, and therefore the measure must be between one and two. However, given that an open set on a sphere has infinitely many points, the counting measure over a finite region will be infinite. Lastly, we want to compare the additivity. Both classical measures are additive, while the quantum measure is not.

Let us list the three properties we have mentioned:
\begin{enumerate}
	\item single states count as one
	\item finite continuous regions correspond to finite state quantification
	\item state quantification is additive on disjoint regions.
\end{enumerate}
These seem very reasonable properties to assume for a quantifying measure of states. \textbf{However, these three conditions are incompatible}. If each state counts as one, and the measure is additive, then a finite continuous region must be infinite as it contains infinite points. \textbf{Of the three conditions, we can only pick two}. This is, in fact, what each measure does. The counting measure rejects the second property. The Liouville (and Lebesgue) measure discards the first. The quantum measure sacrifices the third.

So, in light of this, what is quantization? If we have a system characterized by continuous quantities, such as position, the state space will necessarily be a continuous space and finite regions will have to correspond to finite information as those correspond to the finite precision measurements we can make.\footnote{For a treatment of how finite-time experimental verification is captured by topological concepts, and what necessary and sufficient operational assumptions are required for the use of real valued quantities, see \cite{kelly1996,aop-book}.} If the regions are large enough, the Liouville (or Lebesgue) measure will work. But as we shrink a region we will encounter, given the additivity, regions that contain less than one state, which does not make sense. Note that measure less than one corresponds to negative thermodynamic entropy, which does not make sense either.

Quantization, then, is fixing the quantifying measure so that it is bounded from below by one (and the entropy by zero): each state counts as one. However, this comes at a price: non-additivity. This non-additivity is ultimately why classical probability has to fail as well, given that additivity is one of its axioms. It is also the same reason as why quantization implies contextuality: the non-additivity means that some states cannot be prepared at-all-else-being-equal.

In our view, this provides a good conceptual basis to understand the failure of classical mechanics, the need for the departure from classical ideas and the need for quantum theory. We believe this, by itself, is already sufficient reason to start a research program to explore these ideas fully. However, since most physicists do not seem to be interested in this type of ``clean-up work'', we can easily argue that a full characterization of the quantized measure is critical for the development of future physical theories.

\section{Implication for space-time quantization}

We have seen that quantization can be understood as putting a unitary lower bound on the state quantification: no set of states can have fewer than one state. If we were to quantize space-time, what is it that we are quantizing?

The most fundamental theories that are available to us are field theories. In a field theory, the state is described by the value of the fields at each point in space. At each point, the value of the field is independent from the others: the degrees of freedom form a continuum.\footnote{If we restrict ourselves to continuous fields, then we only need to specify the value of the fields on a countable dense subset (i.e. rational values of the coordinates). However, these details do not change the discussion: a Lebesgue measure on the rationals has the same characteristics as the one extended to the reals.} We can therefore pose the question: how do we quantify the degrees of freedom?

If we have a region of space and we double the volume, we can imagine we are also doubling the number of degrees of freedom. Therefore, we can construct a volume measure from the metric tensor, which would give us an invariant volume that quantifies degrees of freedom, much in the same way that the Liouville measure quantified states.

Since the measure is finite for finite regions and is additive, as we shrink the region, we will find regions with less than one degree of freedom. This does not make sense. As before, we need to put a unitary lower bound on the measure and our measure cannot be additive. We need a quantized measure. Quantizing space-time, then, means putting a quantized measure on the number of degrees of freedom.

What does it mean that two degrees of freedom count less than two? Before answering that question, let's consider time. We said that if we double the spatial volume, we double the number of degrees of freedom. Suppose we have a space-time region and we double the time, are we doubling the number of degrees of freedom? Well, we are certainly doubling the support of the function we have to specify for the field configuration, but we wouldn't consider that doubling the degrees of freedom. Intuitively, we just think of those as being the same degrees of freedom that are getting different values in time. This is because we expect the future and past values to be linked by some dynamical equations, so the future values would be linked, in some way, to the past values. That is, the past and future value of the same field at some point are not independent. This may give us an insight into the physical meaning of the non-additivity of quantification of the number of degrees of freedom.

The same field at two spatial points may count less than two degrees of freedom simply because their values are not independent. The values of the field at two distant points can be chosen arbitrarily, therefore the measure would be additive at long range. But as the points get sufficiently close, this would not be the case, so at short scale we need a quantized measure to quantify independent degrees of freedom. Note that the lack of independence of the field values in close-by regions may mean that high frequencies of the fields are damped: the values have to become closer. A quantized measure of the degrees of freedom would effectively provide a cutoff. Therefore we have a story that is very well motivated and provides a possible solution for high energy divergences in quantum field theories.

While the development of a quantum theory of gravity is not our primary interest, the mathematical tools required to characterize the non-additive quantized measure seem to be of similar nature as the ones that may be used to quantize space-time.

\section{Open questions and future work}

Having outlined the scope and motivation for this new approach, we now outline the main questions we think need to be addressed by this new research program.

Both the counting measure and the Lebesgue/Liouville measures are unique in some sense. For the counting measure, once we define the value for a single point, typically one, the whole measure is determined. Similarly, once we have defined a real variable, the Lebesgue measure is determined. The Liouville measure requires a bit more structure, but once the canonical coordinates are chosen, the measure is determined as well. Therefore the first question is whether these quantized measures are also unique in some sense.

While we have found some literature on non-additive measures and set functions \cite{hamhalter2003quantum, nonadditive2014}, together with modified theories of integration, these seem to concentrate on monotonic measures: measures that increase as the set increases. Unfortunately, the quantized measure does not have this feature. Consider the set $\{ |z^+\>, |z^-\> \}$. An equal mixture will return the maximally mixed state which, as we said, would correspond to a measure equal to 2. Now consider the set $\{ |z^+\>, |z^-\>, |x^+\> \}$. An equal mixture of them will not correspond to the maximally mixed state, and therefore the measure will be less than two. The set is bigger, but the measure is smaller. Therefore there may be new mathematical ground to be explored.

We may want to understand the necessary conditions for a quantifying measure. Some may be derived from the strict concavity of entropy, given the connection between the two. Furthermore, note that the pairwise relationships between the measure over two points, the entropy of the equal mixture of two states and the probability of transition between two states are all invertible. This means that the relationship between two states defines the geometry of the whole state space. Therefore there must be further consistency requirements that can be identified.

Another issue that needs to be understood is how the quantized measure behaves in composite systems and across multiple degrees of freedom. In the classical case, the measure is defined on the product of the corresponding $\sigma$-algebras. Ideally, we would like to show that this simple procedure cannot support a quantized measure, and the $\sigma$-algebra of the tensor product must be taken.\footnote{Technically it would be the $\sigma$-algebra projective space coming from the tensor product of the Hilbert spaces associated with the individual systems.}

The first spaces we should study are the symplectic ones as these are the ones that allow for frame independent densities. If we want to extend to a quantization of space-time, however, Riemannian spaces need to be studied as well. However, we have found a connection between the two: if the symplectic form is expressed in position and velocity, instead of canonical coordinates, its position-velocity component corresponds to the metric tensor. The metric tensor can then be understood as defining volumes in position-velocity instead in position-momentum. This may allow us to transfer the quantized measure onto the tangent bundle and then, hopefully, to the space itself.

Finally, we want to stress that, contrary to what happens in other programs, the physical motivations for the mathematical generalization are tightly driven by the physics. We are not introducing concepts that, like negative probability, have no clear direct physical meaning. The goal is the quantification of states.

\section{Conclusion}

We have seen that, in physics, measure theory is not only used to characterize classical probability but also to quantify states in state spaces. Ideally, such a measure would have the following three conditions:
\begin{enumerate}
	\item single states count as one
	\item finite continuous regions correspond to finite state quantification
	\item state quantification is additive on disjoint regions.
\end{enumerate}
However, these conditions are not compatible, and only two of the three can be chosen. The classical discrete case discards the second; the classical continuous case discards the first; the quantum case discards the third. Quantization, then, can be understood as putting a unitary lower bound on the measure that quantifies states.

The lack of additivity on the whole space makes it impossible to use classical probability on the state space as a whole. Physically, the issue is that states must be counted at-all-else-being-equal, a feature broken by contextuality. But if the states do belong to the same context (e.g. they are the output of the same measurement), additivity is recovered. This tells us very directly why quantization requires contextuality, and why classical probability works within the same context. The fact that single states and finite continuous regions both have finite measure also tells us why quantum mechanics mixes features of the classical discrete and continuous cases.

We believe that further study and characterization of the non-additive quantized measure will not only bring more clarity to the interplay among the above concepts, but it may be necessary for the development of future physical theories. Given that, in a field theory, the number of independent degrees of freedom is taken to be proportional to the spatial volume, quantization of space-time can be understood as the quantization of the measure that quantifies the independent degrees of freedom. The development of physically motivated mathematical tools may help in that regard.

\section*{Acknowledgments}
This paper is part of the ongoing \textit{Assumptions of Physics} project \cite{aop-book}, which aims to identify a handful of physical principles from which the basic laws can be rigorously derived. This article was made possible through the support of grant \#62847 from the John Templeton Foundation.


\bibliography{bibliography}

\newcommand{\pj}[1] {\underbar{$#1$}}


\end{document}
