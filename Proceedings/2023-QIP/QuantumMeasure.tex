\documentclass[10pt,twocolumn, nofootinbib]{revtex4-2}
%\documentclass[aps,pra,10pt,twocolumn,floatfix,nofootinbib]{revtex4-1}
%\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{assumptionsofphysics}
\usepackage{tikz}
\usepackage{breakurl}

%\usepackage{amsmath}
%\usepackage{mathrsfs}
%\usepackage{amsfonts}
%
%\usepackage{graphicx}
%\usepackage{hyperref}
%\hypersetup{
%	colorlinks=true,
%	citecolor=blue,
%	urlcolor=blue,
%	linkcolor=blue
%}
%\urlstyle{same}
%\frenchspacing
%
\def\>{\rangle}
\def\<{\langle}
\DeclareMathOperator{\erf}{erf}

\begin{document}

\title{How quantum mechanics requires non-additive measures}
\author{Gabriele Carcassi}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}
\author{Christine A. Aidala}
\affiliation{Physics Department, University of Michigan, Ann Arbor, MI 48109}

\date{\today}


\begin{abstract}
\end{abstract}

\maketitle

\section{Introduction}

It is well-established that standard measure theoretic tools cannot be used to characterize quantum probability as quantum theory is a non-classical (i.e. non-Kolmogorov) probability theory. To obviate this problem, various alternatives and extensions to measure theory and probability theory have been developed with different goals. Our interest in measure theory, however, is not in its use to capture classical probability. Measures play another crucial use in physical theories: ``counting'', or better ``quantifying'', states. It is this use that will be the focus of this work.

As part of our overall project Assumptions of Physics, we have developed an approach, called reverse physics, which, starting from the laws, aims to find a minimal set of physical assumptions needed to rederive the laws. We found that the ability to quantify states is a crucial component of classical theory, so much so that it is enough to recover the structure of phase space and the laws of evolution. The former (i.e. the symplectic structure) is the only structure that allows to quantify states and configurations over independent degrees of freedom in a way that is observer independent. If one imposes deterministic and reversible evolution, state quantification must be preserved, leading to Hamiltonian evolution (i.e. a symplectomorphism). All elements of classical mechanics, including canonical variables, the Lagrangian and the action principle, can be rederived and understood in terms of quantifying the flow of states. Given the formal similarity between classical and quantum mechanics, we believe a similar approach is possible for the latter. Which raises the question: what is the quantum analogue of the Louisville measure, the measure that quantifies states in the classical theory?

Statistical mechanics and information theory mandate a link between entropy and state quantification, as the Shannon/Gibbs entropy over uniform distributions corresponds to the logarithm of the count of states. In the quantum case, we can start with the von Neumann entropy and define a measure that has the same connection. What we find is that this measure is non-additive and that its features can help us better understand what quantization is, why it requires both contextuality and the failure of classical probability, and why the latter can be recovered separately within each context. Therefore we believe studying this object is useful not only for the above stated goals of reverse physics, but more in general to get a better understanding of the foundations of quantum mechanics.

The goal of this article is to propose and articulate the goals of this new line of research. It provides the overall picture in which future works will add further mathematical detail and physical insights. In section II we will show how measure theory is used to quantify elements in a set and how this is needed to assign probability and probability densities to each element. In section III we will see how statistical mechanics and information theory mandate a link between entropy and state quantification, which we will use to define the quantum analogue of the Louisville measure. We will find that this measure is not additive.\footnote{Technically, measures are defined to be additive, therefore what we find is not a measure, but a set function. However, this term is not widespread, so we use the oxymoron ``non-additive measure'' as it is customary in the mathematical literature (TODO: cite a couple of books on non-additive measure theory).} In section IV we will see that this non-additivity is linked to non-contextuality. In section V we will see that quantization means putting a lower unitary bound on the quantification of states, while keeping a finite value over finite regions. We will see that this very directly requires non-additivity and therefore non-contextuality. In section VI we will argue that quantization of space-time means using a quantized measure on the quantification of degrees of freedom. Lastly, in section VII we will outline a series of goals for a research program that aims to fully characterized the quantized measure.

\section{Measure theory and quantifying states}

As we said in the introduction, measure theory is not only used to characterize Kolmogorovian probability. More in general, measures are used to define the size of sets. Even in probability theory, since probability measures assigns probability to sets of point, to associate probability to points one needs two measures: the \textbf{probability measure} $p : \Sigma_X \to \mathbb{R}$ and another measure quantifying the extent of the set, which we call \textbf{quantifying measure} $\mu : \Sigma_X \to \mathbb{R}$.\footnote{Measure assigns a value to elements to a $\sigma$-algebra that can be thought as a set of experimental procedures.} Since we are interested in physical theories, the measure $\mu$ quantifies the number of states. Given a set $U$, the ratio $\frac{p(U)}{\mu(U)}$ given us the average probability per unit size. As we shrink $U$ to a single point $x$, we will find the probability associated to that point, and how this is done depends on the nature of $\mu$. 

In the discrete case, the quantifying measure is simply the counting measure: the cardinality of the set. Therefore for a single point we simply have $\frac{p(\{x\})}{\mu(\{x\})} = \frac{p(\{x\})}{1} = p(x)$, which corresponds to the probability associated to the state $x$. The counting measure is fully determined by two properties: countable additivity, a defining property of measures, and that each states counts as one.

In the continuous case, things are a bit different.\footnote{Note that ``discrete' and ``continuous'' have different meaning depending on context. At a set theoretic level, continuum is associated with the cardinality of a set. In topology, discrete and continuous spaces are characterized by  properties of open sets, not cardinality: a manifold can be given a discrete topology. In the context of quantifying measures in physics, discrete means that we can count elements with natural numbers, while continuous means we use real valued quantities with units, such as meters for distances, to define the size of regions.} If $X$ is a manifold, a set of local coordinates induce a Lebesgue measure, which can be used to quantify the number of possible value combinations. Note that a single state has measure zero, unlike the discrete case. The Lebesgue measure is defined by two properties: countable additivity and that the size of parallelepipeds corresponds to the volume expressed by the product of the difference of coordinates. Naturally, this is a problem in physics given that the quantification of the number of states should be the same for all observers.

However, the structure of classical phase space (i.e. symplectic structure) is exactly the structure that allows to define a quantifying measure for states that is invariant under change of observers.(TODO cite book, two phil papers) The role of quantifying measure in classical mechanics, then, is assigned to the Louisville measure, which we can understand as a Lebesgue measure on position and conjugate momentum (i.e. canonical coordinates).\footnote{The volume of an infinitesimal parallelepiped is given by $dq^1 dq^2 \cdots dp_1 dp_2$. Since $dq^i$ is controvariant and $dp_i$ is covariant under change of reference, the volume remains the same.} The amount of probability for each state is given by the Radon-Nikodym derivative $\frac{dp}{d\mu}$, which gives us the probability density, not a the probability itself.

%Note that, in both cases, the quantifying measure is potentially unbounded, as the state space can have potentially infinitely many states, as it is the case for one spatial degree of freedom. Therefore the quantifying measures are implicitly different from probability measures, and they require specific attention.

Quantum mechanics, instead, does not provide a clear state quantifying measure. When we characterizing preparations and measurement outcomes, we may use the discrete or continuous quantifying measures in different cases. For example, consider a particle. If we are interested in a discrete observable, like the energy level of a harmonic oscillator, we would use the counting measure; if we are interested in a continuous observable, like position, we would use the Lebesgue measure associated to the variable. As for preparations, the same maximally mixed state for a qubit can be written as discrete mixture $\frac{1}{2} | 0 \> \< 0 | + \frac{1}{2} | 1 \> \< 1 | $ or as a continuous uniform mixture over all possible states $\int_\mathcal{H} \frac{1}{4\pi} | \psi \> \< \psi | d\Omega$.

To be precise, the quantifying measures we considered are really on preparations and measurements, not the states themselves. It was the discrete or continuous character of the space of preparations that selected the appropriate space with the appropriate measure. But we can have different preparations and measurement spaces, the state space of system should always be the same, with always the same way to count states! This, in other words, highlights the problem of quantum probabilities: while the space of preparations and measurement outcomes are both classical, they cannot be connected by classical means. That is, we cannot simply put a classical probability of transition between the two because we are possibly connecting rather different quantifying measures.

To sum up, the typical foundational question is: how do we generalize probability measures for quantum mechanics? We are asking something different: given that state-quantifying measures play an equally fundamental role, how do we quantify the number of states in quantum mechanics?

\section{Measure theory and quantifying states}

While the standard structure of quantum mechanics does not give us an answer to our question, statistical mechanics and quantum information theory have an implicit answer. Statistical mechanics tells us that the thermodynamic entropy of a system, which is a physically meaningful quantity that has numerous empirical consequences, corresponds to the Shannon entropy for a classical discrete system, the Gibbs entropy\footnote{By Gibbs entropy we mean the differential Shannon entropy $-\int \rho \log \rho d\mu$ calculated over phase space using position and momentum (i.e. canonical coordinates).} for a classical continuous system, and the von Neumann entropy for a quantum system.

There is another expression for entropy, often referred to as the fundamental postulate of statistical mechanics, which is the logarithm of the count of microstates: $\log \mu(U)$.\footnote{Note that the Boltzmann constant is left out, as we will be always measured in bits or nats. Physically, the Boltzmann constant is needed to define temperature in Kelvin. Statistical mechanics can be equivalently formulated in terms of beta $\beta = \frac{1}{k_B T}$.TODO: cite the unit definition paper} In classical mechanics, the measure is indeed the quantifying measure discussed above: the counting measure for the discrete case and the Louisville volume in the continuous case. The two expressions for entropy are linked as the Shannon/Gibbs entropy over a uniform distribution matches the logarithm of the measure of the support. That is, if $\rho_U$ is a uniform distribution over $U$, then $S(\rho_U) = \log \mu(U)$. In quantum mechanics, as we said before, we do not have a measure to quantify states. There is, however, a somewhat related expression: the maximum entropy attainable by a quantum system described by a Hilbert space $\mathcal{H}$ is the dimensionality $\dim_{\mathbb{C}}(\mathcal{H})$ of the space. Therefore, $ S(\rho_U) \leq \dim_{\mathbb{C}}(\text{span}(U))$

A similar connection can be established in information theory. The Shannon/Gibbs/von Neumann entropy quantifies the minimum number of bits required on average to send a message from a given source.\footnote{For a more general characterization of entropy that works in statistical mechanics, information theory and other contexts, and a more precise discussion of the continuous case, see TODO: cite our paper.} This, again, is a quantity that leads to empirical consequences. If we have $n$ bits at our disposal, we have $2^n$ possible combination, possible messages, that can be encoded. Therefore the logarithm of the number of available messages corresponds to the number of bits that can be encoded using those messages. Physically, we can use different states to encode different messages, therefore $\log \mu(U)$ tells us the number of bits that can be encoded using the states in that region. This will exactly correspond to the number of bits required by a source characterized by a uniform probability distribution over that number of symbols.

While the above discussion does not cover all technical details, the only point we want to make is that the link $S(\rho_U) = \log \mu(U)$ is fully justified by both statistical mechanics and information theory. It is not a capricious request, but rather a necessary requirement stemming not from just one, but from two disciplines that have empirical consequences. Therefore if quantum mechanics does not directly provide us with a measure to quantify states, we can use what it does provide, the von Neumann entropy, to define and calculate said measure.

We can start by calculating the simplest case, which is when $U = \{ \psi \}$ is a single state. In this case, we have $S(|\psi\>\<\psi|) = 0$ and therefore $\mu(\{\psi\}) = 2^{0} = 1$. The measure returns one for each state as in the classical discrete case.

The next case is when $U = \{ \psi, \phi \}$ is the set of two states. A uniform distribution over two states will correspond to the mixed state
\begin{equation}
	\rho = \frac{1}{2} |\psi\>\<\psi| + \frac{1}{2} |\phi\>\<\phi|.
\end{equation}
The entropy of said state will depend on the probability $p=|\<\psi | \phi \>|^2$ in the following way
\begin{equation}
	S(\rho) = - \frac{1+\sqrt{p}}{2} \log \frac{1+\sqrt{p}}{2} 
	- \frac{1-\sqrt{p}}{2} \log \frac{1-\sqrt{p}}{2}.
\end{equation}
If the states are orthogonal, then $p=0$, and $S(\rho) = 1$ therefore $\mu(U) = 2^1 = 2$. In this case we have that $2 = \mu(\{ \psi, \phi \}) = \mu(\{\psi\}) + \mu(\{\phi\}) = 1 + 1$, and therefore $\mu$ is additive. Moreover, if $U$ is an orthonormal basis, the von Neumann entropy $S(U) = \dim_{\mathbb{C}}(\text{span}(U)) = \log |U|$ is the logarithm of the cardinality of $U$. Therefore the quantifying measure $\mu(U) = |U|$ is equal to the counting measure. But this is a special case that corresponds to the highest entropy reachable with a mixture. In all other cases the entropy will be lower, and therefore, in general, $\mu(\{ \psi, \phi \}) \leq \mu(\{\psi\}) + \mu(\{\phi\})$. In other words, the measure $\mu$ is not additive.

To sum up, the physically motivated way to quantify the number of states in quantum mechanics is, in general, not additive. When counting states in quantum mechanics, $1+1 \leq 2$.

\section{Non-additivity and contextuality}

While unusual claims are common in quantum mechanics, we want to have a clear, physically tenable, reason as to why the quantifying measure for quantum states must be non-additive. In the context of a spin 1/2 system, for example, why is it that $\{ |z^+\>, |z^-\>\}$ count as two states while $\{ |z^+\>, |x^+\> \}$ count as less than two?

Since we saw that the quantifying measure is indeed additive for a set of orthogonal states, this should make us think about contextuality: the impossibility to assign outcomes to all potential measurement of a quantum system. We have, in fact, derived that it is impossible to define an additive quantifying measure on all potential states, but we can do so on the outcomes of one measurement. The ability/inability to define additive measures is exactly the difference between classical/non-classical probability. We are definitely onto something.

While contextuality is often presented as a somewhat abstract, almost metaphysical, property of quantum mechanics, we have a rather more down-to-earth view. The selection of the measurement means a difference in physical process of either the preparation or measurement device. Choosing which direction of spin to prepare or measure means, at some point, orienting some polarizer, magnetic field or equivalent change to a physical device. Something is changed in the physical world, not just on the system, but around the system, on its environment. If Amanda claims that a particle is in the $|z^+\>$ state, then Boris is able to infer something about Amanda's preparation/measuring device.\footnote{This is not at all particular to quantum mechanics. We can study the motion of a cannonball here on earth, but not on the surface of the sun, as the cannonball would be immediately vaporized. The mere claim that a system exists implies that the boundary conditions necessary for that system to exist are satisfies. In this sense, all physics is contextual.} Therefore we can understand the context simply as those conditions on the environment that are necessary on the system to be found in that particular state. A set of orthogonal states, then, has the property that they can be found in the same external conditions.

Given that we have, in general, a correlation between system and environment, it is natural to understand why the contextuality leads to a non-additive measure. Two states, in fact, should be counted as two only if they lead to a change in the system alone. That is, \textbf{states should be quantified all-else-being-equal}. While two states from different contexts are indeed two distinct states, stating we transitioned from one to the other does not give us information only about the system: it tells us something about the environment as well. The degree of incompatibility of observables, then, ultimately measure the degree of change when realizing the context physically.

So we have a clear, physically motivated reason as to why contextuality is linked to a non-additive measure. States should be quantified at-all-else-being-equal, meaning that possible correlations with the environment have to be removed.

\section{Non-additivity and quantization}

While we have clarified what it means for the quantifying measure to be non-additive, and how this is tied to non-contextuality, we haven't shown why it should be linked to quantization. In fact, what exactly is quantization?

Before trying to answer that question, let's make a direct comparison between the quantifying measures for the three cases: the counting measure $\mu_d$ for the classical discrete case, the Louisville measure $\mu_c$ for the classical continuous case and the newly defined quantized measure $\mu_q$ for the quantum case. This requires a space where all three can be defined, which fortunately exists: the surface of a sphere. First, we want to compare the behavior on a single point. The counting measure returns one, and so does the quantum measure. The Louisville measure, however, returns zero. Second, we want to compare the behavior on a finite region, an open set. The Louisville measure returns the solid angle, which is always going to be finite as it must be less than or equal to $4\pi$. The von Neuman entropy of a mixed state over a two state system is between zero and one, and therefore the measure must be between one and $2$. However, given that an open set on a sphere has infinitely many point, the counting measure over a finite region will be infinite. Lastly, we want to compare the additivity. Both classical measures are additive, while the quantum measure is not.

Let us list the three properties we have mentioned:
\begin{enumerate}
	\item single states count as one
	\item finite continuous regions correspond to finite information (and therefore finite state quantification)
	\item state quantification is additive on disjoint regions.
\end{enumerate}
These seem very reasonable properties to assume for a quantifying measure of states. \textbf{Unfortunately, these three conditions are incompatible}. If each state counts as one, and the measure is additive, then a finite continuous region must be infinite as it contains infinite points. \textbf{Of the three conditions, we can only pick two}. This is, in fact, what each measure does. The counting measure rejects the second property. The Louisville (and Lebesgue) measure discards the first. The quantum measure sacrifices the third.

So, in the light of this, what is quantization? If we have a system characterized by continuous quantities, such as position, the state space will necessarily be a continuous space and finite regions will have to correspond to finite information as those correspond to the finite precision measurements we can make.\footnote{For a treatment of how finite-time experimental verification is captured by topological concepts, and what necessary and sufficient operational assumptions are required for the use of real valued quantities, see TODO libro} If the regions are large enough, the Louisville (or Lebesgue) measure will work. But as we shrink a region we will encounter, given the additivity, regions that contain less than one state, which does not make sense. Low value of the quantifying measure correspond to low values of entropy and measure one corresponds do zero entropy. Regions with measure less than one correspond to negative thermodynamic entropy, which also does not make sense. In fact, classical statistical mechanics does fail in those regimes: the classical equations of state for an ideal gas to predict negative entropy, which is fixed by the Fermi and Bose statistics.

Quantization, then, is fixing the counting measure so that it is bounded from below by one (and the entropy by zero): each state counts as one. However, this comes at a price: non-additivity. This non-addivity is ultimately why classical probability has to fail as well, given that additivity is one of the axioms. It is also the same reason as why quantization implies contextuality: the non-addivity means that some states cannot be prepared at-all-else-being equal.
%(https://en.wikipedia.org/wiki/Sackur%E2%80%93Tetrode_equation)

In our view, this provides good conceptual reasons to understand the failure of classical mechanics, the need for the departure from classical ideas and the need for quantum theory. We believe this, by itself, is already sufficient reason to start a research program to explore these ideas fully. However, since most physicists do not seem to be interested in this type of ``clean-up work'', we can easily argue that a full characterization of the quantized measure is critical for the development of future physical theories.

\section{Implication for space-time quantization}

We have seen that quantization can be understood as putting a lower unitary bound on the state quanfication: no set of states can have fever than one state. If we were to quantize space-time, what is it that we are quantizing?

The most fundamental theories that are available to us are field theories. In a field theory, the state is described by the value of the fields at each point in space. At each point, the value of the field is independent from the others: the degrees of freedom form a continuum.\footnote{If we restrict ourselves to continuous fields, then we only need to specify the value of the fields on a countable dense subset (i.e. rational values of the coordinates). Though these kind of details do not change the discussion: a Lebesgue measure on the rationals have the same characteristics of the one extended to the reals.} We can therefore pose the question: how do we quantify the degrees of freedom?

If we have a region of space and we double the volume, we can imagine we are also doubling the number of degrees of freedom. Therefore, we can construct a volume measure from the metric tensor, which would give us an invariant volume that quantifies degrees of freedom, much in the same way that the Louisville measure quantified states.

Since the measure is finite for finite regions and is additive, as we shrink the region, we will find regions with less than one degree of freedom. This does not make sense. As before, we need to put a unitary lower bound to the measure and our measure cannot be additive. We need a quantized measure. Quantizing space-time, means putting a quantized measure on the number of degrees of freedom.

What does it mean that two degrees of freedom count less than two? Before answering that question, let's consider time. We said that if we double the spatial volume, we double the number of degrees of freedom. Suppose we have a space-time region and we double the time, are we doubling the number of degrees of freedom? Well, we certainly doubling the support of the function we have to specify for the field configuration, but we wouldn't consider that doubling the degrees of freedom. Intuitively, we just think of those as being the same degrees of freedom that are getting different values. This is because we expect the future and past values to be linked by some dynamical equations, so the future values would be linked, in some way, to the past values. That is, the past and future value of the same field at some point are not independent.

The same field at two points, then, may count less than to degrees of freedom simply because they are not independent. The values of the field at two distant points can be chosen arbitrarily, therefore the measure would be additive at long range. But as the points get sufficiently close, this would not be the case, so at short scale we need a quantized measure to quantify independent degrees of freedom. Note that the lack of independence of the field values in close-by regions may mean that high frequencies of the fields are damped: the values have to become closer. A quantized measure of the degrees of freedom would effectively provide a cutoff. Therefore we have a story that is very well motivated and provide a possible solution for high energy divergences in quantum field theories.

While the development of a quantum theory of gravity is not our primary interest, the mathematical tools required to characterize the non-additive quantized measure seem to be of similar nature as the ones that may be used to quantize space-time.

\section{Open questions and future work}

Having outlined the scope and motivation for this new approach, we now outline the main questions we think need to be addressed by this new research program.

Both the counting measure and the Lebesgue/Louisville measures are unique in some sense. For the counting measure, once we define the value for a single point, typically one, the whole measure is determined. Similarly, once we have defined a real variable, the Lebesgue measure is determined. The Louisville requires a bit more structure, but once the canonical coordinates are chosen, the measure is determined as well. Therefore the first question is whether these quantized measure are also unique in some sense.

While we have found some literature on non-additive measures and set functions (CITE), together with modified theories of integration, these seem to concentrate on monotonic measures: measures that increase as the set increase. Unfortunately, the quantized measure does not have this feature. Consider the set $\{ z^+, z^- \}$. An equal mixture will return the maximally mixed state which, as we said, would correspond to a measure equal to 2. Now consider the set $\{ z^+, z^-, x^+ \}$. This will not correspond to the maximally mixed state, and therefore the measure will be less than two. The set is bigger, but the measure is smaller. Therefore there may be new mathematical ground to be explored.

We may want to understand the necessary conditions for a quantifying measure. Some may be derived from the strict concavity of entropy, given the connection between the two. Furthermore, note that the relationship between the measure over two points, the entropy of the equal mixture of two states and the probability of transition between two states are invertible. This means that the relationship between two states defines the geometry of the whole state space. Therefore there must be further consistency requirements that can be identified.

Another issue that needs to be understood is how the quantized measure behaves in composite systems and across multiple degrees of freedom. In the classical case, the measure is defined on the product of the $\sigma$-algebra. Ideally, we would like to show that this simple procedure cannot support a quantized measure, and the $\sigma$-algebra of tensor product must be taken.\footnote{Technically it would be the $\sigma$-algebra projective space coming from the tensor product of the Hilbert spaces associated with the individual systems.}

The first spaces we should study are the symplectic ones as these are the ones that allow for observer independent densities. If we want to extend to a quantization of space-time, however, Riemannian spaces need to be studies as well. However, we have found a connection between the two: if the symplectic form is expressed in position and velocity, instead of canonical coordinate, the metric tensor appears as the position-velocity component. The metric tensor can then be understood as defining volumes in position-velocity instead in position-momentum. This may allow us to transfer the quantized measure on the tangent bundle and then, hopefully, to the space itself.

Finally, we want to stress that, contrary to what happens in other programs, the physical motivation for the mathematical generalization are tightly driven by the physics. We are not introducing concepts, like negative probability, have no clear direct physical meaning. The goal is the quantification of states.

\section{Conclusion}

We have seen that, in physics, measure theory is not only used to characterize classical probability but also to quantify states in state spaces. Ideally, such measure would have the following three conditions:
\begin{enumerate}
	\item single states count as one
	\item finite continuous regions correspond to finite information (and therefore finite state quantification)
	\item state quantification is additive on disjoint regions.
\end{enumerate}
However, these conditions are not compatible, and only two of the three can be chosen. The classical discrete cases discards the second; the classical continuous case discards the first; the quantum case discards the third. Quantization, then, can be understood as putting a unitary lower bound to the measure that quantifies states.

The lack of additivity on the whole space makes it impossible to use classical probability on the state space as a whole. Physically, the issue is that states must be counted at-all-else-being-equal, feature broken by contextuality. But if the states do belong to the same context (e.g. they are the output of the same measurement), additivity is recovered. This tells us very directly why quantization requires contextuality, and why classical probability works within the same context. The fact that single states and and finite continuous regions have both finite measure also tells us why quantum mechanics mixes features of the classical and discrete cases.

We believe that further study and characterization of the non-additive quantized measure will not only bring more clarity in the interplay between the above concept, but it may be necessary for the development of future physical theory. Given that, in a field theory, the number of independent degrees of freedom is taken to be proportional to the spatial volume, quantization of space time can be understood as the quantization of the measure that quantifies the indipendent degrees of freedom. The development of physically motivated mathematical tools may help in that regard.

\bibliography{bibliography}

\newcommand{\pj}[1] {\underbar{$#1$}}


\end{document}
